{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8E91DaxZWw3"
      },
      "source": [
        "Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QMQvT4g9brO",
        "outputId": "0f6a6b77-20b2-4a40-a0fb-82c1733c6b54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cp -r /content/drive/MyDrive/TFM/IA/scripts/* /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtxgPQk9bNoz",
        "outputId": "c6f38d88-a895-4974-b4c8-162fa9441df5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.1/677.1 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.5/192.5 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.5/89.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m942.7/942.7 kB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.4/305.4 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fiftyone-db (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q fiftyone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2mx885Va6N8",
        "outputId": "37b1bdad-0826-492e-9989-8d6bcab69c48"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# Torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchHelpers.engine import train_one_epoch, evaluate\n",
        "\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# import fiftyone as fo\n",
        "\n",
        "import numpy as np\n",
        "from PIL import  ImageDraw\n",
        "from PIL import  Image as PILImage\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from enum import Enum\n",
        "\n",
        "# Garbage collector and os operations\n",
        "import gc\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c616h0in9brR"
      },
      "source": [
        "# Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BSVeN99u9brR"
      },
      "outputs": [],
      "source": [
        "# Constants file\n",
        "\n",
        "# Categories\n",
        "CATEGORIES = ['staff']\n",
        "\n",
        "CATEGORIES_TO_NUM = dict({\n",
        "  'background': 0,\n",
        "  'staff': 1,\n",
        "  'empty-staff': 1,\n",
        "})\n",
        "NUM_TO_CATEGORIES = dict({\n",
        "  0: 'background',\n",
        "  1: 'staff',\n",
        "})\n",
        "\n",
        "\n",
        "# Datasets\n",
        "DATASETS = [\n",
        "  'Capitan',\n",
        "  'SEILS',\n",
        "  'FMT_C',\n",
        "]\n",
        "\n",
        "\n",
        "DEBUG_FLAG = False\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "BBOX_REDIMENSION = 0.8\n",
        "BBOX_REDIMENSIONED_RECOVER = 1 / BBOX_REDIMENSION\n",
        "\n",
        "TRAIN_NUM_EPOCHS = 100\n",
        "TRAIN_PATIENTE = 20\n",
        "\n",
        "DRIVE_IA_FOLDER = ''\n",
        "DRIVE_IA_FOLDER = '/content/drive/MyDrive/TFM/IA'\n",
        "\n",
        "DRIVE_DATASETS_FOLDER = f'{DRIVE_IA_FOLDER}/datasets'\n",
        "DRIVE_MODELS_FOLDER = f'{DRIVE_IA_FOLDER}/models'\n",
        "DRIVE_LOGS_FOLDER = f'{DRIVE_IA_FOLDER}/logs'\n",
        "DRIVE_IMG_FOLDER = f'{DRIVE_IA_FOLDER}/img'\n",
        "\n",
        "DRIVE_TRAIN_LOGS_FOLDER = f'{DRIVE_LOGS_FOLDER}/train'\n",
        "DRIVE_VAL_LOGS_FOLDER = f'{DRIVE_LOGS_FOLDER}/val'\n",
        "DRIVE_TEST_LOGS_FOLDER = f'{DRIVE_LOGS_FOLDER}/test'\n",
        "\n",
        "DRIVE_VAL_IMG_FOLDER = f'{DRIVE_IMG_FOLDER}/val'\n",
        "DRIVE_TEST_IMG_FOLDER = f'{DRIVE_IMG_FOLDER}/test'\n",
        "\n",
        "SAE_IMAGE_SIZE =  (512, 512)\n",
        "\n",
        "BIN_UMBRALS = [i/100 for i in range(10, 91, 5)]\n",
        "DROPOUT_VALUES = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "\n",
        "class PredictionsCombinationType(Enum):\n",
        "    NONE = ''\n",
        "    MEAN = 'mean'\n",
        "    MAX  = 'max'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGtj3nxc9brS"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bc0tyI3m9brS"
      },
      "outputs": [],
      "source": [
        "class SAE(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    with_batch_normalization = True\n",
        "    dropout_probability = 0\n",
        "\n",
        "    super().__init__()\n",
        "    self.encoder = torch.nn.Sequential(\n",
        "      nn.Conv2d(in_channels=1  , out_channels=128, kernel_size=(5, 5), padding=2),\n",
        "      nn.BatchNorm2d(128) if with_batch_normalization else nn.Identity(),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout2d(p = dropout_probability),\n",
        "      nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "\n",
        "      nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(5, 5), padding=2),\n",
        "      nn.BatchNorm2d(128) if with_batch_normalization else nn.Identity(),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout2d(p = dropout_probability),\n",
        "      nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "\n",
        "      nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(5, 5), padding=2),\n",
        "      nn.BatchNorm2d(128) if with_batch_normalization else nn.Identity(),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout2d(p = dropout_probability),\n",
        "      nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "    )\n",
        "\n",
        "    self.decoder = torch.nn.Sequential(\n",
        "      nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(5, 5), padding=2),\n",
        "      nn.BatchNorm2d(128) if with_batch_normalization else nn.Identity(),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout2d(p = dropout_probability),\n",
        "      nn.Upsample(scale_factor=(2, 2)),\n",
        "\n",
        "      nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(5, 5), padding=2),\n",
        "      nn.BatchNorm2d(128) if with_batch_normalization else nn.Identity(),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout2d(p = dropout_probability),\n",
        "      nn.Upsample(scale_factor=(2, 2)),\n",
        "\n",
        "      nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(5, 5), padding=2),\n",
        "      nn.BatchNorm2d(128) if with_batch_normalization else nn.Identity(),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout2d(p = dropout_probability),\n",
        "      nn.Upsample(scale_factor=(2, 2)),\n",
        "\n",
        "      nn.Conv2d(in_channels=128, out_channels=1  , kernel_size=(5, 5), padding=2),\n",
        "      nn.Sigmoid(),\n",
        "    )\n",
        "\n",
        "  def set_dropout_probability(self, dropout_probability = 0.2):\n",
        "    for module in self.modules():\n",
        "      if 'Dropout' in type(module).__name__:\n",
        "        module.p = dropout_probability\n",
        "\n",
        "  def enable_eval_dropout(self):\n",
        "    for module in self.modules():\n",
        "      if 'Dropout' in type(module).__name__:\n",
        "        print(module)\n",
        "        module.train()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Checks if input if batched or not. If it's not batched, add a dimension\n",
        "    addDimension = x.dim() == 3\n",
        "\n",
        "    if addDimension:\n",
        "      x = torch.stack([x])\n",
        "\n",
        "    x = self.encoder(x)\n",
        "    x = self.decoder(x)\n",
        "\n",
        "    # If we added a dimension, remove it for the loss function\n",
        "    if addDimension:\n",
        "      x = x.squeeze(0)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UTaoHb_J5lG4"
      },
      "outputs": [],
      "source": [
        "class ModelCheckpoint:\n",
        "  def __init__(self, filepath, model):\n",
        "    \"\"\"\n",
        "      Initialises the class for a ModelCheckpoint callback.\n",
        "      Arguments:\n",
        "        filepath: string\n",
        "          filename with .pt extension to save the model\n",
        "        model: torch.nn\n",
        "          model to save while training\n",
        "    \"\"\"\n",
        "    self.filepath = filepath\n",
        "    self.model: torch.nn.Module = model\n",
        "    self.save()\n",
        "\n",
        "  def save(self):\n",
        "    torch.save(self.model, self.filepath)\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "  def __init__(self, patience, epochs, checkpoint, epsilon, searchFor = 'min', plotMsg = False):\n",
        "    \"\"\"\n",
        "      Initialises the class for an Early Stopping callback.\n",
        "      Arguments:\n",
        "        patience: int\n",
        "          number of epochs without improving the metric\n",
        "        epochs: int\n",
        "          total number of epochs\n",
        "        checkpoint: ModelCheckpoint\n",
        "          object for saving the best model\n",
        "        epsilon: float\n",
        "          threshold in order to update best result\n",
        "        searchFor: str\n",
        "          if best result is min or max\n",
        "    \"\"\"\n",
        "    self.patience = patience\n",
        "    self.epochs = epochs\n",
        "    self.checkpoint = checkpoint\n",
        "    self.stop_training = False\n",
        "    self.actual_patience = 1\n",
        "    self.best_result = None\n",
        "    self.epsilon = epsilon\n",
        "    self.plotMsg = plotMsg\n",
        "    self.searchFor = searchFor\n",
        "\n",
        "    if self.searchFor == 'min':\n",
        "      self.best_result = float('inf')\n",
        "    elif self.searchFor == 'max':\n",
        "      self.best_result = 0.0\n",
        "\n",
        "  def update(self, last_result, epoch):\n",
        "    \"\"\"\n",
        "      Updates results after each epoch and saves the best model\n",
        "      through ModelCheckpoint Object.\n",
        "      Arguments:\n",
        "        last_result: float\n",
        "          result obtained on the selected metric\n",
        "      Returns:\n",
        "        Boolean\n",
        "          True if the current patience has achieved maximum patience\n",
        "          False otherwise\n",
        "    \"\"\"\n",
        "    hasImproved = False\n",
        "\n",
        "    if self.searchFor == 'min':\n",
        "      hasImproved =  last_result < self.best_result\n",
        "    elif self.searchFor == 'max':\n",
        "      hasImproved =  last_result > self.best_result\n",
        "\n",
        "    if hasImproved:\n",
        "      # Improvement\n",
        "      self.actual_patience = 1\n",
        "      last_best = self.best_result\n",
        "      self.best_result = last_result\n",
        "\n",
        "      # Saving best model\n",
        "      self.checkpoint.save()\n",
        "      if self.plotMsg:\n",
        "        print(f'Updating best result ({self.best_result}), last: {last_best}. Difference of {np.round(abs(last_best-self.best_result), 3)}.')\n",
        "        print(f'Saving on {self.checkpoint.filepath}')\n",
        "\n",
        "    else:\n",
        "      # No improvement\n",
        "      self.actual_patience += 1\n",
        "      if self.actual_patience >= self.patience:\n",
        "        self.stop_training = True\n",
        "        if self.plotMsg:\n",
        "          print(f'Training stopped with patience {self.patience}')\n",
        "          print(f'Best result obtained: {np.round(self.best_result, 3)}')\n",
        "      if self.plotMsg:\n",
        "        print(f'Not updating best result ({self.best_result}), last: {last_result}.')\n",
        "\n",
        "\n",
        "    # if self.plotMsg:\n",
        "    print(f'\\tEarly stopping: [{self.actual_patience}/{self.patience}] on epoch {epoch}/{self.epochs}')\n",
        "\n",
        "    return self.stop_training\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLLMUoTI9brT"
      },
      "source": [
        "# Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4meGFP5Q9brT"
      },
      "outputs": [],
      "source": [
        "class Symbol():\n",
        "    def __init__(self, boxes : list[int], position_in_staff :str, agnostic_symbol_type:str) -> None:\n",
        "        self.box = boxes\n",
        "        self.position_in_staff = position_in_staff\n",
        "        self.agnostic_symbol_type = agnostic_symbol_type\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f'Boxes: {self.box}\\t, Position: {self.position_in_staff}\\t, Symbol: {self.agnostic_symbol_type}'\n",
        "\n",
        "    def resize(self, ratio_width: float, ratio_height: float):\n",
        "        box = self.box\n",
        "        self.box = [int(box[0]/ratio_width), int(box[1]/ratio_height), int(box[2]/ratio_width), int(box[3]/ratio_height)]\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "class Region():\n",
        "    def __init__(self, box: list[int], label: int, notes: list[Symbol] = None) -> None:\n",
        "        self.box = box\n",
        "        self.label = label\n",
        "        self.notes = notes\n",
        "\n",
        "    def __str__(self, symbols = True) -> str:\n",
        "        message = f'Bounding boxes: {self.box}\\t, Class: {NUM_TO_CATEGORIES[self.label]}'\n",
        "\n",
        "        if symbols:\n",
        "            message += '\\t, Symbols: '\n",
        "            if self.notes is None:\n",
        "                message += '[]'\n",
        "            else:\n",
        "                message += '\\n\\t\\t['\n",
        "                for sym in self.notes:\n",
        "                    message += f'\\n\\t\\t\\t{sym}'\n",
        "                message += '\\n\\t\\t]'\n",
        "\n",
        "        return message\n",
        "\n",
        "    def isEmpty(self):\n",
        "        return len(self.box) == 0\n",
        "\n",
        "    def resize(self, ratio_width: float, ratio_height: float):\n",
        "\n",
        "        box = self.box\n",
        "        self.box = [int(box[0]/ratio_width), int(box[1]/ratio_height), int(box[2]/ratio_width), int(box[3]/ratio_height)]\n",
        "\n",
        "        if self.notes is not None:\n",
        "            self.notes = [note.resize(ratio_width=ratio_width, ratio_height=ratio_height) for note in self.notes]\n",
        "\n",
        "        return self\n",
        "\n",
        "class ImageExample():\n",
        "    def __init__(self, regions: list[Region], image: PILImage, imageName: str, imagePath: str) -> None:\n",
        "        self.regions = regions\n",
        "        self.image: PILImage = image\n",
        "        self.imageName = imageName\n",
        "        self.imagePath = imagePath\n",
        "\n",
        "    def isEmpty(self):\n",
        "        return len(self.regions) <= 0\n",
        "\n",
        "    def dataAsTensor(self):\n",
        "        if self.isEmpty(): # If image does not contain any object (background)\n",
        "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "            labels = torch.zeros(1, dtype=torch.int64)\n",
        "        else:\n",
        "            boxes = torch.as_tensor([region.box for region in self.regions], dtype=torch.float32) # [xmin, ymin, xmax, ymax]\n",
        "            labels = torch.as_tensor([region.label for region in self.regions], dtype=torch.int64)\n",
        "\n",
        "        return boxes, labels\n",
        "\n",
        "    def get_boxes(self):\n",
        "        return [region.box for region in self.regions]\n",
        "\n",
        "    def get_name(self):\n",
        "        return self.imageName\n",
        "\n",
        "    def resize_examples(self, resize_shape):\n",
        "\n",
        "        if resize_shape is not None:\n",
        "            width, height = self.image.size\n",
        "            ratio_width = width / resize_shape[0]\n",
        "            ratio_height = height / resize_shape[1]\n",
        "\n",
        "            self.image = self.image.resize(resize_shape)\n",
        "\n",
        "            self.regions = [region for region in self.regions if not region.isEmpty()]\n",
        "            self.regions = [region.resize(ratio_width=ratio_width, ratio_height=ratio_height) for region in self.regions]\n",
        "            # for region in self.regions: region.resize(ratio_width=ratio_width, ratio_height=ratio_height)\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        for r in self.regions:\n",
        "            print(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-phXazY9brU"
      },
      "source": [
        "# Dataset Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlV0SZ1uTaCc"
      },
      "source": [
        "## Read JSON functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Lfl6ynnyR4i8"
      },
      "outputs": [],
      "source": [
        "def read_json_symbols(box: json, resize_shape, ratio_width, ratio_height) -> list[Symbol]:\n",
        "    \"\"\"Reads the Symbols from the JSON and returns them all as a list\n",
        "\n",
        "    Args:\n",
        "        box (json): json following the symbols section\n",
        "        resize_shape (list): None if no reshape is needed\n",
        "        ratio_width (float): width ratio to resize\n",
        "        ratio_height (float): height ratio to resize\n",
        "\n",
        "    Returns:\n",
        "        list[Symbol]: list of Symbols read\n",
        "    \"\"\"\n",
        "    listOfSymbols = None\n",
        "\n",
        "    if 'symbols' in box:\n",
        "        listOfSymbols = []\n",
        "        symbols = box['symbols']\n",
        "\n",
        "        for symbol in symbols:\n",
        "            symbolBox = symbol['bounding_box']\n",
        "            symbolType = symbol['agnostic_symbol_type']\n",
        "            symbolPos = symbol['position_in_staff']\n",
        "\n",
        "            xmin, xmax = symbolBox['fromX'], symbolBox['toX']\n",
        "            ymin, ymax = symbolBox['fromY'], symbolBox['toY']\n",
        "\n",
        "            if resize_shape is not None:\n",
        "                xmin, xmax = int(round(xmin/ratio_width, 0)), int(round(xmax/ratio_width, 0))\n",
        "                ymin, ymax = int(round(ymin/ratio_height, 0)), int(round(ymax/ratio_height, 0))\n",
        "\n",
        "            listOfSymbols.append(Symbol([xmin, ymin, xmax, ymax], symbolPos, symbolType))\n",
        "\n",
        "    return listOfSymbols\n",
        "\n",
        "def read_json_datafile(path_json: str, path_image: str, resize_shape = None, read_symbols = False) :\n",
        "    \"\"\"Reads the JSON data file and returns a list of Regions\n",
        "\n",
        "    Args:\n",
        "        path_json (str): path to JSON data file to read\n",
        "        path_image (str): path to image to load\n",
        "        resize_shape (list(int), optional): shape of reshape. Defaults to None.\n",
        "        read_symbols (bool, optional): if Symbols must be read. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        PILImage, list(Region): image and regions from that image\n",
        "    \"\"\"\n",
        "\n",
        "    # print(f'Path_image: {path_image}')\n",
        "    # print(f'Path_json: {filename}')\n",
        "\n",
        "    image = PILImage.open(path_image)#.convert(\"RGB\")\n",
        "    width, height = image.size\n",
        "\n",
        "    ratio_width, ratio_height = 0, 0\n",
        "    if resize_shape is not None:\n",
        "        image = image.resize(resize_shape)\n",
        "        ratio_width = width / resize_shape[0]\n",
        "        ratio_height = height / resize_shape[1]\n",
        "\n",
        "    # getBoxesFunction = lambda b, rW, rH : [int(b['fromX']), int(b['fromY']), int(b['toX']), int(b['toY'])] if resize_shape is None else [int(b['fromX']/rW), int(b['fromY']/rH), int(b['toX']/rW), int(b['toY']/rH)]\n",
        "    # getCategoriInt = lambda c : 'staff' if 'staff' in c else c\n",
        "    # readSymbols = lambda b, r, rW, rH : read_json_symbols(box=b, resize_shape=r, ratio_width=rW, ratio_height=rH)\n",
        "    # createRegionSymbols = lambda b, r, rH, rW : Region(box=getBoxesFunction(b=b['bounding_box'], rH=rH, rW=rW), label=getCategoriInt(b['type']), notes=readSymbols(b=b, r=r, rH=rH, rW=rW))\n",
        "    # createRegionNonSymbols = lambda b, rH, rW : Region(box=getBoxesFunction(b=b['bounding_box'], rH=rH, rW=rW), label=getCategoriInt(b['type']), notes=None)\n",
        "\n",
        "    boxes = []\n",
        "    classes = []\n",
        "    regionsList: list[Region] = []\n",
        "    with open(path_json) as f:\n",
        "        # Read image from json (json info and image)\n",
        "        example_dict = json.load(f)\n",
        "        #filename_image = example_dict['filename'].encode('utf8')\n",
        "        if \"pages\" not in example_dict:\n",
        "            return ImageExample(regions=[Region(boxes, classes)], image=image, imageName=get_file_name(path_image), imagePath=path_image)\n",
        "\n",
        "        pages = example_dict['pages']\n",
        "\n",
        "        for _, page in enumerate(pages):\n",
        "            if \"regions\" not in page:\n",
        "                continue\n",
        "            regions = page['regions']\n",
        "\n",
        "            # if read_symbols:\n",
        "            #     regionsList.append([createRegionSymbols(b=box, r=resize_shape, rH=ratio_height, rW=ratio_width) for box in regions])\n",
        "            # else:\n",
        "            #     regionsList.append([createRegionNonSymbols(b=box, rH=ratio_height, rW=ratio_width) for box in regions])\n",
        "\n",
        "            for box in regions:\n",
        "                box_data = box['bounding_box']\n",
        "                category = box['type']\n",
        "                if \"staff\" in category: # empty-staff => staff\n",
        "                    category = \"staff\"\n",
        "\n",
        "                # skip that aren't interesting categories (author, etc)\n",
        "                if category in CATEGORIES:\n",
        "                    # print(f'region: {box}')\n",
        "                    category_int = CATEGORIES_TO_NUM[category]\n",
        "                    xmin, xmax = box_data['fromX'], box_data['toX']\n",
        "                    ymin, ymax = box_data['fromY'], box_data['toY']\n",
        "\n",
        "                    if resize_shape is not None:\n",
        "                        xmin, xmax = xmin/ratio_width, xmax/ratio_width\n",
        "                        ymin, ymax = ymin/ratio_height, ymax/ratio_height\n",
        "\n",
        "                    boxes_images = [int(xmin), int(ymin), int(xmax), int(ymax)]\n",
        "\n",
        "                    if xmax <= xmin or ymax <= ymin:\n",
        "                        continue\n",
        "\n",
        "                    boxes_images = [int(box) for box in boxes_images]\n",
        "\n",
        "                    if len(boxes_images) == 4:\n",
        "                        if category == \"staff\" and read_symbols:\n",
        "                            listOfSymbols = read_json_symbols(box=box, resize_shape=resize_shape, ratio_width=ratio_width, ratio_height=ratio_height)\n",
        "                            regionsList.append(Region(box = boxes_images, label = category_int, notes = listOfSymbols))\n",
        "                        else:\n",
        "                            regionsList.append(Region(box = boxes_images, label = category_int))\n",
        "\n",
        "    return ImageExample(regions=regionsList, image=image, imageName=get_file_name(path_image), imagePath=path_image)\n",
        "\n",
        "def read_paths_dataset_files(path_listJsons: str):\n",
        "    \"\"\"Read a file that contains a list to the paths to different JSON files\n",
        "\n",
        "    Args:\n",
        "        path_listJsons (str): path to file containing the list of json files\n",
        "\n",
        "    Returns:\n",
        "        [jsonList], [imagesList]: paths to different images and JSON data file\n",
        "    \"\"\"\n",
        "    imagesList: list[str] = []\n",
        "    jsonList: list[str] = []\n",
        "\n",
        "    fileReading = open(path_listJsons, 'r')\n",
        "    path = path_listJsons[:path_listJsons.rfind('/')+1]\n",
        "\n",
        "    for file in fileReading:\n",
        "        singleFileName = file[:file.rfind('.')]\n",
        "        imagesList.append(path + 'images/' + singleFileName)\n",
        "        jsonList.append(path + 'JSON/' + file[:file.rfind('n')+1])  # Removes empty spaces from end\n",
        "\n",
        "    return jsonList, imagesList\n",
        "\n",
        "def pathExist(dir_path):\n",
        "    return os.path.exists(dir_path)\n",
        "\n",
        "def makeDirIfNotExist(dir_path):\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXzqGfHmTdnn"
      },
      "source": [
        "## Dataset loader\n",
        "\n",
        "Reads the dataset files and load the images/bounding boxes from it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mTqT3HUj9brU"
      },
      "outputs": [],
      "source": [
        "def load_dataset(imagesPath: list[str], jsonsPath: list[str], reshape = None):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        imagesPath (list[str]): paths to images\n",
        "        jsonsPath (list[str]): paths to JSON data files\n",
        "        reshape (list[int], optional): images size reshape if needed. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        tuple[list[PILImage], list[list[Region]]]: lists of images and regions for each image\n",
        "    \"\"\"\n",
        "\n",
        "    return [read_json_datafile(iPath, jPath, reshape) for iPath, jPath in zip(imagesPath, jsonsPath)]\n",
        "\n",
        "def get_file_name(filePath: str):\n",
        "    return filePath[filePath.rfind('/')+1:filePath.find('.')]\n",
        "\n",
        "def draw_back_white_bb_image(height, width, boxes):\n",
        "    targetImage = PILImage.new(\"RGB\", (width, height), \"#000000\")\n",
        "\n",
        "    draw = ImageDraw.Draw(targetImage)\n",
        "    for box in boxes:\n",
        "        draw.rectangle([box[0], box[1], box[2]-1, box[3]-1], fill=\"#FFFFFF\")\n",
        "\n",
        "    return targetImage\n",
        "\n",
        "def draw_boxes_on_image(example: ImageExample, saveDirectory: str):\n",
        "    \"\"\"Save image with passes boxes drawn\n",
        "\n",
        "    Args:\n",
        "        image (Image): image to save\n",
        "        boxes (list[list[int]]): bounding boxes to save\n",
        "        imageName (str): image name\n",
        "        saveDirectory (str): directory to save\n",
        "    \"\"\"\n",
        "    # Read file & normalize image\n",
        "    #img, regionsList = read_json_datafile(path_json, path_img, reshape)\n",
        "    img = example.image\n",
        "\n",
        "    print(saveDirectory)\n",
        "\n",
        "    makeDirIfNotExist(saveDirectory)\n",
        "\n",
        "    drawingImage = ImageDraw.Draw(img)\n",
        "\n",
        "    # Save image\n",
        "    # for box in example.get_boxes():\n",
        "    #     drawingImage.rectangle(box, outline=\"red\", width=3)\n",
        "    [drawingImage.rectangle(box, outline=\"red\") for box in example.get_boxes()]\n",
        "\n",
        "    img.save(saveDirectory + example.imageName + '.jpg')\n",
        "\n",
        "def get_transform():\n",
        "    t = [\n",
        "        T.ToTensor(),\n",
        "        T.Grayscale(),\n",
        "        ]\n",
        "\n",
        "    return T.Compose(t)\n",
        "\n",
        "\n",
        "def resize_box(box, vResize, hResize):\n",
        "    \"\"\"\n",
        "      Boxes as [x1, y1, x2, y2]\n",
        "    \"\"\"\n",
        "    if(len(box)) < 4:\n",
        "      print(box)\n",
        "    # Get dimensions\n",
        "    x1, y1, x2, y2 = box[0], box[1], box[2], box[3]\n",
        "    w = x2 - x1 # 100\n",
        "    h = y2 - y1 # 100\n",
        "\n",
        "    # Calculate resized dimensions\n",
        "    w2 = w * hResize # 80\n",
        "    h2 = h * vResize # 80\n",
        "\n",
        "    # Get size difference\n",
        "    wDif = w2 - w  # 100 - 80 = 20\n",
        "    hDif = h2 - h  # 100 - 80 = 20\n",
        "\n",
        "    # Get how much each coordinate must move\n",
        "    xMov = wDif / 2\n",
        "    yMov = hDif / 2\n",
        "\n",
        "    # Get new coordinates\n",
        "    x1 = x1 - xMov\n",
        "    x2 = x2 + xMov\n",
        "\n",
        "    y1 = y1 - yMov\n",
        "    y2 = y2 + yMov\n",
        "\n",
        "    return [x1, y1, x2, y2]\n",
        "\n",
        "class DatasetLoader:\n",
        "    def __init__(self, dataset_name: str, reshape:list[float] = None) -> None:\n",
        "        \"\"\"Create class that loads the dataset\n",
        "\n",
        "        Args:\n",
        "            dataset_name (str): forder path to dataset containing train, test and val's txt files without final / (Ex.: \"datasets/Captitan\")\n",
        "            reshape: reshape of the images\n",
        "        \"\"\"\n",
        "        self.datasetFolder = f'{DRIVE_DATASETS_FOLDER}/{dataset_name}'\n",
        "        self.reshape = reshape\n",
        "\n",
        "    def loadTrainPaths(self):\n",
        "        return read_paths_dataset_files(f'{self.datasetFolder}/train.txt')\n",
        "\n",
        "    def loadTestPaths(self):\n",
        "        return read_paths_dataset_files(f'{self.datasetFolder}/test.txt')\n",
        "\n",
        "    def loadValPaths(self):\n",
        "        return read_paths_dataset_files(f'{self.datasetFolder}/val.txt')\n",
        "\n",
        "    def loadTrainDataset(self):\n",
        "        trainJSonPaths, trainImagesPath = self.loadTrainPaths()\n",
        "        return [read_json_datafile(json, img, self.reshape) for json, img in zip(trainJSonPaths, trainImagesPath)]\n",
        "\n",
        "    def loadValDataset(self):\n",
        "        valJSonPaths, valImagesPath = self.loadValPaths()\n",
        "        return [read_json_datafile(json, img, self.reshape) for json, img in zip(valJSonPaths, valImagesPath)]\n",
        "\n",
        "    def loadTestDataset(self):\n",
        "        testJSonPaths, testImagesPath = self.loadTestPaths()\n",
        "        return [read_json_datafile(json, img, self.reshape) for json, img in zip(testJSonPaths, testImagesPath)]\n",
        "\n",
        "    def loadAllDatasetPaths(self):\n",
        "        return self.loadTrainPaths(), self.loadValPaths(), self.loadTestPaths()\n",
        "\n",
        "    def loadAllDataset(self):\n",
        "        return self.loadTrainDataset(), self.loadValDataset(), self.loadTestDataset()\n",
        "\n",
        "    def drawBoxesInDataset(self):\n",
        "        train, val, test = self.loadAllDataset()\n",
        "        [draw_boxes_on_image(example=example, saveDirectory=self.datasetFolder+\"/train/\") for example in train]\n",
        "        [draw_boxes_on_image(example=example, saveDirectory=self.datasetFolder+\"/val/\") for example in val]\n",
        "        [draw_boxes_on_image(example=example, saveDirectory=self.datasetFolder+\"/test/\") for example in test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j6AM8zVTiSC"
      },
      "source": [
        "## Dataset holders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uMrtheooR4i9"
      },
      "outputs": [],
      "source": [
        "class TrainMusicDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, name, jsonPaths: list[str], imagesPaths: list[str], resize_shape = None, transforms = get_transform(), box_resize_vertical = False, box_resize_horizontal = False):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            name: Name of the dataset (b-59-80, Seils, Mus-Trad-...)\n",
        "            path_json: list with all paths to json\n",
        "            path_json: list with all paths to source images\n",
        "        \"\"\"\n",
        "        self.name = name\n",
        "        self.resize_shape = resize_shape\n",
        "        self.jsonPaths: list[str] = jsonPaths\n",
        "        self.imagesPaths: list[str] = imagesPaths\n",
        "        self.transforms = transforms\n",
        "        self.box_resize_vertical = box_resize_vertical\n",
        "        self.box_resize_horizontal = box_resize_horizontal\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path_img = self.imagesPaths[idx]\n",
        "        path_json = self.jsonPaths[idx]\n",
        "\n",
        "        example: ImageExample = read_json_datafile(path_json, path_img, self.resize_shape)\n",
        "\n",
        "        # example: ImageExample = self.imagesPaths[idx]\n",
        "\n",
        "        example.resize_examples(self.resize_shape)\n",
        "\n",
        "        boxes, labels = example.dataAsTensor()\n",
        "        img = example.image\n",
        "\n",
        "        # image_id = torch.tensor([idx])\n",
        "        # area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # iscrowd = torch.zeros((len(labels),), dtype=torch.int64)\n",
        "\n",
        "        draw_boxes = example.get_boxes()\n",
        "        if self.box_resize_vertical or self.box_resize_horizontal:\n",
        "            vResize = BBOX_REDIMENSION if self.box_resize_vertical else 1\n",
        "            hResize = BBOX_REDIMENSION if self.box_resize_horizontal else 1\n",
        "            draw_boxes = [resize_box(box, vResize = vResize, hResize = hResize) for box in draw_boxes]\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"name\"]  = example.get_name()\n",
        "\n",
        "        width, height = img.size\n",
        "        targetImage = draw_back_white_bb_image(height=height, width=width, boxes=draw_boxes)\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "            targetImage = self.transforms(targetImage)\n",
        "\n",
        "        return img, target, targetImage# torch.tensor([img]), [target]\n",
        "\n",
        "    def getImagesPaths(self):\n",
        "        return self.imagesPaths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imagesPaths)\n",
        "\n",
        "class TestMusicDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, name, jsonPaths: list[str], imagesPaths: list[str], resize_shape = None, transforms = get_transform()):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            name: Name of the dataset (b-59-80, Seils, Mus-Trad-...)\n",
        "            path_json: list with all paths to json\n",
        "            path_json: list with all paths to source images\n",
        "        \"\"\"\n",
        "        self.name = name\n",
        "        self.resize_shape = resize_shape\n",
        "        self.jsonPaths: list[str] = jsonPaths\n",
        "        self.imagesPaths: list[str] = imagesPaths\n",
        "        self.transforms = transforms\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path_img = self.imagesPaths[idx]\n",
        "        path_json = self.jsonPaths[idx]\n",
        "\n",
        "        example: ImageExample = read_json_datafile(path_json, path_img, self.resize_shape)\n",
        "\n",
        "        example.resize_examples(self.resize_shape)\n",
        "\n",
        "        boxes, _ = example.dataAsTensor()\n",
        "        img = example.image\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            #img, target = self.transforms(img, target)\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, boxes# torch.tensor([img]), [target]\n",
        "\n",
        "    def getImagesPaths(self):\n",
        "        return self.imagesPaths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imagesPaths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rV7-GH99brX"
      },
      "source": [
        "# IoU and CC Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "liRhUMra2-XO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\"\"\"IoU and F1\"\"\"\n",
        "def calculate_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Calculate Intersection over Union (IoU) between two bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        box1 (list): Bounding box 1 in the format [x1, y1, x2, y2].\n",
        "        box2 (list): Bounding box 2 in the format [x1, y1, x2, y2].\n",
        "\n",
        "    Returns:\n",
        "        float: IoU value.\n",
        "    \"\"\"\n",
        "    # Get points of boxes\n",
        "    x11, y11, x12, y12 = box1\n",
        "    x21, y21, x22, y22 = box2\n",
        "\n",
        "    # Calculate dimensions of boxes\n",
        "    w1, h1 = x12 - x11, y12 - y11\n",
        "    w2, h2 = x22 - x21, y22 - y21\n",
        "\n",
        "    # Calculate areas of both boxes\n",
        "    area_box1: float = w1 * h1\n",
        "    area_box2: float = w2 * h2\n",
        "\n",
        "    # Calculate coordinates of the intersection rectangle\n",
        "    x_intersection = max(x11, x21)\n",
        "    y_intersection = max(y11, y21)\n",
        "    w_intersection = max(0, min(x12, x22) - x_intersection)\n",
        "    h_intersection = max(0, min(y12, y22) - y_intersection)\n",
        "\n",
        "    # Calculate area of intersection\n",
        "    area_intersection: float = w_intersection * h_intersection\n",
        "\n",
        "    # Calculate area of union\n",
        "    area_union: float = area_box1 + area_box2 - area_intersection\n",
        "\n",
        "    # Calculate IoU\n",
        "    return (area_intersection / area_union) if (area_union > 0.0) else 0.0\n",
        "\n",
        "def calculate_F1(y_true, y_pred, iou_threshold: float =0.5):\n",
        "    \"\"\"Calculate F1 score\n",
        "\n",
        "    Args:\n",
        "        y_true (list): List of true bounding boxes\n",
        "        y_pred (list): List of predicted bounding boxes\n",
        "        iou_threshold (float): IoU Threshold to math prediction with true. Defaults to 0.5.\n",
        "\n",
        "    Returns:\n",
        "        float: F1 score\n",
        "        list: list of matched IoUs\n",
        "    \"\"\"\n",
        "    num_preds = len(y_pred)\n",
        "    num_true  = len(y_true)\n",
        "\n",
        "    if num_preds == 0 and num_true == 0:\n",
        "        return 1, []\n",
        "\n",
        "    true_positives  = 0.0\n",
        "    false_positives = num_preds\n",
        "    false_negatives = num_true\n",
        "\n",
        "    matrix_of_iou = np.asarray([[calculate_iou(box_pred, box_true) for box_true in y_true] for box_pred in y_pred])\n",
        "    matched_ious  = []\n",
        "\n",
        "    if DEBUG_FLAG:\n",
        "      print(f'Matrix of IoU:\\n{matrix_of_iou}')\n",
        "\n",
        "    while matrix_of_iou.size > 0:\n",
        "        # Find the maximum IoU in the matrix\n",
        "        current_max_IoU = np.max(matrix_of_iou)\n",
        "        if current_max_IoU < iou_threshold:\n",
        "            break\n",
        "\n",
        "        # Add the current max IoU to matched IoUs\n",
        "        matched_ious.append(current_max_IoU)\n",
        "\n",
        "        # This is a match, update TP, FP, FN\n",
        "        true_positives  += 1\n",
        "        false_negatives -= 1\n",
        "        false_positives -= 1\n",
        "\n",
        "        # Get the indices of the maximum IoU\n",
        "        max_index = np.unravel_index(matrix_of_iou.argmax(), matrix_of_iou.shape)\n",
        "\n",
        "        # Remove the matched bounding boxes (row and column) from the IoU matrix\n",
        "        matrix_of_iou = np.delete(matrix_of_iou, max_index[1], axis=1)  # Remove column\n",
        "        matrix_of_iou = np.delete(matrix_of_iou, max_index[0], axis=0)  # Remove row\n",
        "\n",
        "    # Number of predictions that haven't been matched (false predictions)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score\n",
        "    precision = true_positives / (true_positives + false_positives) if num_preds > 0 else 0.\n",
        "    recall    = true_positives / (true_positives + false_negatives) if num_true  > 0 else 0.\n",
        "\n",
        "    if DEBUG_FLAG:\n",
        "      print(f'Precision: {precision} \\t Recall: {recall}')\n",
        "\n",
        "    # print(f'{precision} -- {recall}')\n",
        "    f1score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return f1score, matched_ious"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3m9XlAk_EB6"
      },
      "source": [
        "# Draw Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "P5wgqCrB_Alc"
      },
      "outputs": [],
      "source": [
        "\"\"\" Draw functions \"\"\"\n",
        "\n",
        "def drawTensorImageColor(tensor_image):\n",
        "\n",
        "  img = np.uint8(tensor_image.squeeze().permute(1, 2, 0).numpy() * 255)\n",
        "\n",
        "  # display the image\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "def drawTensorImageGrayScale(tensor_image, plot=False, save=False, image_name=None):\n",
        "\n",
        "  gray_image = np.uint8(tensor_image.squeeze().numpy() * 255)  # Assuming tensor values are in [0, 1] range\n",
        "\n",
        "  if plot:\n",
        "    img = cv2.cvtColor(gray_image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    # display the image\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "  if save:\n",
        "    image = PILImage.fromarray(gray_image, mode='L')\n",
        "\n",
        "    image.save(image_name)\n",
        "\n",
        "def drawBoxesOnTensor(tensor_image, bboxes, plot=False, save=False, image_name=None) :\n",
        "\n",
        "  gray_image = np.uint8(tensor_image.numpy() * 255).squeeze()  # Assuming tensor values are in [0, 1] range\n",
        "  contour_image = cv2.cvtColor(gray_image, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "  for b in bboxes:\n",
        "    x1, y1, x2, y2 = [int(elem) for elem in b]\n",
        "    cv2.rectangle(contour_image, (x1, y1), (x2, y2), (0, 255, 0), 1, cv2.LINE_AA)\n",
        "\n",
        "  # convert BGR to RGB\n",
        "  img = cv2.cvtColor(contour_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  if plot:\n",
        "    # display the image\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "  if save:\n",
        "    # save the image\n",
        "    cv2.imwrite(image_name, img)\n",
        "\n",
        "def drawBoxesPredictedAndGroundTruth(tensor_image, bboxes_predicted, bboxes_groundtruth, is_normalized, plot, save, image_name):\n",
        "\n",
        "  gray_image = np.uint8(tensor_image.numpy()).squeeze() if is_normalized else np.uint8(tensor_image.numpy() * 255).squeeze() # Assuming tensor values are in [0, 1] range\n",
        "  contour_image = cv2.cvtColor(gray_image, cv2.COLOR_GRAY2BGR)\n",
        "  green_color = (0, 255, 0)\n",
        "  red_color   = (0, 0, 255)\n",
        "\n",
        "  for b in bboxes_predicted:\n",
        "    x1, y1, x2, y2 = [int(elem) for elem in b]\n",
        "    cv2.rectangle(contour_image, (x1, y1), (x2, y2), red_color, 1, cv2.LINE_AA)\n",
        "\n",
        "  for b in bboxes_groundtruth:\n",
        "    x1, y1, x2, y2 = [int(elem) for elem in b]\n",
        "    cv2.rectangle(contour_image, (x1, y1), (x2, y2), green_color, 1, cv2.LINE_AA)\n",
        "\n",
        "  # convert BGR to RGB\n",
        "  img = cv2.cvtColor(contour_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  if plot:\n",
        "    # display the image\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "  if save:\n",
        "    if DEBUG_FLAG:\n",
        "      print(f'Saving in {image_name}')\n",
        "    cv2.imwrite(image_name, img)\n",
        "\n",
        "\n",
        "def immediatDrawGrayImg(gray_image):\n",
        "  img = cv2.cvtColor(gray_image, cv2.COLOR_GRAY2RGB)\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def saveTensorToImageGrayScale(tensor_image, image_name):\n",
        "\n",
        "  gray_image = np.uint8(tensor_image.squeeze().numpy() * 255)  # Assuming tensor values are in [0, 1] range\n",
        "\n",
        "  image = PILImage.fromarray(gray_image, mode='L')\n",
        "\n",
        "  image.save(image_name)\n",
        "\n",
        "\n",
        "def saveBoxesOnTensorToImage(tensor_image, bboxes, image_name):\n",
        "  gray_image = np.uint8(tensor_image.numpy() * 255).squeeze()  # Assuming tensor values are in [0, 1] range\n",
        "  contour_image = cv2.cvtColor(gray_image, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "  for b in bboxes:\n",
        "    x1, y1, x2, y2 = [int(elem) for elem in b]\n",
        "    cv2.rectangle(contour_image, (x1, y1), (x2, y2), (0, 255, 0), 1, cv2.LINE_AA)\n",
        "\n",
        "  # convert BGR to RGB\n",
        "  img = cv2.cvtColor(contour_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  # save the image\n",
        "  cv2.imwrite(image_name, img)\n",
        "\n",
        "\n",
        "def saveBoxesPredictedAndGroundTruth(tensor_image, bboxes_predicted, bboxes_groundtruth, image_name):\n",
        "  # gray_image = np.uint8(tensor_image.numpy() * 255).squeeze()  # Assuming tensor values are in [0, 1] range\n",
        "  gray_image = np.uint8(tensor_image.numpy() * 255).squeeze() # Assuming tensor values are in [0, 1] range\n",
        "  contour_image = cv2.cvtColor(gray_image, cv2.COLOR_GRAY2BGR)\n",
        "  green_color = (0, 255, 0)\n",
        "  red_color   = (255, 0, 0)\n",
        "\n",
        "  for b in bboxes_predicted:\n",
        "    x1, y1, x2, y2 = [int(elem) for elem in b]\n",
        "    cv2.rectangle(contour_image, (x1, y1), (x2, y2), red_color, 1, cv2.LINE_AA)\n",
        "\n",
        "  for b in bboxes_groundtruth:\n",
        "    x1, y1, x2, y2 = [int(elem) for elem in b]\n",
        "    cv2.rectangle(contour_image, (x1, y1), (x2, y2), green_color, 1, cv2.LINE_AA)\n",
        "\n",
        "  # cv2.drawContours(contour_image, contours, -1, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "  # convert BGR to RGB\n",
        "  img = cv2.cvtColor(contour_image, cv2.COLOR_BGR2RGB)\n",
        "  cv2.imwrite(image_name, img)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plotTrainval_losses(train_losses, val_losses):\n",
        "  # Convert the loss history lists to numpy arrays\n",
        "  train_loss_history = np.array(train_losses)\n",
        "  val_loss_history = np.array(val_losses)\n",
        "\n",
        "  # Plot the loss curves\n",
        "  plt.plot(train_loss_history, label='Training loss')\n",
        "  plt.plot(val_loss_history, label='Validation loss')\n",
        "  plt.legend()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n93Emmzu-Yai"
      },
      "source": [
        "# Componentes conexas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_ZyElGSQ9brX"
      },
      "outputs": [],
      "source": [
        "def binarizarTensor(tensor_image, bin_threshold_percentaje = 0.5):\n",
        "     # convertir el tensor a true/false si superan o no el umbral\n",
        "    return tensor_image > bin_threshold_percentaje\n",
        "\n",
        "\n",
        "def getConnectedComponents(tensor_image, bin_threshold_percentaje, min_area = 100, morphologyOps = True):\n",
        "  \"\"\"\n",
        "    Input:\n",
        "      tensor_image: tensor of values [0, 1]\n",
        "\n",
        "    Return bounding boxes as [x1, y1, x2, y2]\n",
        "  \"\"\"\n",
        "  # bin_threshold = int(bin_threshold_percentaje * 255)\n",
        "\n",
        "  # Convert NumPy array to grayscale OpenCV image\n",
        "  # gray_image = np.uint8(tensor_image.numpy() * 255).squeeze()  # Assuming tensor values are in [0, 1] range\n",
        "\n",
        "  # Create a binary image\n",
        "  # _, binary_image = cv2.threshold(gray_image, bin_threshold, 255, cv2.THRESH_BINARY)\n",
        "  # binary_image2 = (gray_image > bin_threshold).astype(np.uint8) * 255\n",
        "\n",
        "  # convertir el tensor a true/false si superan o no el umbral\n",
        "  pixels_with_higher_umbral = tensor_image.numpy().squeeze() > bin_threshold_percentaje\n",
        "\n",
        "  # convertir el tensor en uint8 con valores 0 o 255 en negro/blanco\n",
        "  binary_image = pixels_with_higher_umbral.astype(np.uint8) * 255\n",
        "\n",
        "  # print('BINARY IMAGE')\n",
        "  # immediatDrawGrayImg(binary_image)\n",
        "\n",
        "  if morphologyOps:\n",
        "    # definir un kernel de 3x3\n",
        "    kernel = np.ones((5,5), np.uint8)\n",
        "\n",
        "    # aplicar la operación de apertura\n",
        "    opening = cv2.morphologyEx(binary_image, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "    # aplicar la operación de cierre\n",
        "    binary_image = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "    # print('AFTER MORPHOLOGIC OPS')\n",
        "    # immediatDrawGrayImg(binary_image)\n",
        "\n",
        "  # Find contours in the binary image\n",
        "  contours, _ =  cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "  # `contours` is a list of contours found in the image\n",
        "  # `hierarchy` is the optional output hierarchy of the contours\n",
        "\n",
        "  # Loop over the contours and draw them on the original image\n",
        "  # contour_image = cv2.cvtColor(closing, cv2.COLOR_GRAY2BGR)\n",
        "  detected_boxes = 0\n",
        "\n",
        "  bboxes = []\n",
        "  for c in contours:\n",
        "      area = cv2.contourArea(c)\n",
        "      if area > min_area:\n",
        "        (x, y, w, h) = cv2.boundingRect(c)\n",
        "        bboxes.append([x, y, x+w, y+h])\n",
        "        detected_boxes += 1\n",
        "\n",
        "  # drawBoxesPredictedAndGroundTruth(torch.from_numpy(closing), bboxes, targetBoxes, is_normalized = True)\n",
        "\n",
        "  return bboxes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Mt9DBEHjAAL"
      },
      "source": [
        "# Train functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxhfC8wsRVsl"
      },
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QzhGmXLKRVsm"
      },
      "outputs": [],
      "source": [
        "def getModelFileName(dataset_name: str, dropout_value: float, uses_redimension_vertical: bool, uses_redimension_horizontal: bool):\n",
        "    sae_file = 'SAE'\n",
        "\n",
        "    sae_file += f'_D{int(dropout_value * 10)}'\n",
        "\n",
        "    if uses_redimension_vertical or uses_redimension_horizontal:\n",
        "        sae_file += '_R'\n",
        "        if uses_redimension_vertical:\n",
        "            sae_file += 'V'\n",
        "        if uses_redimension_horizontal:\n",
        "            sae_file += 'H'\n",
        "\n",
        "    return f'{sae_file}_{dataset_name}'\n",
        "\n",
        "def getLogsTrainingTrainFolder(dropout_value: float, uses_redimension_vertical: bool, uses_redimension_horizontal: bool):\n",
        "    r_folder = 'R'\n",
        "    if uses_redimension_vertical:\n",
        "        r_folder += 'V'\n",
        "    if uses_redimension_horizontal:\n",
        "        r_folder += 'H'\n",
        "    return f'{DRIVE_TRAIN_LOGS_FOLDER}/trainLoss/D{dropout_value}/{r_folder}'\n",
        "\n",
        "def getLogsTrainingValFolder(dropout_value: float, uses_redimension_vertical: bool, uses_redimension_horizontal: bool):\n",
        "    r_folder = 'R'\n",
        "    if uses_redimension_vertical:\n",
        "        r_folder += 'V'\n",
        "    if uses_redimension_horizontal:\n",
        "        r_folder += 'H'\n",
        "    return f'{DRIVE_TRAIN_LOGS_FOLDER}/valLoss/D{dropout_value}/{r_folder}'\n",
        "\n",
        "def getLogsValidationFolder(val_dropout: float, times_pass_model: int, type_combination: PredictionsCombinationType):\n",
        "    return f'{DRIVE_VAL_LOGS_FOLDER}/{type_combination.name}/D{val_dropout}/T{times_pass_model}'\n",
        "\n",
        "def getLogsTestFolder(val_dropout: float, times_pass_model: int, type_combination: PredictionsCombinationType):\n",
        "    return f'{DRIVE_TEST_LOGS_FOLDER}/{type_combination.name}/D{val_dropout}/T{times_pass_model}'\n",
        "\n",
        "def getImgsValidationFolder(val_dropout: float, times_pass_model: int, type_combination: PredictionsCombinationType, dataset_name: str, model_name: str):\n",
        "    return f'{DRIVE_VAL_IMG_FOLDER}/{type_combination.name}/D{val_dropout}/T{times_pass_model}/{dataset_name}/{model_name}'\n",
        "\n",
        "def getImgsTestFolder(val_dropout: float, times_pass_model: int, type_combination: PredictionsCombinationType, dataset_name: str, model_name: str):\n",
        "    return f'{DRIVE_TEST_IMG_FOLDER}/{type_combination.name}/D{val_dropout}/T{times_pass_model}/{dataset_name}/{model_name}'\n",
        "\n",
        "\n",
        "def generateTrainDatasetLoader(dataset_name: str, uses_redimension_vertical: bool, uses_redimension_horizontal: bool):\n",
        "    batch_size = 1\n",
        "\n",
        "    # Create datasets to train and val\n",
        "    datasetLoader = DatasetLoader(dataset_name)\n",
        "\n",
        "    # Load the datasets\n",
        "    train_json, train_img = datasetLoader.loadTrainPaths()\n",
        "\n",
        "\n",
        "    dataset_train = TrainMusicDataset(\n",
        "        name=dataset_name,\n",
        "        jsonPaths=train_json,\n",
        "        imagesPaths=train_img,\n",
        "        box_resize_vertical=uses_redimension_vertical,\n",
        "        box_resize_horizontal=uses_redimension_horizontal,\n",
        "        resize_shape=SAE_IMAGE_SIZE,\n",
        "        transforms=get_transform()\n",
        "        )\n",
        "\n",
        "    # Training dataset\n",
        "    data_loader_train = DataLoader(\n",
        "        dataset_train, batch_size=batch_size, shuffle=True, num_workers=0\n",
        "    )\n",
        "\n",
        "    return data_loader_train\n",
        "\n",
        "def generateValDatasetLoaderForTrain(dataset_name, uses_redimension_vertical, uses_redimension_horizontal):\n",
        "    batch_size = 1\n",
        "\n",
        "    # Create datasets to train and val\n",
        "    datasetLoader = DatasetLoader(dataset_name)\n",
        "\n",
        "    # Load the datasets\n",
        "    val_json, val_img = datasetLoader.loadValPaths()\n",
        "\n",
        "\n",
        "    dataset_eval  = TrainMusicDataset(\n",
        "        name=dataset_name,\n",
        "        jsonPaths=val_json,\n",
        "        imagesPaths=val_img,\n",
        "        box_resize_vertical=uses_redimension_vertical,\n",
        "        box_resize_horizontal=uses_redimension_horizontal,\n",
        "        resize_shape=SAE_IMAGE_SIZE,\n",
        "        transforms=get_transform()\n",
        "        )\n",
        "\n",
        "\n",
        "    # Validation  dataset (no redimension in order to calculate metrics)\n",
        "    data_loader_eval = DataLoader(\n",
        "        dataset_eval, batch_size=batch_size, shuffle=True, num_workers=0\n",
        "    )\n",
        "\n",
        "    return data_loader_eval\n",
        "\n",
        "def generateValDatasetLoaderForTest(dataset_name):\n",
        "    batch_size = 1\n",
        "\n",
        "    # Create datasets to train and val\n",
        "    datasetLoader = DatasetLoader(dataset_name)\n",
        "\n",
        "    # Load the datasets\n",
        "    val_json, val_img = datasetLoader.loadValPaths()\n",
        "\n",
        "\n",
        "    dataset_eval  = TestMusicDataset(\n",
        "        name=dataset_name,\n",
        "        jsonPaths=val_json,\n",
        "        imagesPaths=val_img,\n",
        "        resize_shape=SAE_IMAGE_SIZE,\n",
        "        transforms=get_transform()\n",
        "        )\n",
        "\n",
        "\n",
        "    # Validation  dataset (no redimension in order to calculate metrics)\n",
        "    data_loader_eval = DataLoader(\n",
        "        dataset_eval, batch_size=batch_size, shuffle=True, num_workers=0\n",
        "    )\n",
        "\n",
        "    return data_loader_eval\n",
        "\n",
        "\n",
        "def generateTestDatasetLoader(dataset_name):\n",
        "    batch_size = 1\n",
        "\n",
        "    # Create datasets to train and val\n",
        "    datasetLoader = DatasetLoader(dataset_name)\n",
        "\n",
        "    # Load the datasets\n",
        "    test_json, test_img = datasetLoader.loadTestPaths()\n",
        "\n",
        "\n",
        "    dataset_test  = TestMusicDataset(\n",
        "        name=dataset_name,\n",
        "        jsonPaths=test_json,\n",
        "        imagesPaths=test_img,\n",
        "        resize_shape=SAE_IMAGE_SIZE,\n",
        "        transforms=get_transform())\n",
        "\n",
        "\n",
        "    # Validation  dataset (no redimension in order to calculate metrics)\n",
        "    data_loader_test = DataLoader(\n",
        "        dataset_test, batch_size=batch_size, shuffle=True, num_workers=0\n",
        "    )\n",
        "\n",
        "    return data_loader_test\n",
        "\n",
        "def forwardToModel(model, image, times_pass_model, type_combination):\n",
        "    if type_combination == PredictionsCombinationType.MEAN:\n",
        "        result = model(image.to(device))\n",
        "        for i in range(1, times_pass_model):\n",
        "            print(f'\\r\\tForward passing with mean result {i+1}', end='')\n",
        "            result += model(image.to(device))\n",
        "        result /= times_pass_model\n",
        "        print()\n",
        "        return result.cpu()\n",
        "\n",
        "    elif type_combination == PredictionsCombinationType.MAX:\n",
        "        result = model(image.to(device))\n",
        "        for i in range(1, times_pass_model):\n",
        "            print(f'\\r\\tForward passing with max result {i+1}', end='')\n",
        "            result = torch.max(result, model(image.to(device)))\n",
        "        print()\n",
        "        return result.cpu()\n",
        "\n",
        "    else:\n",
        "        return model(image.to(device)).cpu()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C3Kep-DYKQx"
      },
      "source": [
        "## TFM Train\n",
        "\n",
        "Obtiene los parámetros que debe tener el modelo y entrena un modelo con esos parámetros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NZmYuWYaR4jA"
      },
      "outputs": [],
      "source": [
        "def TFMTrainModel(dataset_name: str, dropout_value: float, \n",
        "                  uses_redimension_vertical: bool, uses_redimension_horizontal: bool, \n",
        "                  save_train_info: bool, plot_epochs: bool\n",
        "                  ):\n",
        "    # Crear cache\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Generate path to save model\n",
        "    sae_file = getModelFileName(dataset_name=dataset_name, dropout_value=dropout_value, uses_redimension_vertical=uses_redimension_vertical, uses_redimension_horizontal=uses_redimension_horizontal)\n",
        "\n",
        "    path_checkpoint = f'{DRIVE_MODELS_FOLDER}/{sae_file}.pt'\n",
        "\n",
        "    train_loss_log_folder = getLogsTrainingTrainFolder (\n",
        "        dropout_value=dropout_value,\n",
        "        uses_redimension_vertical=uses_redimension_vertical,\n",
        "        uses_redimension_horizontal=uses_redimension_horizontal\n",
        "        )\n",
        "    val_loss_log_folder = getLogsTrainingValFolder (\n",
        "        dropout_value=dropout_value,\n",
        "        uses_redimension_vertical=uses_redimension_vertical,\n",
        "        uses_redimension_horizontal=uses_redimension_horizontal\n",
        "        )\n",
        "\n",
        "    train_loss_log_file = f'{train_loss_log_folder}/{sae_file}.md'\n",
        "    val_loss_log_file = f'{val_loss_log_folder}/{sae_file}.md'\n",
        "\n",
        "    print(f'Training {sae_file}')\n",
        "\n",
        "    if pathExist(val_loss_log_file) and pathExist(train_loss_log_file):\n",
        "      print('SKIPPED')\n",
        "      return\n",
        "\n",
        "    makeDirIfNotExist(DRIVE_MODELS_FOLDER)\n",
        "    makeDirIfNotExist(train_loss_log_folder)\n",
        "    makeDirIfNotExist(val_loss_log_folder)\n",
        "\n",
        "    # Create model\n",
        "    model = SAE()\n",
        "    model.to(device)\n",
        "\n",
        "    model.set_dropout_probability(dropout_probability=dropout_value)\n",
        "\n",
        "    # Create datasets to train and val\n",
        "    data_loader_train = generateTrainDatasetLoader (\n",
        "        dataset_name=dataset_name,\n",
        "        uses_redimension_vertical=uses_redimension_vertical,\n",
        "        uses_redimension_horizontal=uses_redimension_horizontal\n",
        "        )\n",
        "    data_loader_eval = generateValDatasetLoaderForTrain (\n",
        "        dataset_name=dataset_name,\n",
        "        uses_redimension_vertical=uses_redimension_vertical,\n",
        "        uses_redimension_horizontal=uses_redimension_horizontal\n",
        "        )\n",
        "\n",
        "    # Set losses\n",
        "    loss_function = torch.nn.MSELoss()\n",
        "    val_loss_function = torch.nn.MSELoss()\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # Params and optimizer\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1, weight_decay = 1e-8)\n",
        "    optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=1e-5, nesterov=True)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "\n",
        "\n",
        "    # Callbacks\n",
        "    modelCheckpoint = ModelCheckpoint(path_checkpoint, model)\n",
        "    earlyStopping = EarlyStopping(patience=TRAIN_PATIENTE, epochs=TRAIN_NUM_EPOCHS, checkpoint=modelCheckpoint, epsilon=0.00001)\n",
        "\n",
        "    # Num batches\n",
        "    num_batches_train = len(data_loader_train)\n",
        "    num_batches_val = len(data_loader_eval)\n",
        "\n",
        "\n",
        "    for epoch in range(TRAIN_NUM_EPOCHS):\n",
        "        if DEBUG_FLAG:\n",
        "            print(f'Epoch: {epoch+1}/{TRAIN_NUM_EPOCHS}')\n",
        "\n",
        "        ### TRAIN ONE EPOCH\n",
        "        # Set model to train\n",
        "        model.train()\n",
        "\n",
        "        epochTrainLoss = []\n",
        "        for iteration, batch in enumerate(data_loader_train):\n",
        "            image, info, targetImage = batch\n",
        "\n",
        "            if DEBUG_FLAG:\n",
        "                print(f'\\r\\tTraining Batch {iteration+1} of {num_batches_train} ({info[\"name\"]})', end='')\n",
        "\n",
        "            # Get the inputs and labels from the batch\n",
        "            image, targetImage = image.to(device), targetImage.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            reconstructed = model(image)\n",
        "            loss = loss_function(reconstructed, targetImage)\n",
        "            # reconstructed.detach(), image.detach(), targetImage.detach()\n",
        "\n",
        "            # Backward pass and update parameters\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Append training loss\n",
        "            epochTrainLoss.append(loss.item())\n",
        "\n",
        "        if DEBUG_FLAG:\n",
        "            print()\n",
        "\n",
        "        # Optimizer step\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # clear GPU memory usage\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        ### VALIDATE ONE EPOCH\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        epochValLoss = []\n",
        "        with torch.no_grad():\n",
        "            for iteration, batch in enumerate(data_loader_eval):\n",
        "                if DEBUG_FLAG:\n",
        "                    print(f'\\r\\tValidation Batch {iteration+1} of {num_batches_val}', end='')\n",
        "\n",
        "                # Get the inputs and labels from the batch\n",
        "                image, _, targetImage = batch\n",
        "                image, targetImage = image.to(device), targetImage.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                reconstructed = model(image)\n",
        "\n",
        "                # Get loss function\n",
        "                valLoss = val_loss_function(reconstructed,  targetImage)\n",
        "\n",
        "                # Save loss for single example\n",
        "                epochValLoss.append(valLoss.item())\n",
        "\n",
        "            if DEBUG_FLAG:\n",
        "                print()\n",
        "\n",
        "        # Losses\n",
        "        mean_train_loss = np.mean(epochTrainLoss)\n",
        "        mean_val_loss = np.mean(epochValLoss)\n",
        "\n",
        "        train_losses.append(mean_train_loss)\n",
        "        val_losses.append(mean_val_loss)\n",
        "\n",
        "        print(f'\\tEpoch {epoch}: \\t Train Loss - {round(mean_train_loss, 5)} | Val Loss - {round(mean_val_loss, 5)}')\n",
        "\n",
        "        # Update EarlyStopping\n",
        "        stopTraining = earlyStopping.update(mean_val_loss, epoch)\n",
        "\n",
        "        # clear GPU memory usage\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # If EarlyStopping says to stop, we stop training\n",
        "        if stopTraining:\n",
        "            print('Early Stopped!')\n",
        "            break\n",
        "\n",
        "    # Plot train val losses\n",
        "    if plot_epochs:\n",
        "        plotTrainval_losses(train_losses, val_losses)\n",
        "\n",
        "    if save_train_info:\n",
        "        # Save Log of the training\n",
        "        with open(train_loss_log_file, 'w') as f:\n",
        "            [f.write(f'{it}, {result}\\n') for it, result in enumerate(train_losses)]\n",
        "\n",
        "        with open(val_loss_log_file, 'w') as f:\n",
        "            [f.write(f'{it}, {result}\\n') for it, result in enumerate(val_losses)]\n",
        "\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0djNH8k0YKQx"
      },
      "source": [
        "# Validation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "infkka2hYKQx"
      },
      "outputs": [],
      "source": [
        "def TFMValidation(\n",
        "    dataset_name: str, dropout_value: float, val_dropout: float, \n",
        "    times_pass_model: int, type_combination: PredictionsCombinationType, \n",
        "    uses_redimension_vertical: bool, uses_redimension_horizontal: bool, \n",
        "    save_val_info: bool, save_val_imgs: bool\n",
        "    ):\n",
        "    # Crear cache\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Generate path of the saved model\n",
        "    sae_file = getModelFileName (\n",
        "        dataset_name=dataset_name,\n",
        "        dropout_value=dropout_value,\n",
        "        uses_redimension_vertical=uses_redimension_vertical,\n",
        "        uses_redimension_horizontal=uses_redimension_horizontal\n",
        "        )\n",
        "\n",
        "    # Generate folder to save this validation process\n",
        "    folder_validation = getLogsValidationFolder (\n",
        "        val_dropout=val_dropout,\n",
        "        times_pass_model=times_pass_model,\n",
        "        type_combination=type_combination\n",
        "        )\n",
        "\n",
        "    makeDirIfNotExist(folder_validation)\n",
        "\n",
        "    path_model = f'{DRIVE_MODELS_FOLDER}/{sae_file}.pt'\n",
        "\n",
        "    val_best_bin_log_folder = f'{folder_validation}/bestBin'\n",
        "    val_f1_log_folder = f'{folder_validation}/valF1'\n",
        "    val_iou_log_folder = f'{folder_validation}/valIoU'\n",
        "\n",
        "    val_best_bin_log_file = f'{val_best_bin_log_folder}/{sae_file}.bestBin'\n",
        "    val_f1_log_file = f'{val_f1_log_folder}/{sae_file}.valF1'\n",
        "    val_iou_log_file = f'{val_iou_log_folder}/{sae_file}.valIoU'\n",
        "\n",
        "    makeDirIfNotExist(val_best_bin_log_folder)\n",
        "    makeDirIfNotExist(val_f1_log_folder)\n",
        "    makeDirIfNotExist(val_iou_log_folder)\n",
        "\n",
        "    val_img_folder = getImgsValidationFolder (\n",
        "        val_dropout=val_dropout,\n",
        "        times_pass_model=times_pass_model,\n",
        "        type_combination=type_combination,\n",
        "        dataset_name=dataset_name,\n",
        "        model_name=sae_file\n",
        "        )\n",
        "\n",
        "    print(f'Evaluating {sae_file}')\n",
        "\n",
        "    # Create model\n",
        "    model: SAE = torch.load(path_model, map_location=torch.device(device))\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    if val_dropout > 0:\n",
        "        model.enable_eval_dropout()\n",
        "        model.set_dropout_probability(dropout_probability=val_dropout)\n",
        "\n",
        "    # Create datasets to train and val\n",
        "    data_loader_eval = generateValDatasetLoaderForTest (\n",
        "        dataset_name=dataset_name\n",
        "        )\n",
        "\n",
        "\n",
        "    # Evaluation\n",
        "    bin_F1score_map = {i: 0.0 for i in BIN_UMBRALS}\n",
        "    bin_IoUscore_map = {i: 0.0 for i in BIN_UMBRALS}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Iterate over each example of the eval dataset\n",
        "        for iteration, batch in enumerate(data_loader_eval):\n",
        "            if DEBUG_FLAG:\n",
        "                print(f'Eval with batch {iteration}/{len(data_loader_eval)}')\n",
        "\n",
        "            # Get the inputs and labels from the batch\n",
        "            image, target = batch\n",
        "            targetBoxes = target.squeeze().numpy().tolist()\n",
        "\n",
        "            # Forward pass\n",
        "            result = forwardToModel(model=model,\n",
        "                                    image=image,\n",
        "                                    times_pass_model=times_pass_model,\n",
        "                                    type_combination=type_combination\n",
        "                                    )\n",
        "\n",
        "            # Creates a dictionary with the bin thresholds and F1 score for each\n",
        "            for bin_umbral in BIN_UMBRALS:\n",
        "                if DEBUG_FLAG:\n",
        "                    print(f'\\r\\tTesting with {bin_umbral} binarization umbral', end='')\n",
        "\n",
        "                # Extract BB from prediction\n",
        "                boxes = getConnectedComponents(result, bin_threshold_percentaje=bin_umbral)\n",
        "                if uses_redimension_vertical or uses_redimension_horizontal:  # If we're using a resized BB, resize it to original\n",
        "                    vResize = BBOX_REDIMENSIONED_RECOVER if uses_redimension_vertical   else 1\n",
        "                    hResize = BBOX_REDIMENSIONED_RECOVER if uses_redimension_horizontal else 1\n",
        "                    boxes = [resize_box(box, vResize=vResize, hResize=hResize) for box in boxes]\n",
        "\n",
        "                # Calculate F1 and IoU\n",
        "                f1, matched_ious = calculate_F1(y_true = targetBoxes, y_pred = boxes, iou_threshold=0.5)\n",
        "\n",
        "                # Accumulate F1 and IoU score in the bin umbral used for this concrete example\n",
        "                bin_F1score_map[bin_umbral]  += f1\n",
        "                bin_IoUscore_map[bin_umbral] += np.mean(matched_ious) if len(matched_ious) > 0 else 0\n",
        "\n",
        "            if DEBUG_FLAG:\n",
        "                print()\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            # gc.collect()\n",
        "\n",
        "\n",
        "    # Calculate mean of F1 and IoU scores\n",
        "    number_elements = len(data_loader_eval)\n",
        "    for key in bin_F1score_map:\n",
        "        bin_F1score_map[key] /= number_elements\n",
        "\n",
        "    for key in bin_IoUscore_map:\n",
        "        bin_IoUscore_map[key] /= number_elements\n",
        "\n",
        "    # Calculate mean of each bin theshold and decide max\n",
        "    best_bin_threshold = max(bin_F1score_map, key=bin_F1score_map.get)\n",
        "\n",
        "    # Check for dupes in best F1 score\n",
        "    common_keys = []\n",
        "    for key, value in bin_F1score_map.items():\n",
        "        if value == bin_F1score_map[best_bin_threshold]:\n",
        "            common_keys.append(key)\n",
        "\n",
        "    # If there's dupes, select the one with most IoU\n",
        "    if len(common_keys) > 1:\n",
        "        for k in common_keys:\n",
        "            best_bin_threshold = best_bin_threshold if bin_IoUscore_map[best_bin_threshold] > bin_IoUscore_map[k] else k\n",
        "\n",
        "\n",
        "    # Create the \"best umbral\" info message\n",
        "    stringBestMsg = f'Evaluated {sae_file}: \\t Bin Threshold {best_bin_threshold} --> F1 - {bin_F1score_map[best_bin_threshold]} | IoU - {bin_IoUscore_map[best_bin_threshold]}'\n",
        "    print(stringBestMsg)\n",
        "\n",
        "    # Save in a file the F1 and IoU metrics\n",
        "    if save_val_info:\n",
        "        with open(val_best_bin_log_file, 'w') as f:\n",
        "            f.write(f'{best_bin_threshold}\\n{stringBestMsg}')\n",
        "\n",
        "        stringF1List = '\\n'.join([f' {round(key, 2)} {value}' for key, value in bin_F1score_map.items()])\n",
        "        with open(val_f1_log_file, 'w') as f:\n",
        "            f.write(stringF1List)\n",
        "\n",
        "        stringIoUList = '\\n'.join([f' {round(key, 2)} {value}' for key, value in bin_IoUscore_map.items()])\n",
        "        with open(val_iou_log_file, 'w') as f:\n",
        "            f.write(stringIoUList)\n",
        "\n",
        "    return best_bin_threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TFM Forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getPredictionModel(dataset_name):\n",
        "  model = torch.load(f'SAE_{dataset_name}.pt', map_location=torch.device(device))\n",
        "  model.to(device)\n",
        "  return model\n",
        "\n",
        "def TFMForward(\n",
        "    dataset_name, image: PILImage, uses_redimension_vertical: bool, uses_redimension_horizontal: bool, \n",
        "    bin_umbral: float, val_dropout: float, times_pass_model: int, type_combination: PredictionsCombinationType\n",
        "    ):\n",
        "    \"\"\"Forward de una imagen para obtener los bounding boxes\n",
        "\n",
        "    Args:\n",
        "        dataset_name (string): Nombre del dataset\n",
        "        image (Image): Imagen a hacer forward\n",
        "        uses_redimension_vertical (bool): Si el modelo usa redimensión vertical\n",
        "        uses_redimension_horizontal (bool): Si el modelo usa redimensión horizontal\n",
        "        bin_umbral (float): Umbral de binarización\n",
        "        val_dropout (float): Dropout a aplicar en prediccion\n",
        "        times_pass_model (int): Cantidad de predicciones a combinar en prediccion\n",
        "        type_combination (PredictionsCombinationType): Tipo de combinacion a aplicar sobre las predicciones\n",
        "\n",
        "    Returns:\n",
        "        list: Bounding boxes extraídas de la imagen\n",
        "    \"\"\"\n",
        "    # Create model\n",
        "    model = getPredictionModel(dataset_name=dataset_name)\n",
        "\n",
        "    if val_dropout > 0:\n",
        "        model.enable_eval_dropout()\n",
        "        model.set_dropout_probability(dropout_probability=val_dropout)\n",
        "    \n",
        "    image = image.resize(SAE_IMAGE_SIZE)\n",
        "    \n",
        "    transforms = get_transform()\n",
        "    transformedImage = transforms(image)\n",
        "    \n",
        "    with T.no_grad():\n",
        "        result = forwardToModel(model=model,\n",
        "                                image=transformedImage,\n",
        "                                times_pass_model=times_pass_model,\n",
        "                                type_combination=type_combination\n",
        "                                )\n",
        "\n",
        "        boxes = getConnectedComponents(result, bin_threshold_percentaje=bin_umbral)\n",
        "\n",
        "        if uses_redimension_vertical or uses_redimension_horizontal:\n",
        "            vResize = BBOX_REDIMENSIONED_RECOVER if uses_redimension_vertical   else 1\n",
        "            hResize = BBOX_REDIMENSIONED_RECOVER if uses_redimension_horizontal else 1\n",
        "            boxes = [resize_box(box, vResize=vResize, hResize=hResize) for box in boxes]\n",
        "\n",
        "    return boxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEr6hgTS20tf"
      },
      "source": [
        "# Test functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vcsNhk7j9fax"
      },
      "outputs": [],
      "source": [
        "def TFMTest(\n",
        "    dataset_name: str, dropout_value: float, val_dropout: float, \n",
        "    times_pass_model: int, type_combination: PredictionsCombinationType,\n",
        "    uses_redimension_vertical: float, uses_redimension_horizontal: float, \n",
        "    save_test_info: bool, save_test_img: bool\n",
        "    ):\n",
        "    # Crear cache\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Generate path of the saved model\n",
        "    sae_file = getModelFileName (\n",
        "        dataset_name=dataset_name,\n",
        "        dropout_value=dropout_value,\n",
        "        uses_redimension_vertical=uses_redimension_vertical,\n",
        "        uses_redimension_horizontal=uses_redimension_horizontal\n",
        "        )\n",
        "\n",
        "    # Get folder to obtain best bin threshold\n",
        "    folder_validation = getLogsValidationFolder (\n",
        "        val_dropout=val_dropout,\n",
        "        times_pass_model=times_pass_model,\n",
        "        type_combination=type_combination\n",
        "        )\n",
        "\n",
        "    val_best_bin_log_file = f'{folder_validation}/bestBin/{sae_file}.bestBin'\n",
        "    bin_umbral_for_model = 0\n",
        "    with open(val_best_bin_log_file, 'r') as file:\n",
        "        bin_umbral_for_model = float(file.readline())\n",
        "\n",
        "    folder_test = getLogsTestFolder (\n",
        "        val_dropout=val_dropout,\n",
        "        times_pass_model=times_pass_model,\n",
        "        type_combination=type_combination\n",
        "        )\n",
        "\n",
        "    test_img_folder = getImgsValidationFolder (\n",
        "        val_dropout=val_dropout,\n",
        "        times_pass_model=times_pass_model,\n",
        "        type_combination=type_combination,\n",
        "        dataset_name=dataset_name,\n",
        "        model_name=sae_file\n",
        "        )\n",
        "\n",
        "    print(f'Testing model {sae_file} with Bin umbral {bin_umbral_for_model}')\n",
        "\n",
        "    makeDirIfNotExist(folder_test)\n",
        "    makeDirIfNotExist(test_img_folder)\n",
        "\n",
        "    path_model = f'{DRIVE_MODELS_FOLDER}/{sae_file}.pt'\n",
        "\n",
        "    test_best_bin_log_file = f'{folder_test}/{sae_file}.test'\n",
        "\n",
        "    # Create model\n",
        "    model: SAE = torch.load(path_model, map_location=torch.device(device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    if val_dropout > 0:\n",
        "        model.enable_eval_dropout()\n",
        "        model.set_dropout_probability(dropout_probability=val_dropout)\n",
        "\n",
        "    # Create datasets to train and val\n",
        "    data_loader_test = generateTestDatasetLoader (\n",
        "        dataset_name=dataset_name\n",
        "        )\n",
        "\n",
        "\n",
        "    # Evaluation\n",
        "    bin_F1score_sum = 0\n",
        "    bin_IoUscore_sum = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Iterate over each example of the eval dataset\n",
        "        for iteration, batch in enumerate(data_loader_test):\n",
        "            if DEBUG_FLAG:\n",
        "                print(f'Eval with batch {iteration}/{len(data_loader_test)}')\n",
        "\n",
        "            # Get the inputs and labels from the batch\n",
        "            image, target = batch\n",
        "            targetBoxes = target.squeeze().numpy().tolist()\n",
        "\n",
        "            # Forward pass\n",
        "            result = forwardToModel(model=model,\n",
        "                                    image=image,\n",
        "                                    times_pass_model=times_pass_model,\n",
        "                                    type_combination=type_combination\n",
        "                                    )\n",
        "\n",
        "            # Creates a dictionary with the bin thresholds and F1 score for each\n",
        "            if DEBUG_FLAG:\n",
        "                print(f'\\r\\tTesting with {bin_umbral_for_model} binarization umbral', end='')\n",
        "\n",
        "            # Extract BB from prediction\n",
        "            boxes = getConnectedComponents(result, bin_threshold_percentaje=bin_umbral_for_model)\n",
        "            if uses_redimension_vertical or uses_redimension_horizontal:  # If we're using a resized BB, resize it to original\n",
        "                vResize = BBOX_REDIMENSIONED_RECOVER if uses_redimension_vertical   else 1\n",
        "                hResize = BBOX_REDIMENSIONED_RECOVER if uses_redimension_horizontal else 1\n",
        "                boxes = [resize_box(box, vResize=vResize, hResize=hResize) for box in boxes]\n",
        "\n",
        "            # Calculate F1 and IoU\n",
        "            f1, matched_ious = calculate_F1(y_true=targetBoxes, y_pred=boxes, iou_threshold=0.5)\n",
        "\n",
        "            # Accumulate F1 and IoU score in the bin umbral used for this concrete example\n",
        "            bin_F1score_sum  += f1\n",
        "            bin_IoUscore_sum += np.mean(matched_ious) if len(matched_ious) > 0 else 0\n",
        "\n",
        "            if save_test_img:\n",
        "                drawBoxesPredictedAndGroundTruth (\n",
        "                    tensor_image=image,\n",
        "                    bboxes_predicted=boxes,\n",
        "                    bboxes_groundtruth=targetBoxes,\n",
        "                    is_normalized=False,\n",
        "                    image_name=f'{test_img_folder}/{iteration}.png',\n",
        "                    plot=False,\n",
        "                    save=save_test_img\n",
        "                    )\n",
        "\n",
        "            if DEBUG_FLAG:\n",
        "                print()\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            # gc.collect()\n",
        "\n",
        "\n",
        "    # Calculate mean of F1 and IoU scores\n",
        "    number_elements = len(data_loader_test)\n",
        "\n",
        "    bin_F1score_sum /= number_elements\n",
        "    bin_IoUscore_sum /= number_elements\n",
        "\n",
        "    print(f'Tested {sae_file}: \\t Bin Umbral {bin_umbral_for_model}, Combination {type_combination.value}, Times {times_pass_model} --> F1 - {bin_F1score_sum} | IoU - {bin_IoUscore_sum}')\n",
        "\n",
        "    # Save in a file the F1 and IoU metrics\n",
        "    if save_test_info:\n",
        "        with open(test_best_bin_log_file, 'w') as f:\n",
        "            f.write(f'F1: {bin_F1score_sum}\\nIoU: {bin_IoUscore_sum}\\n Test model {sae_file} with Bin Umbral {bin_umbral_for_model}, Combination {type_combination.value}, Times {times_pass_model}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCOYoJH2R4jB"
      },
      "source": [
        "# TFM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFgo62RAR4jC"
      },
      "source": [
        "## Paso 1\n",
        "\n",
        "Recreamos el SAE sin Dropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaYBmOVzR4jC"
      },
      "source": [
        "## Paso 2\n",
        "\n",
        "Probar redimensión de pentagramas con los 3 datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx1ubdn1R4jC"
      },
      "source": [
        "Entrenamos 4 modelos sin Dropout: uno sin redimensión, otro con redimensión vertical, otro con redimensión horizontal y otro con redimensión vertical y horizontal. Entrenamos para los 3 Datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXki7krkR4jC",
        "outputId": "bee50ca7-8fa9-46cd-d94c-beaad50bc037"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training SAE_D0_Capitan\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "[Errno 30] Read-only file system: '/content'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m uses_redimension_vertical, uses_redimension_horizontal \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(usesRedimensionVerticalList, usesRedimensionHorizontalList):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dataset_name \u001b[38;5;129;01min\u001b[39;00m DATASETS:\n\u001b[0;32m---> 12\u001b[0m         \u001b[43mTFMTrainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mdropout_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                      \u001b[49m\u001b[43muses_redimension_vertical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muses_redimension_vertical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                      \u001b[49m\u001b[43muses_redimension_horizontal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muses_redimension_horizontal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                      \u001b[49m\u001b[43msave_train_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_train_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mplot_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot_epochs\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[17], line 31\u001b[0m, in \u001b[0;36mTFMTrainModel\u001b[0;34m(dataset_name, dropout_value, uses_redimension_vertical, uses_redimension_horizontal, save_train_info, plot_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSKIPPED\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mmakeDirIfNotExist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDRIVE_MODELS_FOLDER\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m makeDirIfNotExist(train_loss_log_folder)\n\u001b[1;32m     33\u001b[0m makeDirIfNotExist(val_loss_log_folder)\n",
            "Cell \u001b[0;32mIn[10], line 148\u001b[0m, in \u001b[0;36mmakeDirIfNotExist\u001b[0;34m(dir_path)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmakeDirIfNotExist\u001b[39m(dir_path):\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dir_path):\n\u001b[0;32m--> 148\u001b[0m         \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_path\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
            "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
            "    \u001b[0;31m[... skipping similar frames: makedirs at line 215 (2 times)]\u001b[0m\n",
            "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
            "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/content'"
          ]
        }
      ],
      "source": [
        "# Models parameters\n",
        "dropout_value = 0\n",
        "save_train_info = True\n",
        "plot_epochs = False\n",
        "\n",
        "                                # Mod1,  Mod2,  Mod3,  Mod4\n",
        "usesRedimensionVerticalList   = [False, False, True , True]\n",
        "usesRedimensionHorizontalList = [False, True , False, True]\n",
        "\n",
        "for uses_redimension_vertical, uses_redimension_horizontal in zip(usesRedimensionVerticalList, usesRedimensionHorizontalList):\n",
        "    for dataset_name in DATASETS:\n",
        "        TFMTrainModel(dataset_name=dataset_name,\n",
        "                      dropout_value=dropout_value,\n",
        "                      uses_redimension_vertical=uses_redimension_vertical,\n",
        "                      uses_redimension_horizontal=uses_redimension_horizontal,\n",
        "                      save_train_info=save_train_info,\n",
        "                      plot_epochs=plot_epochs\n",
        "                      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPnuApzqR4jC"
      },
      "source": [
        "Extraemos el mejor umbral de binarización para cada modelo entrenado y obtenemos los resultados con el conjunto de validación. Fijamos la redimensión al mejor obtenido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "NqQIb6nBYKQ3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating SAE_D0_Capitan\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/rx/htv12nn55bq6w7t016qbx0d80000gn/T/ipykernel_35230/2279567646.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(path_model, map_location=torch.device(device))\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/TFM/IA/models/SAE_D0_Capitan.pt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m uses_redimension_vertical, uses_redimension_horizontal \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(usesRedimensionVerticalList, usesRedimensionHorizontalList):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dataset_name \u001b[38;5;129;01min\u001b[39;00m DATASETS:\n\u001b[0;32m---> 15\u001b[0m         \u001b[43mTFMValidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mdropout_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mval_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mtimes_pass_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimes_pass_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mtype_combination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_combination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                      \u001b[49m\u001b[43muses_redimension_vertical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muses_redimension_vertical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                      \u001b[49m\u001b[43muses_redimension_horizontal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muses_redimension_horizontal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                      \u001b[49m\u001b[43msave_val_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_val_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                      \u001b[49m\u001b[43msave_val_imgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_val_imgs\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[24], line 48\u001b[0m, in \u001b[0;36mTFMValidation\u001b[0;34m(dataset_name, dropout_value, val_dropout, times_pass_model, type_combination, uses_redimension_vertical, uses_redimension_horizontal, save_val_info, save_val_imgs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msae_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Create model\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     51\u001b[0m model\u001b[38;5;241m.\u001b[39mset_dropout_probability(dropout_probability\u001b[38;5;241m=\u001b[39mdropout_value)\n",
            "File \u001b[0;32m~/Desktop/MasterMoviles/TFM/TFM_Api/.venv/lib/python3.12/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
            "File \u001b[0;32m~/Desktop/MasterMoviles/TFM/TFM_Api/.venv/lib/python3.12/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
            "File \u001b[0;32m~/Desktop/MasterMoviles/TFM/TFM_Api/.venv/lib/python3.12/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/TFM/IA/models/SAE_D0_Capitan.pt'"
          ]
        }
      ],
      "source": [
        "# Models parameters\n",
        "dropout_value = 0\n",
        "save_val_info = True\n",
        "save_val_imgs = True\n",
        "val_dropout = 0\n",
        "times_pass_model = 1\n",
        "type_combination = PredictionsCombinationType.NONE\n",
        "\n",
        "                                # Mod1,  Mod2,  Mod3,  Mod4\n",
        "usesRedimensionVerticalList   = [False, False, True , True]\n",
        "usesRedimensionHorizontalList = [False, True , False, True]\n",
        "\n",
        "for uses_redimension_vertical, uses_redimension_horizontal in zip(usesRedimensionVerticalList, usesRedimensionHorizontalList):\n",
        "    for dataset_name in DATASETS:\n",
        "        TFMValidation(dataset_name=dataset_name,\n",
        "                      dropout_value=dropout_value,\n",
        "                      val_dropout=val_dropout,\n",
        "                      times_pass_model=times_pass_model,\n",
        "                      type_combination=type_combination,\n",
        "                      uses_redimension_vertical=uses_redimension_vertical,\n",
        "                      uses_redimension_horizontal=uses_redimension_horizontal,\n",
        "                      save_val_info=save_val_info,\n",
        "                      save_val_imgs=save_val_imgs\n",
        "                      )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0TPnKyv9fa1"
      },
      "source": [
        "Con el test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JMrXqa89fa1"
      },
      "outputs": [],
      "source": [
        "# Models parameters\n",
        "dropout_value = 0\n",
        "save_test_info = True\n",
        "save_test_img = True\n",
        "val_dropout = 0\n",
        "times_pass_model = 1\n",
        "type_combination = PredictionsCombinationType.NONE\n",
        "\n",
        "                                # Mod1,  Mod2,  Mod3,  Mod4\n",
        "usesRedimensionVerticalList   = [False, False, True , True]\n",
        "usesRedimensionHorizontalList = [False, True , False, True]\n",
        "\n",
        "for uses_redimension_vertical, uses_redimension_horizontal in zip(usesRedimensionVerticalList, usesRedimensionHorizontalList):\n",
        "    for dataset_name in DATASETS:\n",
        "        TFMTest(dataset_name=dataset_name,\n",
        "                      dropout_value=dropout_value,\n",
        "                      val_dropout=val_dropout,\n",
        "                      times_pass_model=times_pass_model,\n",
        "                      type_combination=type_combination,\n",
        "                      uses_redimension_vertical=uses_redimension_vertical,\n",
        "                      uses_redimension_horizontal=uses_redimension_horizontal,\n",
        "                      save_test_info=save_test_info,\n",
        "                      save_test_img=save_test_img\n",
        "                      )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHKeYDrJR4jC"
      },
      "source": [
        "## Paso 3\n",
        "\n",
        "Probar Dropout en entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBzGEoCCR4jD"
      },
      "source": [
        "Entrenamos 6 modelos, usando Dropout desde 0 hasta 0'5, en entrenamiento. Entrenamos para los 3 Datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVwqMNOPYKQ3",
        "outputId": "8082cea1-6ee7-4ab0-df10-a662e400d1d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training SAE_D0_RVH_Capitan\n",
            "SKIPPED\n",
            "Training SAE_D0_RVH_SEILS\n",
            "SKIPPED\n",
            "Training SAE_D0_RVH_FMT_C\n",
            "SKIPPED\n",
            "Training SAE_D1_RVH_Capitan\n",
            "SKIPPED\n",
            "Training SAE_D1_RVH_SEILS\n",
            "SKIPPED\n",
            "Training SAE_D1_RVH_FMT_C\n",
            "\tEpoch 0: \t Train Loss - 0.09089 | Val Loss - 0.10445\n",
            "\tEarly stopping: [1/20] on epoch 0/100\n",
            "\tEpoch 1: \t Train Loss - 0.066 | Val Loss - 0.06128\n",
            "\tEarly stopping: [1/20] on epoch 1/100\n",
            "\tEpoch 2: \t Train Loss - 0.05973 | Val Loss - 0.06309\n",
            "\tEarly stopping: [2/20] on epoch 2/100\n",
            "\tEpoch 3: \t Train Loss - 0.05557 | Val Loss - 0.05959\n",
            "\tEarly stopping: [1/20] on epoch 3/100\n",
            "\tEpoch 4: \t Train Loss - 0.05419 | Val Loss - 0.05746\n",
            "\tEarly stopping: [1/20] on epoch 4/100\n",
            "\tEpoch 5: \t Train Loss - 0.05367 | Val Loss - 0.05733\n",
            "\tEarly stopping: [1/20] on epoch 5/100\n",
            "\tEpoch 6: \t Train Loss - 0.05304 | Val Loss - 0.05731\n",
            "\tEarly stopping: [1/20] on epoch 6/100\n",
            "\tEpoch 7: \t Train Loss - 0.05178 | Val Loss - 0.05755\n",
            "\tEarly stopping: [2/20] on epoch 7/100\n",
            "\tEpoch 8: \t Train Loss - 0.05253 | Val Loss - 0.05756\n",
            "\tEarly stopping: [3/20] on epoch 8/100\n",
            "\tEpoch 9: \t Train Loss - 0.05176 | Val Loss - 0.0575\n",
            "\tEarly stopping: [4/20] on epoch 9/100\n",
            "\tEpoch 10: \t Train Loss - 0.05219 | Val Loss - 0.05758\n",
            "\tEarly stopping: [5/20] on epoch 10/100\n",
            "\tEpoch 11: \t Train Loss - 0.05192 | Val Loss - 0.05734\n",
            "\tEarly stopping: [6/20] on epoch 11/100\n",
            "\tEpoch 12: \t Train Loss - 0.05196 | Val Loss - 0.0577\n",
            "\tEarly stopping: [7/20] on epoch 12/100\n",
            "\tEpoch 13: \t Train Loss - 0.05221 | Val Loss - 0.05779\n",
            "\tEarly stopping: [8/20] on epoch 13/100\n",
            "\tEpoch 14: \t Train Loss - 0.05218 | Val Loss - 0.05755\n",
            "\tEarly stopping: [9/20] on epoch 14/100\n",
            "\tEpoch 15: \t Train Loss - 0.05172 | Val Loss - 0.05722\n",
            "\tEarly stopping: [1/20] on epoch 15/100\n",
            "\tEpoch 16: \t Train Loss - 0.0516 | Val Loss - 0.05779\n",
            "\tEarly stopping: [2/20] on epoch 16/100\n",
            "\tEpoch 17: \t Train Loss - 0.05179 | Val Loss - 0.05771\n",
            "\tEarly stopping: [3/20] on epoch 17/100\n",
            "\tEpoch 18: \t Train Loss - 0.05195 | Val Loss - 0.05739\n",
            "\tEarly stopping: [4/20] on epoch 18/100\n",
            "\tEpoch 19: \t Train Loss - 0.05174 | Val Loss - 0.05805\n",
            "\tEarly stopping: [5/20] on epoch 19/100\n",
            "\tEpoch 20: \t Train Loss - 0.0511 | Val Loss - 0.0581\n",
            "\tEarly stopping: [6/20] on epoch 20/100\n",
            "\tEpoch 21: \t Train Loss - 0.05153 | Val Loss - 0.05735\n",
            "\tEarly stopping: [7/20] on epoch 21/100\n",
            "\tEpoch 22: \t Train Loss - 0.05075 | Val Loss - 0.05759\n",
            "\tEarly stopping: [8/20] on epoch 22/100\n",
            "\tEpoch 23: \t Train Loss - 0.05129 | Val Loss - 0.05768\n",
            "\tEarly stopping: [9/20] on epoch 23/100\n",
            "\tEpoch 24: \t Train Loss - 0.05189 | Val Loss - 0.05795\n",
            "\tEarly stopping: [10/20] on epoch 24/100\n",
            "\tEpoch 25: \t Train Loss - 0.0515 | Val Loss - 0.05777\n",
            "\tEarly stopping: [11/20] on epoch 25/100\n",
            "\tEpoch 26: \t Train Loss - 0.05218 | Val Loss - 0.05738\n",
            "\tEarly stopping: [12/20] on epoch 26/100\n",
            "\tEpoch 27: \t Train Loss - 0.0528 | Val Loss - 0.05734\n",
            "\tEarly stopping: [13/20] on epoch 27/100\n",
            "\tEpoch 28: \t Train Loss - 0.05261 | Val Loss - 0.05764\n",
            "\tEarly stopping: [14/20] on epoch 28/100\n",
            "\tEpoch 29: \t Train Loss - 0.05127 | Val Loss - 0.05761\n",
            "\tEarly stopping: [15/20] on epoch 29/100\n",
            "\tEpoch 30: \t Train Loss - 0.05238 | Val Loss - 0.05768\n",
            "\tEarly stopping: [16/20] on epoch 30/100\n",
            "\tEpoch 31: \t Train Loss - 0.05174 | Val Loss - 0.05782\n",
            "\tEarly stopping: [17/20] on epoch 31/100\n",
            "\tEpoch 32: \t Train Loss - 0.05185 | Val Loss - 0.05806\n",
            "\tEarly stopping: [18/20] on epoch 32/100\n",
            "\tEpoch 33: \t Train Loss - 0.05143 | Val Loss - 0.05747\n",
            "\tEarly stopping: [19/20] on epoch 33/100\n",
            "\tEpoch 34: \t Train Loss - 0.05229 | Val Loss - 0.05769\n",
            "\tEarly stopping: [20/20] on epoch 34/100\n",
            "Early Stopped!\n",
            "Training SAE_D2_RVH_Capitan\n",
            "\tEpoch 0: \t Train Loss - 0.10607 | Val Loss - 0.04399\n",
            "\tEarly stopping: [1/20] on epoch 0/100\n",
            "\tEpoch 1: \t Train Loss - 0.05319 | Val Loss - 0.04285\n",
            "\tEarly stopping: [1/20] on epoch 1/100\n",
            "\tEpoch 2: \t Train Loss - 0.04016 | Val Loss - 0.04205\n",
            "\tEarly stopping: [1/20] on epoch 2/100\n",
            "\tEpoch 3: \t Train Loss - 0.03258 | Val Loss - 0.03814\n",
            "\tEarly stopping: [1/20] on epoch 3/100\n",
            "\tEpoch 4: \t Train Loss - 0.03082 | Val Loss - 0.03711\n",
            "\tEarly stopping: [1/20] on epoch 4/100\n",
            "\tEpoch 5: \t Train Loss - 0.03033 | Val Loss - 0.03747\n",
            "\tEarly stopping: [2/20] on epoch 5/100\n",
            "\tEpoch 6: \t Train Loss - 0.03012 | Val Loss - 0.03892\n",
            "\tEarly stopping: [3/20] on epoch 6/100\n",
            "\tEpoch 7: \t Train Loss - 0.0295 | Val Loss - 0.03823\n",
            "\tEarly stopping: [4/20] on epoch 7/100\n",
            "\tEpoch 8: \t Train Loss - 0.02926 | Val Loss - 0.03784\n",
            "\tEarly stopping: [5/20] on epoch 8/100\n",
            "\tEpoch 9: \t Train Loss - 0.03068 | Val Loss - 0.0367\n",
            "\tEarly stopping: [1/20] on epoch 9/100\n",
            "\tEpoch 10: \t Train Loss - 0.02957 | Val Loss - 0.03769\n",
            "\tEarly stopping: [2/20] on epoch 10/100\n",
            "\tEpoch 11: \t Train Loss - 0.03005 | Val Loss - 0.03742\n",
            "\tEarly stopping: [3/20] on epoch 11/100\n",
            "\tEpoch 12: \t Train Loss - 0.02976 | Val Loss - 0.03813\n",
            "\tEarly stopping: [4/20] on epoch 12/100\n",
            "\tEpoch 13: \t Train Loss - 0.03023 | Val Loss - 0.03776\n",
            "\tEarly stopping: [5/20] on epoch 13/100\n",
            "\tEpoch 14: \t Train Loss - 0.03115 | Val Loss - 0.03801\n",
            "\tEarly stopping: [6/20] on epoch 14/100\n",
            "\tEpoch 15: \t Train Loss - 0.02947 | Val Loss - 0.03838\n",
            "\tEarly stopping: [7/20] on epoch 15/100\n",
            "\tEpoch 16: \t Train Loss - 0.03012 | Val Loss - 0.03808\n",
            "\tEarly stopping: [8/20] on epoch 16/100\n",
            "\tEpoch 17: \t Train Loss - 0.03015 | Val Loss - 0.03955\n",
            "\tEarly stopping: [9/20] on epoch 17/100\n",
            "\tEpoch 18: \t Train Loss - 0.03062 | Val Loss - 0.03835\n",
            "\tEarly stopping: [10/20] on epoch 18/100\n",
            "\tEpoch 19: \t Train Loss - 0.03073 | Val Loss - 0.03795\n",
            "\tEarly stopping: [11/20] on epoch 19/100\n",
            "\tEpoch 20: \t Train Loss - 0.02924 | Val Loss - 0.0371\n",
            "\tEarly stopping: [12/20] on epoch 20/100\n",
            "\tEpoch 21: \t Train Loss - 0.03127 | Val Loss - 0.03746\n",
            "\tEarly stopping: [13/20] on epoch 21/100\n",
            "\tEpoch 22: \t Train Loss - 0.03053 | Val Loss - 0.0375\n",
            "\tEarly stopping: [14/20] on epoch 22/100\n",
            "\tEpoch 23: \t Train Loss - 0.03016 | Val Loss - 0.03719\n",
            "\tEarly stopping: [15/20] on epoch 23/100\n",
            "\tEpoch 24: \t Train Loss - 0.02981 | Val Loss - 0.03883\n",
            "\tEarly stopping: [16/20] on epoch 24/100\n",
            "\tEpoch 25: \t Train Loss - 0.02949 | Val Loss - 0.03703\n",
            "\tEarly stopping: [17/20] on epoch 25/100\n",
            "\tEpoch 26: \t Train Loss - 0.03058 | Val Loss - 0.03701\n",
            "\tEarly stopping: [18/20] on epoch 26/100\n",
            "\tEpoch 27: \t Train Loss - 0.0297 | Val Loss - 0.03826\n",
            "\tEarly stopping: [19/20] on epoch 27/100\n",
            "\tEpoch 28: \t Train Loss - 0.0305 | Val Loss - 0.03761\n",
            "\tEarly stopping: [20/20] on epoch 28/100\n",
            "Early Stopped!\n",
            "Training SAE_D2_RVH_SEILS\n",
            "\tEpoch 0: \t Train Loss - 0.04426 | Val Loss - 0.0314\n",
            "\tEarly stopping: [1/20] on epoch 0/100\n",
            "\tEpoch 1: \t Train Loss - 0.0247 | Val Loss - 0.02354\n",
            "\tEarly stopping: [1/20] on epoch 1/100\n",
            "\tEpoch 2: \t Train Loss - 0.02313 | Val Loss - 0.02162\n",
            "\tEarly stopping: [1/20] on epoch 2/100\n",
            "\tEpoch 3: \t Train Loss - 0.02195 | Val Loss - 0.02157\n",
            "\tEarly stopping: [1/20] on epoch 3/100\n",
            "\tEpoch 4: \t Train Loss - 0.02216 | Val Loss - 0.02196\n",
            "\tEarly stopping: [2/20] on epoch 4/100\n",
            "\tEpoch 5: \t Train Loss - 0.02207 | Val Loss - 0.02094\n",
            "\tEarly stopping: [1/20] on epoch 5/100\n",
            "\tEpoch 6: \t Train Loss - 0.02184 | Val Loss - 0.02112\n",
            "\tEarly stopping: [2/20] on epoch 6/100\n",
            "\tEpoch 7: \t Train Loss - 0.02182 | Val Loss - 0.02191\n",
            "\tEarly stopping: [3/20] on epoch 7/100\n",
            "\tEpoch 8: \t Train Loss - 0.02191 | Val Loss - 0.02147\n",
            "\tEarly stopping: [4/20] on epoch 8/100\n",
            "\tEpoch 9: \t Train Loss - 0.02193 | Val Loss - 0.02112\n",
            "\tEarly stopping: [5/20] on epoch 9/100\n",
            "\tEpoch 10: \t Train Loss - 0.02136 | Val Loss - 0.02102\n",
            "\tEarly stopping: [6/20] on epoch 10/100\n",
            "\tEpoch 11: \t Train Loss - 0.02158 | Val Loss - 0.0209\n",
            "\tEarly stopping: [1/20] on epoch 11/100\n",
            "\tEpoch 12: \t Train Loss - 0.02162 | Val Loss - 0.02231\n",
            "\tEarly stopping: [2/20] on epoch 12/100\n",
            "\tEpoch 13: \t Train Loss - 0.02146 | Val Loss - 0.02123\n",
            "\tEarly stopping: [3/20] on epoch 13/100\n",
            "\tEpoch 14: \t Train Loss - 0.02195 | Val Loss - 0.02116\n",
            "\tEarly stopping: [4/20] on epoch 14/100\n",
            "\tEpoch 15: \t Train Loss - 0.02206 | Val Loss - 0.02208\n",
            "\tEarly stopping: [5/20] on epoch 15/100\n",
            "\tEpoch 16: \t Train Loss - 0.02133 | Val Loss - 0.02319\n",
            "\tEarly stopping: [6/20] on epoch 16/100\n",
            "\tEpoch 17: \t Train Loss - 0.02174 | Val Loss - 0.02294\n",
            "\tEarly stopping: [7/20] on epoch 17/100\n",
            "\tEpoch 18: \t Train Loss - 0.02149 | Val Loss - 0.02159\n",
            "\tEarly stopping: [8/20] on epoch 18/100\n",
            "\tEpoch 19: \t Train Loss - 0.02155 | Val Loss - 0.02111\n",
            "\tEarly stopping: [9/20] on epoch 19/100\n",
            "\tEpoch 20: \t Train Loss - 0.02163 | Val Loss - 0.0211\n",
            "\tEarly stopping: [10/20] on epoch 20/100\n",
            "\tEpoch 21: \t Train Loss - 0.02162 | Val Loss - 0.02101\n",
            "\tEarly stopping: [11/20] on epoch 21/100\n",
            "\tEpoch 22: \t Train Loss - 0.02158 | Val Loss - 0.02103\n",
            "\tEarly stopping: [12/20] on epoch 22/100\n",
            "\tEpoch 23: \t Train Loss - 0.0215 | Val Loss - 0.02099\n",
            "\tEarly stopping: [13/20] on epoch 23/100\n",
            "\tEpoch 24: \t Train Loss - 0.02179 | Val Loss - 0.02119\n",
            "\tEarly stopping: [14/20] on epoch 24/100\n",
            "\tEpoch 25: \t Train Loss - 0.02178 | Val Loss - 0.02199\n",
            "\tEarly stopping: [15/20] on epoch 25/100\n",
            "\tEpoch 26: \t Train Loss - 0.02153 | Val Loss - 0.02103\n",
            "\tEarly stopping: [16/20] on epoch 26/100\n",
            "\tEpoch 27: \t Train Loss - 0.02147 | Val Loss - 0.02097\n",
            "\tEarly stopping: [17/20] on epoch 27/100\n",
            "\tEpoch 28: \t Train Loss - 0.02154 | Val Loss - 0.02088\n",
            "\tEarly stopping: [1/20] on epoch 28/100\n",
            "\tEpoch 29: \t Train Loss - 0.02163 | Val Loss - 0.022\n",
            "\tEarly stopping: [2/20] on epoch 29/100\n",
            "\tEpoch 30: \t Train Loss - 0.02165 | Val Loss - 0.0213\n",
            "\tEarly stopping: [3/20] on epoch 30/100\n",
            "\tEpoch 31: \t Train Loss - 0.02182 | Val Loss - 0.02084\n",
            "\tEarly stopping: [1/20] on epoch 31/100\n",
            "\tEpoch 32: \t Train Loss - 0.02184 | Val Loss - 0.02115\n",
            "\tEarly stopping: [2/20] on epoch 32/100\n",
            "\tEpoch 33: \t Train Loss - 0.02159 | Val Loss - 0.02206\n",
            "\tEarly stopping: [3/20] on epoch 33/100\n",
            "\tEpoch 34: \t Train Loss - 0.02184 | Val Loss - 0.02116\n",
            "\tEarly stopping: [4/20] on epoch 34/100\n",
            "\tEpoch 35: \t Train Loss - 0.02167 | Val Loss - 0.02134\n",
            "\tEarly stopping: [5/20] on epoch 35/100\n",
            "\tEpoch 36: \t Train Loss - 0.02159 | Val Loss - 0.02117\n",
            "\tEarly stopping: [6/20] on epoch 36/100\n",
            "\tEpoch 37: \t Train Loss - 0.0216 | Val Loss - 0.0217\n",
            "\tEarly stopping: [7/20] on epoch 37/100\n",
            "\tEpoch 38: \t Train Loss - 0.02138 | Val Loss - 0.02124\n",
            "\tEarly stopping: [8/20] on epoch 38/100\n",
            "\tEpoch 39: \t Train Loss - 0.02189 | Val Loss - 0.02199\n",
            "\tEarly stopping: [9/20] on epoch 39/100\n",
            "\tEpoch 40: \t Train Loss - 0.02192 | Val Loss - 0.02126\n",
            "\tEarly stopping: [10/20] on epoch 40/100\n",
            "\tEpoch 41: \t Train Loss - 0.02175 | Val Loss - 0.02099\n",
            "\tEarly stopping: [11/20] on epoch 41/100\n",
            "\tEpoch 42: \t Train Loss - 0.02166 | Val Loss - 0.02114\n",
            "\tEarly stopping: [12/20] on epoch 42/100\n",
            "\tEpoch 43: \t Train Loss - 0.02156 | Val Loss - 0.02102\n",
            "\tEarly stopping: [13/20] on epoch 43/100\n",
            "\tEpoch 44: \t Train Loss - 0.02166 | Val Loss - 0.02113\n",
            "\tEarly stopping: [14/20] on epoch 44/100\n",
            "\tEpoch 45: \t Train Loss - 0.02189 | Val Loss - 0.02101\n",
            "\tEarly stopping: [15/20] on epoch 45/100\n",
            "\tEpoch 46: \t Train Loss - 0.02174 | Val Loss - 0.02096\n",
            "\tEarly stopping: [16/20] on epoch 46/100\n",
            "\tEpoch 47: \t Train Loss - 0.02174 | Val Loss - 0.02271\n",
            "\tEarly stopping: [17/20] on epoch 47/100\n",
            "\tEpoch 48: \t Train Loss - 0.02184 | Val Loss - 0.02106\n",
            "\tEarly stopping: [18/20] on epoch 48/100\n",
            "\tEpoch 49: \t Train Loss - 0.02153 | Val Loss - 0.02155\n",
            "\tEarly stopping: [19/20] on epoch 49/100\n",
            "\tEpoch 50: \t Train Loss - 0.02163 | Val Loss - 0.02087\n",
            "\tEarly stopping: [20/20] on epoch 50/100\n",
            "Early Stopped!\n",
            "Training SAE_D2_RVH_FMT_C\n",
            "\tEpoch 0: \t Train Loss - 0.10601 | Val Loss - 0.07517\n",
            "\tEarly stopping: [1/20] on epoch 0/100\n",
            "\tEpoch 1: \t Train Loss - 0.07118 | Val Loss - 0.06732\n",
            "\tEarly stopping: [1/20] on epoch 1/100\n",
            "\tEpoch 2: \t Train Loss - 0.06706 | Val Loss - 0.06072\n",
            "\tEarly stopping: [1/20] on epoch 2/100\n",
            "\tEpoch 3: \t Train Loss - 0.0608 | Val Loss - 0.05853\n",
            "\tEarly stopping: [1/20] on epoch 3/100\n",
            "\tEpoch 4: \t Train Loss - 0.05858 | Val Loss - 0.05842\n",
            "\tEarly stopping: [1/20] on epoch 4/100\n",
            "\tEpoch 5: \t Train Loss - 0.05854 | Val Loss - 0.05788\n",
            "\tEarly stopping: [1/20] on epoch 5/100\n",
            "\tEpoch 6: \t Train Loss - 0.05816 | Val Loss - 0.05838\n",
            "\tEarly stopping: [2/20] on epoch 6/100\n",
            "\tEpoch 7: \t Train Loss - 0.05785 | Val Loss - 0.05875\n",
            "\tEarly stopping: [3/20] on epoch 7/100\n",
            "\tEpoch 8: \t Train Loss - 0.05774 | Val Loss - 0.05819\n",
            "\tEarly stopping: [4/20] on epoch 8/100\n",
            "\tEpoch 9: \t Train Loss - 0.05716 | Val Loss - 0.05866\n",
            "\tEarly stopping: [5/20] on epoch 9/100\n",
            "\tEpoch 10: \t Train Loss - 0.05744 | Val Loss - 0.05793\n",
            "\tEarly stopping: [6/20] on epoch 10/100\n",
            "\tEpoch 11: \t Train Loss - 0.05756 | Val Loss - 0.05938\n",
            "\tEarly stopping: [7/20] on epoch 11/100\n",
            "\tEpoch 12: \t Train Loss - 0.05589 | Val Loss - 0.05832\n",
            "\tEarly stopping: [8/20] on epoch 12/100\n",
            "\tEpoch 13: \t Train Loss - 0.05788 | Val Loss - 0.05936\n",
            "\tEarly stopping: [9/20] on epoch 13/100\n",
            "\tEpoch 14: \t Train Loss - 0.0571 | Val Loss - 0.05846\n",
            "\tEarly stopping: [10/20] on epoch 14/100\n",
            "\tEpoch 15: \t Train Loss - 0.05647 | Val Loss - 0.05839\n",
            "\tEarly stopping: [11/20] on epoch 15/100\n",
            "\tEpoch 16: \t Train Loss - 0.05714 | Val Loss - 0.05812\n",
            "\tEarly stopping: [12/20] on epoch 16/100\n",
            "\tEpoch 17: \t Train Loss - 0.05845 | Val Loss - 0.05858\n",
            "\tEarly stopping: [13/20] on epoch 17/100\n",
            "\tEpoch 18: \t Train Loss - 0.05757 | Val Loss - 0.05804\n",
            "\tEarly stopping: [14/20] on epoch 18/100\n",
            "\tEpoch 19: \t Train Loss - 0.05749 | Val Loss - 0.05793\n",
            "\tEarly stopping: [15/20] on epoch 19/100\n",
            "\tEpoch 20: \t Train Loss - 0.05645 | Val Loss - 0.05839\n",
            "\tEarly stopping: [16/20] on epoch 20/100\n",
            "\tEpoch 21: \t Train Loss - 0.0572 | Val Loss - 0.05908\n",
            "\tEarly stopping: [17/20] on epoch 21/100\n",
            "\tEpoch 22: \t Train Loss - 0.05672 | Val Loss - 0.05827\n",
            "\tEarly stopping: [18/20] on epoch 22/100\n",
            "\tEpoch 23: \t Train Loss - 0.05779 | Val Loss - 0.0579\n",
            "\tEarly stopping: [19/20] on epoch 23/100\n",
            "\tEpoch 24: \t Train Loss - 0.05821 | Val Loss - 0.05804\n",
            "\tEarly stopping: [20/20] on epoch 24/100\n",
            "Early Stopped!\n",
            "Training SAE_D3_RVH_Capitan\n",
            "\tEpoch 0: \t Train Loss - 0.11157 | Val Loss - 0.05361\n",
            "\tEarly stopping: [1/20] on epoch 0/100\n",
            "\tEpoch 1: \t Train Loss - 0.07508 | Val Loss - 0.06545\n",
            "\tEarly stopping: [2/20] on epoch 1/100\n",
            "\tEpoch 2: \t Train Loss - 0.0543 | Val Loss - 0.06482\n",
            "\tEarly stopping: [3/20] on epoch 2/100\n",
            "\tEpoch 3: \t Train Loss - 0.04607 | Val Loss - 0.03928\n",
            "\tEarly stopping: [1/20] on epoch 3/100\n",
            "\tEpoch 4: \t Train Loss - 0.03713 | Val Loss - 0.0388\n",
            "\tEarly stopping: [1/20] on epoch 4/100\n",
            "\tEpoch 5: \t Train Loss - 0.03933 | Val Loss - 0.03924\n",
            "\tEarly stopping: [2/20] on epoch 5/100\n",
            "\tEpoch 6: \t Train Loss - 0.03806 | Val Loss - 0.04053\n",
            "\tEarly stopping: [3/20] on epoch 6/100\n",
            "\tEpoch 7: \t Train Loss - 0.03777 | Val Loss - 0.03848\n",
            "\tEarly stopping: [1/20] on epoch 7/100\n",
            "\tEpoch 8: \t Train Loss - 0.0356 | Val Loss - 0.03883\n",
            "\tEarly stopping: [2/20] on epoch 8/100\n",
            "\tEpoch 9: \t Train Loss - 0.03624 | Val Loss - 0.0397\n",
            "\tEarly stopping: [3/20] on epoch 9/100\n",
            "\tEpoch 10: \t Train Loss - 0.03648 | Val Loss - 0.03783\n",
            "\tEarly stopping: [1/20] on epoch 10/100\n",
            "\tEpoch 11: \t Train Loss - 0.03538 | Val Loss - 0.03766\n",
            "\tEarly stopping: [1/20] on epoch 11/100\n",
            "\tEpoch 12: \t Train Loss - 0.03801 | Val Loss - 0.03895\n",
            "\tEarly stopping: [2/20] on epoch 12/100\n",
            "\tEpoch 13: \t Train Loss - 0.03909 | Val Loss - 0.03972\n",
            "\tEarly stopping: [3/20] on epoch 13/100\n",
            "\tEpoch 14: \t Train Loss - 0.03613 | Val Loss - 0.04032\n",
            "\tEarly stopping: [4/20] on epoch 14/100\n",
            "\tEpoch 15: \t Train Loss - 0.03648 | Val Loss - 0.03801\n",
            "\tEarly stopping: [5/20] on epoch 15/100\n",
            "\tEpoch 16: \t Train Loss - 0.03761 | Val Loss - 0.03801\n",
            "\tEarly stopping: [6/20] on epoch 16/100\n",
            "\tEpoch 17: \t Train Loss - 0.03686 | Val Loss - 0.03998\n",
            "\tEarly stopping: [7/20] on epoch 17/100\n",
            "\tEpoch 18: \t Train Loss - 0.03634 | Val Loss - 0.03853\n",
            "\tEarly stopping: [8/20] on epoch 18/100\n",
            "\tEpoch 19: \t Train Loss - 0.03624 | Val Loss - 0.038\n",
            "\tEarly stopping: [9/20] on epoch 19/100\n",
            "\tEpoch 20: \t Train Loss - 0.03473 | Val Loss - 0.03969\n",
            "\tEarly stopping: [10/20] on epoch 20/100\n",
            "\tEpoch 21: \t Train Loss - 0.03631 | Val Loss - 0.03866\n",
            "\tEarly stopping: [11/20] on epoch 21/100\n",
            "\tEpoch 22: \t Train Loss - 0.03618 | Val Loss - 0.04081\n",
            "\tEarly stopping: [12/20] on epoch 22/100\n",
            "\tEpoch 23: \t Train Loss - 0.03765 | Val Loss - 0.04146\n",
            "\tEarly stopping: [13/20] on epoch 23/100\n",
            "\tEpoch 24: \t Train Loss - 0.03721 | Val Loss - 0.03933\n",
            "\tEarly stopping: [14/20] on epoch 24/100\n",
            "\tEpoch 25: \t Train Loss - 0.03723 | Val Loss - 0.03759\n",
            "\tEarly stopping: [1/20] on epoch 25/100\n",
            "\tEpoch 26: \t Train Loss - 0.03709 | Val Loss - 0.03748\n",
            "\tEarly stopping: [1/20] on epoch 26/100\n",
            "\tEpoch 27: \t Train Loss - 0.03673 | Val Loss - 0.03853\n",
            "\tEarly stopping: [2/20] on epoch 27/100\n",
            "\tEpoch 28: \t Train Loss - 0.03655 | Val Loss - 0.03814\n",
            "\tEarly stopping: [3/20] on epoch 28/100\n",
            "\tEpoch 29: \t Train Loss - 0.03617 | Val Loss - 0.04156\n",
            "\tEarly stopping: [4/20] on epoch 29/100\n",
            "\tEpoch 30: \t Train Loss - 0.03419 | Val Loss - 0.03817\n",
            "\tEarly stopping: [5/20] on epoch 30/100\n",
            "\tEpoch 31: \t Train Loss - 0.03723 | Val Loss - 0.03822\n",
            "\tEarly stopping: [6/20] on epoch 31/100\n",
            "\tEpoch 32: \t Train Loss - 0.03831 | Val Loss - 0.03819\n",
            "\tEarly stopping: [7/20] on epoch 32/100\n",
            "\tEpoch 33: \t Train Loss - 0.03641 | Val Loss - 0.03924\n",
            "\tEarly stopping: [8/20] on epoch 33/100\n",
            "\tEpoch 34: \t Train Loss - 0.03739 | Val Loss - 0.0397\n",
            "\tEarly stopping: [9/20] on epoch 34/100\n",
            "\tEpoch 35: \t Train Loss - 0.03797 | Val Loss - 0.04185\n",
            "\tEarly stopping: [10/20] on epoch 35/100\n",
            "\tEpoch 36: \t Train Loss - 0.03681 | Val Loss - 0.03963\n",
            "\tEarly stopping: [11/20] on epoch 36/100\n",
            "\tEpoch 37: \t Train Loss - 0.03593 | Val Loss - 0.0379\n",
            "\tEarly stopping: [12/20] on epoch 37/100\n",
            "\tEpoch 38: \t Train Loss - 0.03793 | Val Loss - 0.03741\n",
            "\tEarly stopping: [1/20] on epoch 38/100\n",
            "\tEpoch 39: \t Train Loss - 0.03748 | Val Loss - 0.0413\n",
            "\tEarly stopping: [2/20] on epoch 39/100\n",
            "\tEpoch 40: \t Train Loss - 0.03888 | Val Loss - 0.03966\n",
            "\tEarly stopping: [3/20] on epoch 40/100\n",
            "\tEpoch 41: \t Train Loss - 0.03685 | Val Loss - 0.03993\n",
            "\tEarly stopping: [4/20] on epoch 41/100\n",
            "\tEpoch 42: \t Train Loss - 0.03872 | Val Loss - 0.03954\n",
            "\tEarly stopping: [5/20] on epoch 42/100\n",
            "\tEpoch 43: \t Train Loss - 0.03438 | Val Loss - 0.03944\n",
            "\tEarly stopping: [6/20] on epoch 43/100\n",
            "\tEpoch 44: \t Train Loss - 0.0369 | Val Loss - 0.03787\n",
            "\tEarly stopping: [7/20] on epoch 44/100\n",
            "\tEpoch 45: \t Train Loss - 0.0378 | Val Loss - 0.03923\n",
            "\tEarly stopping: [8/20] on epoch 45/100\n",
            "\tEpoch 46: \t Train Loss - 0.03728 | Val Loss - 0.04304\n",
            "\tEarly stopping: [9/20] on epoch 46/100\n",
            "\tEpoch 47: \t Train Loss - 0.03628 | Val Loss - 0.03856\n",
            "\tEarly stopping: [10/20] on epoch 47/100\n",
            "\tEpoch 48: \t Train Loss - 0.03653 | Val Loss - 0.03785\n",
            "\tEarly stopping: [11/20] on epoch 48/100\n",
            "\tEpoch 49: \t Train Loss - 0.03783 | Val Loss - 0.03734\n",
            "\tEarly stopping: [1/20] on epoch 49/100\n",
            "\tEpoch 50: \t Train Loss - 0.03669 | Val Loss - 0.04019\n",
            "\tEarly stopping: [2/20] on epoch 50/100\n",
            "\tEpoch 51: \t Train Loss - 0.03491 | Val Loss - 0.03984\n",
            "\tEarly stopping: [3/20] on epoch 51/100\n",
            "\tEpoch 52: \t Train Loss - 0.03618 | Val Loss - 0.03947\n",
            "\tEarly stopping: [4/20] on epoch 52/100\n",
            "\tEpoch 53: \t Train Loss - 0.03873 | Val Loss - 0.03863\n",
            "\tEarly stopping: [5/20] on epoch 53/100\n",
            "\tEpoch 54: \t Train Loss - 0.03664 | Val Loss - 0.03902\n",
            "\tEarly stopping: [6/20] on epoch 54/100\n",
            "\tEpoch 55: \t Train Loss - 0.0365 | Val Loss - 0.03967\n",
            "\tEarly stopping: [7/20] on epoch 55/100\n",
            "\tEpoch 56: \t Train Loss - 0.03651 | Val Loss - 0.03728\n",
            "\tEarly stopping: [1/20] on epoch 56/100\n",
            "\tEpoch 57: \t Train Loss - 0.03847 | Val Loss - 0.03785\n",
            "\tEarly stopping: [2/20] on epoch 57/100\n",
            "\tEpoch 58: \t Train Loss - 0.03692 | Val Loss - 0.03852\n",
            "\tEarly stopping: [3/20] on epoch 58/100\n",
            "\tEpoch 59: \t Train Loss - 0.0365 | Val Loss - 0.03889\n",
            "\tEarly stopping: [4/20] on epoch 59/100\n",
            "\tEpoch 60: \t Train Loss - 0.03595 | Val Loss - 0.03857\n",
            "\tEarly stopping: [5/20] on epoch 60/100\n",
            "\tEpoch 61: \t Train Loss - 0.03699 | Val Loss - 0.03835\n",
            "\tEarly stopping: [6/20] on epoch 61/100\n",
            "\tEpoch 62: \t Train Loss - 0.03672 | Val Loss - 0.03752\n",
            "\tEarly stopping: [7/20] on epoch 62/100\n",
            "\tEpoch 63: \t Train Loss - 0.03914 | Val Loss - 0.03855\n",
            "\tEarly stopping: [8/20] on epoch 63/100\n",
            "\tEpoch 64: \t Train Loss - 0.03541 | Val Loss - 0.03865\n",
            "\tEarly stopping: [9/20] on epoch 64/100\n",
            "\tEpoch 65: \t Train Loss - 0.0366 | Val Loss - 0.0382\n",
            "\tEarly stopping: [10/20] on epoch 65/100\n",
            "\tEpoch 66: \t Train Loss - 0.0347 | Val Loss - 0.03906\n",
            "\tEarly stopping: [11/20] on epoch 66/100\n",
            "\tEpoch 67: \t Train Loss - 0.03527 | Val Loss - 0.03763\n",
            "\tEarly stopping: [12/20] on epoch 67/100\n",
            "\tEpoch 68: \t Train Loss - 0.0363 | Val Loss - 0.03872\n",
            "\tEarly stopping: [13/20] on epoch 68/100\n",
            "\tEpoch 69: \t Train Loss - 0.03603 | Val Loss - 0.03915\n",
            "\tEarly stopping: [14/20] on epoch 69/100\n",
            "\tEpoch 70: \t Train Loss - 0.03651 | Val Loss - 0.03847\n",
            "\tEarly stopping: [15/20] on epoch 70/100\n",
            "\tEpoch 71: \t Train Loss - 0.03851 | Val Loss - 0.03823\n",
            "\tEarly stopping: [16/20] on epoch 71/100\n",
            "\tEpoch 72: \t Train Loss - 0.03526 | Val Loss - 0.03879\n",
            "\tEarly stopping: [17/20] on epoch 72/100\n",
            "\tEpoch 73: \t Train Loss - 0.0365 | Val Loss - 0.0383\n",
            "\tEarly stopping: [18/20] on epoch 73/100\n",
            "\tEpoch 74: \t Train Loss - 0.03852 | Val Loss - 0.03883\n",
            "\tEarly stopping: [19/20] on epoch 74/100\n",
            "\tEpoch 75: \t Train Loss - 0.03701 | Val Loss - 0.04135\n",
            "\tEarly stopping: [20/20] on epoch 75/100\n",
            "Early Stopped!\n",
            "Training SAE_D3_RVH_SEILS\n",
            "\tEpoch 0: \t Train Loss - 0.04806 | Val Loss - 0.02663\n",
            "\tEarly stopping: [1/20] on epoch 0/100\n",
            "\tEpoch 1: \t Train Loss - 0.02703 | Val Loss - 0.02489\n",
            "\tEarly stopping: [1/20] on epoch 1/100\n",
            "\tEpoch 2: \t Train Loss - 0.02502 | Val Loss - 0.02463\n",
            "\tEarly stopping: [1/20] on epoch 2/100\n",
            "\tEpoch 3: \t Train Loss - 0.02359 | Val Loss - 0.02253\n",
            "\tEarly stopping: [1/20] on epoch 3/100\n",
            "\tEpoch 4: \t Train Loss - 0.02352 | Val Loss - 0.02317\n",
            "\tEarly stopping: [2/20] on epoch 4/100\n",
            "\tEpoch 5: \t Train Loss - 0.02348 | Val Loss - 0.02304\n",
            "\tEarly stopping: [3/20] on epoch 5/100\n",
            "\tEpoch 6: \t Train Loss - 0.02343 | Val Loss - 0.02151\n",
            "\tEarly stopping: [1/20] on epoch 6/100\n",
            "\tEpoch 7: \t Train Loss - 0.02288 | Val Loss - 0.02194\n",
            "\tEarly stopping: [2/20] on epoch 7/100\n",
            "\tEpoch 8: \t Train Loss - 0.02283 | Val Loss - 0.02513\n",
            "\tEarly stopping: [3/20] on epoch 8/100\n",
            "\tEpoch 9: \t Train Loss - 0.02321 | Val Loss - 0.02213\n",
            "\tEarly stopping: [4/20] on epoch 9/100\n",
            "\tEpoch 10: \t Train Loss - 0.02335 | Val Loss - 0.02241\n",
            "\tEarly stopping: [5/20] on epoch 10/100\n",
            "\tEpoch 11: \t Train Loss - 0.02334 | Val Loss - 0.02225\n",
            "\tEarly stopping: [6/20] on epoch 11/100\n",
            "\tEpoch 12: \t Train Loss - 0.02294 | Val Loss - 0.02198\n",
            "\tEarly stopping: [7/20] on epoch 12/100\n",
            "\tEpoch 13: \t Train Loss - 0.02308 | Val Loss - 0.02213\n",
            "\tEarly stopping: [8/20] on epoch 13/100\n",
            "\tEpoch 14: \t Train Loss - 0.0228 | Val Loss - 0.02241\n",
            "\tEarly stopping: [9/20] on epoch 14/100\n",
            "\tEpoch 15: \t Train Loss - 0.02264 | Val Loss - 0.02248\n",
            "\tEarly stopping: [10/20] on epoch 15/100\n",
            "\tEpoch 16: \t Train Loss - 0.02299 | Val Loss - 0.0223\n",
            "\tEarly stopping: [11/20] on epoch 16/100\n",
            "\tEpoch 17: \t Train Loss - 0.02295 | Val Loss - 0.02267\n",
            "\tEarly stopping: [12/20] on epoch 17/100\n",
            "\tEpoch 18: \t Train Loss - 0.02352 | Val Loss - 0.02249\n",
            "\tEarly stopping: [13/20] on epoch 18/100\n",
            "\tEpoch 19: \t Train Loss - 0.02303 | Val Loss - 0.02204\n",
            "\tEarly stopping: [14/20] on epoch 19/100\n",
            "\tEpoch 20: \t Train Loss - 0.02329 | Val Loss - 0.02222\n",
            "\tEarly stopping: [15/20] on epoch 20/100\n",
            "\tEpoch 21: \t Train Loss - 0.02282 | Val Loss - 0.02205\n",
            "\tEarly stopping: [16/20] on epoch 21/100\n",
            "\tEpoch 22: \t Train Loss - 0.02342 | Val Loss - 0.02241\n",
            "\tEarly stopping: [17/20] on epoch 22/100\n",
            "\tEpoch 23: \t Train Loss - 0.02331 | Val Loss - 0.02268\n",
            "\tEarly stopping: [18/20] on epoch 23/100\n",
            "\tEpoch 24: \t Train Loss - 0.02315 | Val Loss - 0.02185\n",
            "\tEarly stopping: [19/20] on epoch 24/100\n",
            "\tEpoch 25: \t Train Loss - 0.023 | Val Loss - 0.02241\n",
            "\tEarly stopping: [20/20] on epoch 25/100\n",
            "Early Stopped!\n",
            "Training SAE_D3_RVH_FMT_C\n",
            "\tEpoch 0: \t Train Loss - 0.11554 | Val Loss - 0.0782\n",
            "\tEarly stopping: [1/20] on epoch 0/100\n",
            "\tEpoch 1: \t Train Loss - 0.08007 | Val Loss - 0.07433\n",
            "\tEarly stopping: [1/20] on epoch 1/100\n",
            "\tEpoch 2: \t Train Loss - 0.07226 | Val Loss - 0.06509\n",
            "\tEarly stopping: [1/20] on epoch 2/100\n",
            "\tEpoch 3: \t Train Loss - 0.06289 | Val Loss - 0.06023\n",
            "\tEarly stopping: [1/20] on epoch 3/100\n",
            "\tEpoch 4: \t Train Loss - 0.06296 | Val Loss - 0.0614\n",
            "\tEarly stopping: [2/20] on epoch 4/100\n",
            "\tEpoch 5: \t Train Loss - 0.06349 | Val Loss - 0.0607\n",
            "\tEarly stopping: [3/20] on epoch 5/100\n",
            "\tEpoch 6: \t Train Loss - 0.06211 | Val Loss - 0.06052\n",
            "\tEarly stopping: [4/20] on epoch 6/100\n",
            "\tEpoch 7: \t Train Loss - 0.06129 | Val Loss - 0.06112\n",
            "\tEarly stopping: [5/20] on epoch 7/100\n",
            "\tEpoch 8: \t Train Loss - 0.06115 | Val Loss - 0.06029\n",
            "\tEarly stopping: [6/20] on epoch 8/100\n",
            "\tEpoch 9: \t Train Loss - 0.0614 | Val Loss - 0.06037\n",
            "\tEarly stopping: [7/20] on epoch 9/100\n",
            "\tEpoch 10: \t Train Loss - 0.05883 | Val Loss - 0.06007\n",
            "\tEarly stopping: [1/20] on epoch 10/100\n",
            "\tEpoch 11: \t Train Loss - 0.06305 | Val Loss - 0.06031\n",
            "\tEarly stopping: [2/20] on epoch 11/100\n",
            "\tEpoch 12: \t Train Loss - 0.06148 | Val Loss - 0.05983\n",
            "\tEarly stopping: [1/20] on epoch 12/100\n",
            "\tEpoch 13: \t Train Loss - 0.06133 | Val Loss - 0.06034\n",
            "\tEarly stopping: [2/20] on epoch 13/100\n",
            "\tEpoch 14: \t Train Loss - 0.06073 | Val Loss - 0.0599\n",
            "\tEarly stopping: [3/20] on epoch 14/100\n",
            "\tEpoch 15: \t Train Loss - 0.06124 | Val Loss - 0.06054\n",
            "\tEarly stopping: [4/20] on epoch 15/100\n",
            "\tEpoch 16: \t Train Loss - 0.0596 | Val Loss - 0.06029\n",
            "\tEarly stopping: [5/20] on epoch 16/100\n",
            "\tEpoch 17: \t Train Loss - 0.05982 | Val Loss - 0.06005\n",
            "\tEarly stopping: [6/20] on epoch 17/100\n",
            "\tEpoch 18: \t Train Loss - 0.05949 | Val Loss - 0.06055\n",
            "\tEarly stopping: [7/20] on epoch 18/100\n",
            "\tEpoch 19: \t Train Loss - 0.06278 | Val Loss - 0.06033\n",
            "\tEarly stopping: [8/20] on epoch 19/100\n",
            "\tEpoch 20: \t Train Loss - 0.06098 | Val Loss - 0.05993\n",
            "\tEarly stopping: [9/20] on epoch 20/100\n",
            "\tEpoch 21: \t Train Loss - 0.06227 | Val Loss - 0.06075\n",
            "\tEarly stopping: [10/20] on epoch 21/100\n",
            "\tEpoch 22: \t Train Loss - 0.06165 | Val Loss - 0.05993\n",
            "\tEarly stopping: [11/20] on epoch 22/100\n",
            "\tEpoch 23: \t Train Loss - 0.06208 | Val Loss - 0.06011\n",
            "\tEarly stopping: [12/20] on epoch 23/100\n",
            "\tEpoch 24: \t Train Loss - 0.06254 | Val Loss - 0.06134\n",
            "\tEarly stopping: [13/20] on epoch 24/100\n",
            "\tEpoch 25: \t Train Loss - 0.06156 | Val Loss - 0.06146\n",
            "\tEarly stopping: [14/20] on epoch 25/100\n",
            "\tEpoch 26: \t Train Loss - 0.06143 | Val Loss - 0.06015\n",
            "\tEarly stopping: [15/20] on epoch 26/100\n",
            "\tEpoch 27: \t Train Loss - 0.06096 | Val Loss - 0.06121\n",
            "\tEarly stopping: [16/20] on epoch 27/100\n",
            "\tEpoch 28: \t Train Loss - 0.06145 | Val Loss - 0.05963\n",
            "\tEarly stopping: [1/20] on epoch 28/100\n",
            "\tEpoch 29: \t Train Loss - 0.06163 | Val Loss - 0.06025\n",
            "\tEarly stopping: [2/20] on epoch 29/100\n",
            "\tEpoch 30: \t Train Loss - 0.06096 | Val Loss - 0.06002\n",
            "\tEarly stopping: [3/20] on epoch 30/100\n",
            "\tEpoch 31: \t Train Loss - 0.06177 | Val Loss - 0.06012\n",
            "\tEarly stopping: [4/20] on epoch 31/100\n",
            "\tEpoch 32: \t Train Loss - 0.06315 | Val Loss - 0.05936\n",
            "\tEarly stopping: [1/20] on epoch 32/100\n",
            "\tEpoch 33: \t Train Loss - 0.06277 | Val Loss - 0.06148\n",
            "\tEarly stopping: [2/20] on epoch 33/100\n",
            "\tEpoch 34: \t Train Loss - 0.06245 | Val Loss - 0.06073\n",
            "\tEarly stopping: [3/20] on epoch 34/100\n",
            "\tEpoch 35: \t Train Loss - 0.0625 | Val Loss - 0.06009\n",
            "\tEarly stopping: [4/20] on epoch 35/100\n",
            "\tEpoch 36: \t Train Loss - 0.06093 | Val Loss - 0.06018\n",
            "\tEarly stopping: [5/20] on epoch 36/100\n",
            "\tEpoch 37: \t Train Loss - 0.06007 | Val Loss - 0.06087\n",
            "\tEarly stopping: [6/20] on epoch 37/100\n",
            "\tEpoch 38: \t Train Loss - 0.06007 | Val Loss - 0.05987\n",
            "\tEarly stopping: [7/20] on epoch 38/100\n",
            "\tEpoch 39: \t Train Loss - 0.06107 | Val Loss - 0.05947\n",
            "\tEarly stopping: [8/20] on epoch 39/100\n",
            "\tEpoch 40: \t Train Loss - 0.05932 | Val Loss - 0.05989\n",
            "\tEarly stopping: [9/20] on epoch 40/100\n",
            "\tEpoch 41: \t Train Loss - 0.06228 | Val Loss - 0.0597\n",
            "\tEarly stopping: [10/20] on epoch 41/100\n",
            "\tEpoch 42: \t Train Loss - 0.06141 | Val Loss - 0.06004\n",
            "\tEarly stopping: [11/20] on epoch 42/100\n",
            "\tEpoch 43: \t Train Loss - 0.06063 | Val Loss - 0.06682\n",
            "\tEarly stopping: [12/20] on epoch 43/100\n",
            "\tEpoch 44: \t Train Loss - 0.061 | Val Loss - 0.06051\n",
            "\tEarly stopping: [13/20] on epoch 44/100\n",
            "\tEpoch 45: \t Train Loss - 0.06226 | Val Loss - 0.06004\n",
            "\tEarly stopping: [14/20] on epoch 45/100\n",
            "\tEpoch 46: \t Train Loss - 0.06177 | Val Loss - 0.06025\n",
            "\tEarly stopping: [15/20] on epoch 46/100\n",
            "\tEpoch 47: \t Train Loss - 0.06108 | Val Loss - 0.05978\n",
            "\tEarly stopping: [16/20] on epoch 47/100\n",
            "\tEpoch 48: \t Train Loss - 0.05937 | Val Loss - 0.06\n",
            "\tEarly stopping: [17/20] on epoch 48/100\n",
            "\tEpoch 49: \t Train Loss - 0.06279 | Val Loss - 0.06029\n",
            "\tEarly stopping: [18/20] on epoch 49/100\n",
            "\tEpoch 50: \t Train Loss - 0.06016 | Val Loss - 0.06071\n",
            "\tEarly stopping: [19/20] on epoch 50/100\n",
            "\tEpoch 51: \t Train Loss - 0.0607 | Val Loss - 0.05973\n",
            "\tEarly stopping: [20/20] on epoch 51/100\n",
            "Early Stopped!\n",
            "Training SAE_D4_RVH_Capitan\n",
            "\tEpoch 0: \t Train Loss - 0.13543 | Val Loss - 0.05484\n",
            "\tEarly stopping: [1/20] on epoch 0/100\n",
            "\tEpoch 1: \t Train Loss - 0.08762 | Val Loss - 0.05945\n",
            "\tEarly stopping: [2/20] on epoch 1/100\n",
            "\tEpoch 2: \t Train Loss - 0.07178 | Val Loss - 0.05187\n",
            "\tEarly stopping: [1/20] on epoch 2/100\n",
            "\tEpoch 3: \t Train Loss - 0.0594 | Val Loss - 0.04999\n",
            "\tEarly stopping: [1/20] on epoch 3/100\n",
            "\tEpoch 4: \t Train Loss - 0.05784 | Val Loss - 0.04427\n",
            "\tEarly stopping: [1/20] on epoch 4/100\n",
            "\tEpoch 5: \t Train Loss - 0.05416 | Val Loss - 0.04453\n",
            "\tEarly stopping: [2/20] on epoch 5/100\n",
            "\tEpoch 6: \t Train Loss - 0.05098 | Val Loss - 0.04538\n",
            "\tEarly stopping: [3/20] on epoch 6/100\n",
            "\tEpoch 7: \t Train Loss - 0.05133 | Val Loss - 0.04806\n",
            "\tEarly stopping: [4/20] on epoch 7/100\n",
            "\tEpoch 8: \t Train Loss - 0.05027 | Val Loss - 0.04509\n",
            "\tEarly stopping: [5/20] on epoch 8/100\n",
            "\tEpoch 9: \t Train Loss - 0.04748 | Val Loss - 0.04737\n",
            "\tEarly stopping: [6/20] on epoch 9/100\n",
            "\tEpoch 10: \t Train Loss - 0.04879 | Val Loss - 0.04517\n",
            "\tEarly stopping: [7/20] on epoch 10/100\n",
            "\tEpoch 11: \t Train Loss - 0.05071 | Val Loss - 0.04428\n",
            "\tEarly stopping: [8/20] on epoch 11/100\n",
            "\tEpoch 12: \t Train Loss - 0.04941 | Val Loss - 0.04404\n",
            "\tEarly stopping: [1/20] on epoch 12/100\n",
            "\tEpoch 13: \t Train Loss - 0.04867 | Val Loss - 0.04748\n",
            "\tEarly stopping: [2/20] on epoch 13/100\n",
            "\tEpoch 14: \t Train Loss - 0.04759 | Val Loss - 0.04485\n",
            "\tEarly stopping: [3/20] on epoch 14/100\n",
            "\tEpoch 15: \t Train Loss - 0.04973 | Val Loss - 0.04935\n",
            "\tEarly stopping: [4/20] on epoch 15/100\n",
            "\tEpoch 16: \t Train Loss - 0.04707 | Val Loss - 0.04444\n",
            "\tEarly stopping: [5/20] on epoch 16/100\n",
            "\tEpoch 17: \t Train Loss - 0.05034 | Val Loss - 0.04797\n",
            "\tEarly stopping: [6/20] on epoch 17/100\n",
            "\tEpoch 18: \t Train Loss - 0.04726 | Val Loss - 0.04576\n",
            "\tEarly stopping: [7/20] on epoch 18/100\n",
            "\tEpoch 19: \t Train Loss - 0.04826 | Val Loss - 0.05414\n",
            "\tEarly stopping: [8/20] on epoch 19/100\n",
            "\tEpoch 20: \t Train Loss - 0.04936 | Val Loss - 0.04501\n",
            "\tEarly stopping: [9/20] on epoch 20/100\n",
            "\tEpoch 21: \t Train Loss - 0.04959 | Val Loss - 0.04678\n",
            "\tEarly stopping: [10/20] on epoch 21/100\n",
            "\tEpoch 22: \t Train Loss - 0.04842 | Val Loss - 0.04893\n",
            "\tEarly stopping: [11/20] on epoch 22/100\n",
            "\tEpoch 23: \t Train Loss - 0.04838 | Val Loss - 0.04887\n",
            "\tEarly stopping: [12/20] on epoch 23/100\n",
            "\tEpoch 24: \t Train Loss - 0.05205 | Val Loss - 0.04583\n",
            "\tEarly stopping: [13/20] on epoch 24/100\n",
            "\tEpoch 25: \t Train Loss - 0.04813 | Val Loss - 0.05005\n",
            "\tEarly stopping: [14/20] on epoch 25/100\n",
            "\tEpoch 26: \t Train Loss - 0.04893 | Val Loss - 0.04693\n",
            "\tEarly stopping: [15/20] on epoch 26/100\n",
            "\tEpoch 27: \t Train Loss - 0.05609 | Val Loss - 0.04238\n",
            "\tEarly stopping: [1/20] on epoch 27/100\n",
            "\tEpoch 28: \t Train Loss - 0.05068 | Val Loss - 0.04593\n",
            "\tEarly stopping: [2/20] on epoch 28/100\n",
            "\tEpoch 29: \t Train Loss - 0.04963 | Val Loss - 0.04189\n",
            "\tEarly stopping: [1/20] on epoch 29/100\n",
            "\tEpoch 30: \t Train Loss - 0.0475 | Val Loss - 0.04631\n",
            "\tEarly stopping: [2/20] on epoch 30/100\n",
            "\tEpoch 31: \t Train Loss - 0.05136 | Val Loss - 0.04845\n",
            "\tEarly stopping: [3/20] on epoch 31/100\n",
            "\tEpoch 32: \t Train Loss - 0.04796 | Val Loss - 0.04897\n",
            "\tEarly stopping: [4/20] on epoch 32/100\n",
            "\tEpoch 33: \t Train Loss - 0.04692 | Val Loss - 0.04813\n",
            "\tEarly stopping: [5/20] on epoch 33/100\n",
            "\tEpoch 34: \t Train Loss - 0.04684 | Val Loss - 0.04799\n",
            "\tEarly stopping: [6/20] on epoch 34/100\n",
            "\tEpoch 35: \t Train Loss - 0.04853 | Val Loss - 0.04908\n",
            "\tEarly stopping: [7/20] on epoch 35/100\n",
            "\tEpoch 36: \t Train Loss - 0.05051 | Val Loss - 0.05228\n",
            "\tEarly stopping: [8/20] on epoch 36/100\n",
            "\tEpoch 37: \t Train Loss - 0.05229 | Val Loss - 0.04633\n",
            "\tEarly stopping: [9/20] on epoch 37/100\n",
            "\tEpoch 38: \t Train Loss - 0.05162 | Val Loss - 0.04272\n",
            "\tEarly stopping: [10/20] on epoch 38/100\n",
            "\tEpoch 39: \t Train Loss - 0.04546 | Val Loss - 0.04745\n",
            "\tEarly stopping: [11/20] on epoch 39/100\n",
            "\tEpoch 40: \t Train Loss - 0.04898 | Val Loss - 0.04585\n",
            "\tEarly stopping: [12/20] on epoch 40/100\n",
            "\tEpoch 41: \t Train Loss - 0.04619 | Val Loss - 0.04137\n",
            "\tEarly stopping: [1/20] on epoch 41/100\n",
            "\tEpoch 42: \t Train Loss - 0.04853 | Val Loss - 0.04552\n",
            "\tEarly stopping: [2/20] on epoch 42/100\n",
            "\tEpoch 43: \t Train Loss - 0.0485 | Val Loss - 0.04268\n",
            "\tEarly stopping: [3/20] on epoch 43/100\n",
            "\tEpoch 44: \t Train Loss - 0.05084 | Val Loss - 0.04328\n",
            "\tEarly stopping: [4/20] on epoch 44/100\n"
          ]
        }
      ],
      "source": [
        "# Models parameters\n",
        "save_train_info = True\n",
        "plot_epochs = False\n",
        "\n",
        "uses_redimension_vertical = True\n",
        "uses_redimension_horizontal = True\n",
        "\n",
        "for dropout_value in DROPOUT_VALUES:\n",
        "    for dataset_name in DATASETS:\n",
        "        TFMTrainModel(dataset_name=dataset_name,\n",
        "                      dropout_value=dropout_value,\n",
        "                      uses_redimension_vertical=uses_redimension_vertical,\n",
        "                      uses_redimension_horizontal=uses_redimension_horizontal,\n",
        "                      save_train_info=save_train_info,\n",
        "                      plot_epochs=plot_epochs\n",
        "                      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Gtd-gIFR4jD"
      },
      "source": [
        "Extraemos el mejor umbral de binarización para cada modelo entrenado y obtenemos los resultados con el conjunto de validación. Fijamos el Dropout en entrenamiento al mejor obtenido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skfqFbSsYKQ3"
      },
      "outputs": [],
      "source": [
        "# Models parameters\n",
        "save_val_info = True\n",
        "val_dropout = 0\n",
        "times_pass_model = 1\n",
        "type_combination = PredictionsCombinationType.NONE\n",
        "\n",
        "uses_redimension_vertical = True\n",
        "uses_redimension_horizontal = True\n",
        "\n",
        "for dropout_value in DROPOUT_VALUES:\n",
        "    for dataset_name in DATASETS:\n",
        "        TFMValidation(dataset_name=dataset_name,\n",
        "                      dropout_value=dropout_value,\n",
        "                      val_dropout=val_dropout,\n",
        "                      times_pass_model=times_pass_model,\n",
        "                      type_combination=type_combination,\n",
        "                      uses_redimension_vertical=uses_redimension_vertical,\n",
        "                      uses_redimension_horizontal=uses_redimension_horizontal,\n",
        "                      save_val_info=save_val_info\n",
        "                      )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0cZFF6ZR4jD"
      },
      "source": [
        "## Paso 4\n",
        "\n",
        "Probamos el Dropout en test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrTmrMxmec5D"
      },
      "source": [
        "# Pruebas TFM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ja8KKfIZec5E",
        "outputId": "860676e9-25b9-455d-b16a-30b00fa3b9a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matrix of IoU:\n",
            "[[0.]]\n",
            "Precision: 0.0 \t Recall: 0.0\n",
            "(0, [])\n"
          ]
        }
      ],
      "source": [
        "# Pruebas TFM\n",
        "\n",
        "targetBoxes = [[0, 0, 5, 5]]\n",
        "boxes = [[5, 5, 11, 15]]\n",
        "\n",
        "print(calculate_F1(y_true = targetBoxes, y_pred = boxes, iou_threshold=0.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaTUUfisIu_R"
      },
      "source": [
        "# Experimento 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8-aAbSp-gpi"
      },
      "source": [
        "## Iniciar variables y datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-KS2sqzb8RK"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'Capitan' # 'Capitan'\n",
        "batch_size = 1\n",
        "modelName = 'SAE'\n",
        "resized_shape = SAE_IMAGE_SIZE\n",
        "\n",
        "resize_boxes = True\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoQ90qvEb-Eu"
      },
      "outputs": [],
      "source": [
        "datasetLoader = DatasetLoader(dataset_name)\n",
        "\n",
        "# datasetLoader.drawBoxesInDataset()\n",
        "\n",
        "# Load the datasets\n",
        "# train, val, test = datasetLoader.loadAllDataset()\n",
        "train_json, train_img = datasetLoader.loadTrainPaths()\n",
        "val_json, val_img = datasetLoader.loadValPaths()\n",
        "test_json, test_img = datasetLoader.loadTestPaths()\n",
        "\n",
        "# train_json, train_img = train_json[:5], train_img[:5]\n",
        "\n",
        "\n",
        "dataset_train = TrainMusicDataset(name=dataset_name, jsonPaths=train_json, imagesPaths=train_img, resize_shape=resized_shape, transforms=get_transform(), box_resize_vertical = False, box_resize_horizontal = False)\n",
        "dataset_eval = TrainMusicDataset(name=dataset_name, jsonPaths=val_json, imagesPaths=val_img, resize_shape=resized_shape, transforms=get_transform(), box_resize_vertical = False, box_resize_horizontal = False)\n",
        "\n",
        "# Training dataset\n",
        "data_loader_train = DataLoader(\n",
        "    dataset_train, batch_size=batch_size, shuffle=True, num_workers=0\n",
        ")\n",
        "\n",
        "# Validation  dataset (no redimension in order to calculate metrics)\n",
        "data_loader_eval = DataLoader(\n",
        "    dataset_eval, batch_size=batch_size, shuffle=False, num_workers=0\n",
        ")\n",
        "\n",
        "# Resized for coco (to check if metrics change)\n",
        "dataset_test = TestMusicDataset(name=dataset_name, jsonPaths=test_json, imagesPaths=test_img, resize_shape=resized_shape, transforms=get_transform())\n",
        "data_loader_test = DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coBs1zJbdpuy"
      },
      "outputs": [],
      "source": [
        "print(f'Training on {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doJvYfEzJNae"
      },
      "source": [
        "## Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRDN7kBwJPk8"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZeHZQco9brY"
      },
      "source": [
        "\n",
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtVPq6IA9brY"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/SAE_dropout02_redim8.pt'\n",
        "# model_path = '/content/SAE_dropout02.pt'\n",
        "model = torch.load(model_path, map_location=torch.device(device))\n",
        "model.to(device)\n",
        "# model.set_dropout_probability(0.2)\n",
        "# model.load_state_dict(torch.load('model.pth', map_location=torch.device(device)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwaHk_ov9brY"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zet7HwrzacQ5"
      },
      "outputs": [],
      "source": [
        "path_checkpoint = 'SAE_dropout02_redim8.pt'\n",
        "model = SAE()\n",
        "model.set_dropout_probability(0.2)\n",
        "model = trainModel(data_loader_train, data_loader_eval,\n",
        "           model = model, num_epochs = TRAIN_NUM_EPOCHS,\n",
        "           train_patience = TRAIN_PATIENTE, path_checkpoint = path_checkpoint)\n",
        "\n",
        "path_checkpoint = 'SAE_dropout03_redim8.pt'\n",
        "model = SAE()\n",
        "model.set_dropout_probability(0.3)\n",
        "model = trainModel(data_loader_train, data_loader_eval,\n",
        "           model = model, num_epochs = TRAIN_NUM_EPOCHS,\n",
        "           train_patience = TRAIN_PATIENTE, path_checkpoint = path_checkpoint)\n",
        "\n",
        "path_checkpoint = 'SAE_dropout05_redim8.pt'\n",
        "model = SAE()\n",
        "model.set_dropout_probability(0.5)\n",
        "model = trainModel(data_loader_train, data_loader_eval,\n",
        "           model = model, num_epochs = TRAIN_NUM_EPOCHS,\n",
        "           train_patience = TRAIN_PATIENTE, path_checkpoint = path_checkpoint)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_phovXS5lG_"
      },
      "source": [
        "## Val binarization hiperparameter ajustment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vpwZbbP5lG_"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/models/SAE_dropout02_redim8.pt'\n",
        "# model_path = '/content/SAE_dropout02.pt'\n",
        "model = torch.load(model_path, map_location=torch.device(device))\n",
        "\n",
        "model.eval()\n",
        "model.enable_eval_dropout()\n",
        "model.to(device)\n",
        "\n",
        "bin_umbrals_to_try = np.arange(0, 1, 0.01)\n",
        "best_bin_thresholdMean = []\n",
        "best_bin_thresholdMax = []\n",
        "\n",
        "\n",
        "for times_pass_model in [10, 20, 30, 50, 100]:\n",
        "  best_bin_thresholdMean.append(getBestIoUThreshold(model, data_loader_eval = data_loader_eval, dropout_value = 0.2, type_combination = PredictionsCombinationType.MEAN, model_resize_boxes = True, bin_umbrals_to_try = bin_umbrals_to_try, times_pass_model = times_pass_model))\n",
        "\n",
        "for times_pass_model in [10, 20, 30, 50, 100]:\n",
        "  best_bin_thresholdMax.append(getBestIoUThreshold(model, data_loader_eval = data_loader_eval, type_combination = 'max', model_resize_boxes = True, bin_umbrals_to_try = bin_umbrals_to_try, times_pass_model = times_pass_model))\n",
        "\n",
        "best_bin_threshold = max(max(best_bin_thresholdMean), max(best_bin_thresholdMax))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucsgvv1l3l4A"
      },
      "outputs": [],
      "source": [
        "# Validation  dataset (no redimension in order to calculate metrics)\n",
        "dataset_eval = TrainMusicDataset(name=dataset_name, jsonPaths=val_json, imagesPaths=val_img, resize_shape=resized_shape, transforms=get_transform())\n",
        "data_loader_eval = DataLoader(\n",
        "    dataset_eval, batch_size=batch_size, shuffle=False, num_workers=0\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Model 20% Dropout resized\n",
        "model_path = '/content/models/SAE_dropout02_redim8.pt'\n",
        "model_dropout = 0.2\n",
        "model = torch.load(model_path, map_location=torch.device(device))\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "bin_umbrals_to_try = np.arange(0, 1, 0.01)\n",
        "best_bin_thresholdMean = []\n",
        "best_bin_thresholdMax = []\n",
        "\n",
        "times_pass_model = 1\n",
        "\n",
        "best_bin_thresholdMean.append(getBestIoUThreshold(model, type_combination = 'max', model_resize_boxes = True, bin_umbrals_to_try = bin_umbrals_to_try, times_pass_model = times_pass_model, dropout_value=model_dropout))\n",
        "\n",
        "\n",
        "model_path = '/content/models/SAE_dropout03_redim8.pt'\n",
        "model_dropout = 0.3\n",
        "model = torch.load(model_path, map_location=torch.device(device))\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "bin_umbrals_to_try = np.arange(0, 1, 0.01)\n",
        "best_bin_thresholdMean = []\n",
        "best_bin_thresholdMax = []\n",
        "\n",
        "\n",
        "best_bin_thresholdMax.append(getBestIoUThreshold(model, type_combination = 'max', model_resize_boxes = True, bin_umbrals_to_try = bin_umbrals_to_try, times_pass_model = times_pass_model, dropout_value=model_dropout))\n",
        "\n",
        "model_path = '/content/models/SAE_dropout05_redim8.pt'\n",
        "model_dropout = 0.5\n",
        "model = torch.load(model_path, map_location=torch.device(device))\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "bin_umbrals_to_try = np.arange(0, 1, 0.01)\n",
        "best_bin_thresholdMean = []\n",
        "best_bin_thresholdMax = []\n",
        "\n",
        "\n",
        "best_bin_thresholdMax.append(getBestIoUThreshold(model, type_combination = 'max', model_resize_boxes = True, bin_umbrals_to_try = bin_umbrals_to_try, times_pass_model = times_pass_model, dropout_value=model_dropout))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dccMke2q4DLT"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "\n",
        "# Path to the source folder\n",
        "source_folder = '/content'\n",
        "\n",
        "# Path to the destination folder in Google Drive\n",
        "destination_folder = '/content/drive/MyDrive/TFG/BinResults'\n",
        "\n",
        "# Get a list of all files in the source folder\n",
        "file_list = os.listdir(source_folder)\n",
        "\n",
        "# Loop over each file in the list\n",
        "for file_name in file_list:\n",
        "    # Check if the file name starts with \"IoU\" or \"F1\"\n",
        "    if file_name.startswith('IoU') or file_name.startswith('F1'):\n",
        "        # Construct the full path to the file\n",
        "        file_path = os.path.join(source_folder, file_name)\n",
        "\n",
        "        # Move the file to the destination folder in Google Drive\n",
        "        shutil.copy(file_path, destination_folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kky0VwlY5lG_"
      },
      "source": [
        "## Test binarization thresholds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rfy1L_nebvFH"
      },
      "outputs": [],
      "source": [
        "modelNames = ['Model1', 'Model2', 'Model3', 'Model4', 'Model5', 'Model6']\n",
        "modelLoad  = {\n",
        "    modelNames[0]: 'SAE_dropout03.pt',\n",
        "    modelNames[1]: 'SAE_dropout02_redim8.pt',\n",
        "    modelNames[2]: 'SAE_dropout02.pt',\n",
        "    modelNames[3]: 'SAE_dropout03.pt',\n",
        "    modelNames[4]: 'SAE_dropout02_redim8.pt',\n",
        "    modelNames[5]: 'SAE_dropout02_redim8.pt',\n",
        "}\n",
        "\n",
        "modelResizedBB  = {\n",
        "    modelNames[0]: False,\n",
        "    modelNames[1]: True,\n",
        "    modelNames[2]: False,\n",
        "    modelNames[3]: False,\n",
        "    modelNames[4]: True,\n",
        "    modelNames[5]: True,\n",
        "}\n",
        "\n",
        "modelPredictDropout  = {\n",
        "    modelNames[0]: False,\n",
        "    modelNames[1]: False,\n",
        "    modelNames[2]: True,\n",
        "    modelNames[3]: True,\n",
        "    modelNames[4]: True,\n",
        "    modelNames[5]: True,\n",
        "}\n",
        "\n",
        "modelDropoutProb = {\n",
        "    modelNames[0]: 0.3,\n",
        "    modelNames[1]: 0.2,\n",
        "    modelNames[2]: 0.2,\n",
        "    modelNames[3]: 0.3,\n",
        "    modelNames[4]: 0.2,\n",
        "    modelNames[5]: 0.2,\n",
        "}\n",
        "\n",
        "modelCombinationOp  = {\n",
        "    modelNames[0]: 'max',\n",
        "    modelNames[1]: 'max',\n",
        "    modelNames[2]: 'max',\n",
        "    modelNames[3]: 'mean',\n",
        "    modelNames[4]: 'max',\n",
        "    modelNames[5]: 'mean',\n",
        "}\n",
        "\n",
        "modelCombinationNum  = {\n",
        "    modelNames[0]: 1,\n",
        "    modelNames[1]: 1,\n",
        "    modelNames[2]: 10,\n",
        "    modelNames[3]: 100,\n",
        "    modelNames[4]: 10,\n",
        "    modelNames[5]: 100,\n",
        "}\n",
        "\n",
        "modelBestUmbral  = {\n",
        "    modelNames[0]: 0.75,\n",
        "    modelNames[1]: 0.65,\n",
        "    modelNames[2]: 0.97,\n",
        "    modelNames[3]: 0.61,\n",
        "    modelNames[4]: 0.88,\n",
        "    modelNames[5]: 0.41,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGXWGxViKhDe"
      },
      "outputs": [],
      "source": [
        "modelNames = [ 'Model5', 'Model6']\n",
        "modelLoad  = {\n",
        "    modelNames[0]: 'SAE_dropout02_redim8.pt',\n",
        "    modelNames[1]: 'SAE_dropout02_redim8.pt',\n",
        "}\n",
        "\n",
        "modelResizedBB  = {\n",
        "    modelNames[0]: True,\n",
        "    modelNames[1]: True,\n",
        "}\n",
        "\n",
        "modelPredictDropout  = {\n",
        "    modelNames[0]: True,\n",
        "    modelNames[1]: True,\n",
        "}\n",
        "\n",
        "modelDropoutProb = {\n",
        "    modelNames[0]: 0.2,\n",
        "    modelNames[1]: 0.2,\n",
        "}\n",
        "\n",
        "modelCombinationOp  = {\n",
        "    modelNames[0]: 'max',\n",
        "    modelNames[1]: 'mean',\n",
        "}\n",
        "\n",
        "modelBestUmbral  = {\n",
        "    modelNames[0]: 0.88,\n",
        "    modelNames[1]: 0.41,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdJ46RP-5lG_"
      },
      "outputs": [],
      "source": [
        "dirToSave = '/content/drive/MyDrive/TFG/testResultsCapitan'\n",
        "\n",
        "for modelName in modelNames:\n",
        "    model_path = f'/content/models/exp1/{modelLoad[modelName]}'\n",
        "    model = torch.load(model_path, map_location=torch.device(device))\n",
        "    print(f'Loading model {modelLoad[modelName]}')\n",
        "    model.eval()\n",
        "\n",
        "    if modelPredictDropout[modelName]:\n",
        "        model.enable_eval_dropout()\n",
        "        model.set_dropout_probability(modelDropoutProb[modelName])\n",
        "\n",
        "    model.to(device)\n",
        "    # for times_pass_model in [10, 20, 30, 50, 100]:\n",
        "    testModel(model,\n",
        "              model_name = modelName,\n",
        "              dir_to_save = dirToSave,\n",
        "              data_loader_test = data_loader_test,\n",
        "              bin_umbrals_to_try = [0.5, 0.7, modelBestUmbral[modelName]],\n",
        "              model_resize_boxes = modelResizedBB[modelName],\n",
        "              times_pass_model  = modelCombinationNum[modelName],\n",
        "              type_combination  =  modelCombinationOp[modelName])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiWSi3WBGhcx"
      },
      "outputs": [],
      "source": [
        "### Test multiple passes\n",
        "\n",
        "dirToSave = '/content/drive/MyDrive/TFG/testResultsCapitan'\n",
        "\n",
        "for modelName in modelNames:\n",
        "    model_path = f'/content/models/exp1/{modelLoad[modelName]}'\n",
        "    model = torch.load(model_path, map_location=torch.device(device))\n",
        "    print(f'Loading model {modelLoad[modelName]}')\n",
        "    model.eval()\n",
        "\n",
        "    if modelPredictDropout[modelName]:\n",
        "        model.enable_eval_dropout()\n",
        "        model.set_dropout_probability(modelDropoutProb[modelName])\n",
        "\n",
        "    model.to(device)\n",
        "    for times_pass_model in [10, 20, 30, 50, 100]:\n",
        "        testModel(model,\n",
        "                  model_name = modelName,\n",
        "                  dir_to_save = dirToSave,\n",
        "                  data_loader_test = data_loader_test,\n",
        "                  bin_umbrals_to_try = [0.5, 0.7, modelBestUmbral[modelName]],\n",
        "                  model_resize_boxes = modelResizedBB[modelName],\n",
        "                  times_pass_model  = times_pass_model,\n",
        "                  type_combination  =  modelCombinationOp[modelName])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0FZs6Eu0X2K"
      },
      "outputs": [],
      "source": [
        "#Save images\n",
        "\n",
        "modelName = 'Model6'\n",
        "model_path = f'/content/models/exp1/{modelLoad[modelName]}'\n",
        "model = torch.load(model_path, map_location=torch.device(device))\n",
        "print(f'Loading model {modelLoad[modelName]}')\n",
        "model.eval()\n",
        "\n",
        "if modelPredictDropout[modelName]:\n",
        "    model.enable_eval_dropout()\n",
        "    model.set_dropout_probability(modelDropoutProb[modelName])\n",
        "\n",
        "model.to(device)\n",
        "for times_pass_model in [10, 20, 30, 50, 100]:\n",
        "    testModel(model,\n",
        "              model_name = modelName,\n",
        "              dir_to_save = dirToSave,\n",
        "              data_loader_test = data_loader_test,\n",
        "              bin_umbrals_to_try = [0.5, 0.7, modelBestUmbral[modelName]],\n",
        "              model_resize_boxes = modelResizedBB[modelName],\n",
        "              times_pass_model  = times_pass_model,\n",
        "              type_combination  =  modelCombinationOp[modelName])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuwkW_u59brZ"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbZpRIBL9brZ"
      },
      "outputs": [],
      "source": [
        "datasetLoader = DatasetLoader(dataset_name)\n",
        "test_json, test_img = datasetLoader.loadTestPaths()\n",
        "\n",
        "resize_boxes = True\n",
        "\n",
        "if resize_boxes:\n",
        "  dataset_testing = TestMusicDataset(name=dataset_name, jsonPaths=test_json, imagesPaths=test_img, resize_shape=resized_shape, transforms=get_transform())\n",
        "else:\n",
        "  dataset_testing = TestMusicDataset(name=dataset_name, jsonPaths=train_json, imagesPaths=train_img, resize_shape=resized_shape, transforms=get_transform())\n",
        "\n",
        "# Training dataset\n",
        "data_loader_testing = DataLoader(\n",
        "    dataset_testing, batch_size=batch_size, shuffle=True, num_workers=0\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_path = '/content/models/exp1/SAE_dropout02.pt'\n",
        "# model_path = '/content/SAE_dropout02.pt'\n",
        "model = torch.load(model_path, map_location=torch.device(device))\n",
        "model.eval()\n",
        "model.enable_eval_dropout()\n",
        "model.set_dropout_probability(0.2)\n",
        "\n",
        "\n",
        "times_pass_model =  100\n",
        "type_combination = PredictionsCombinationType.MEAN\n",
        "\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "f1_metric = []\n",
        "for iteration, batch in enumerate(data_loader_testing):\n",
        "    print()\n",
        "    print(iteration)\n",
        "    print()\n",
        "\n",
        "    # Get the inputs and labels from the batch\n",
        "    image, target = batch\n",
        "    # image, targetImage = image.to(device), targetImage.to(device)\n",
        "\n",
        "    model.to(device)\n",
        "    # Forward pass\n",
        "    reconstructed = forwardToModel(model=model,\n",
        "                                    image=image,\n",
        "                                    times_pass_model=times_pass_model,\n",
        "                                    type_combination=type_combination\n",
        "                                    )\n",
        "    detached_reconstructed = reconstructed.cpu().detach()\n",
        "    boxes = getConnectedComponents(detached_reconstructed, bin_threshold_percentaje=0.41)\n",
        "    boxes = [resize_box(box, BBOX_REDIMENSIONED_RECOVER, BBOX_REDIMENSIONED_RECOVER) for box in boxes]\n",
        "    saveBoxesOnTensorToImage(tensor_image=image, bboxes=boxes, image_name=f'/content/drive/MyDrive/TFG/images/{target[\"name\"]}_bestModel_bin0.41.png')\n",
        "\n",
        "\n",
        "    reconstructed = forwardToModel(model=model,\n",
        "                                    image=image,\n",
        "                                    times_pass_model=times_pass_model,\n",
        "                                    type_combination=type_combination\n",
        "                                    )\n",
        "    detached_reconstructed = reconstructed.cpu().detach()\n",
        "    boxes = getConnectedComponents(detached_reconstructed, bin_threshold_percentaje=0.7)\n",
        "    saveBoxesOnTensorToImage(tensor_image=image, bboxes=boxes, image_name=f'/content/drive/MyDrive/TFG/images/{target[\"name\"]}_bestModel_bin0.7.png')\n",
        "\n",
        "    # f1, matched_ious = calculate_F1(y_true = targetBoxes, y_pred = boxes, iou_threshold=0.5)\n",
        "    # f1_metric.append(f1)\n",
        "\n",
        "    # print(f1)\n",
        "    # drawBoxesPredictedAndGroundTruth(detached_reconstructed, boxes, targetBoxes)\n",
        "    # drawBoxesOnTensor(reconstructed.cpu().detach(), boxes)\n",
        "\n",
        "    # targetBoxes = target['boxes'].squeeze().numpy().tolist()\n",
        "    # saveBoxesOnTensorToImage(tensor_image=image, bboxes=targetBoxes, image_name=f'/content/drive/MyDrive/TFG/images/{target[\"name\"]}_GT.png')\n",
        "    # saveTensorToImageGrayScale(tensor_image=image, image_name=f'/content/drive/MyDrive/TFG/images/{iteration}_GT.png')\n",
        "    # drawBoxesOnTensor(reconstructed.cpu().detach(),targetBoxes)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print()\n",
        "\n",
        "# print(np.mean(f1_metric))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# drawBoxesOnTensor(reconstructed.cpu().detach(), boxes)\n",
        "# drawBoxesOnTensor(reconstructed.cpu().detach(),targetBoxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GDTJZGu9brZ"
      },
      "source": [
        "## One image eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u63Rmf-m5lHA"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    evalData = iter(data_loader_eval)\n",
        "    next(evalData)\n",
        "    next(evalData)\n",
        "    batch = next(evalData)\n",
        "    image, target, targetImage = batch\n",
        "\n",
        "    GT_bbox = target['boxes'].squeeze().numpy().tolist()\n",
        "\n",
        "    # saveTensorToImageGrayScale(model(image.to(device)).cpu(), 'dropout3_mean_pass1.jpg')\n",
        "    # saveTensorToImageGrayScale(model(image.to(device)).cpu(), 'dropout3_mean_pass2.jpg')\n",
        "    # saveTensorToImageGrayScale(model(image.to(device)).cpu(), 'dropout3_mean_pass3.jpg')\n",
        "\n",
        "    # drawTensorImageGrayScale(model(image.to(device)).cpu())\n",
        "    # drawTensorImageGrayScale(model(image.to(device)).cpu())\n",
        "    # drawTensorImageGrayScale(model(image.to(device)).cpu())\n",
        "\n",
        "    imagenRes = model(image.to(device)).cpu()\n",
        "    imagen_bin5 = binarizarTensor(imagenRes, bin_threshold_percentaje = 0.3)\n",
        "    imagen_bin7 = binarizarTensor(imagenRes, bin_threshold_percentaje = 0.5)\n",
        "    imagen_bin9 = binarizarTensor(imagenRes, bin_threshold_percentaje = 0.7)\n",
        "\n",
        "    # saveTensorToImageGrayScale(imagenRes, 'bin_prediction.jpg')\n",
        "    # saveTensorToImageGrayScale(imagen_bin5, 'bin_output3.jpg')\n",
        "    # saveTensorToImageGrayScale(imagen_bin7, 'bin_output5.jpg')\n",
        "    # saveTensorToImageGrayScale(imagen_bin9, 'bin_output7.jpg')\n",
        "\n",
        "    # saveTensorToImageGrayScale(imagenRes, 'bin_prediction.jpg')\n",
        "    drawTensorImageGrayScale(imagen_bin5)\n",
        "    drawTensorImageGrayScale(imagen_bin7)\n",
        "    # saveTensorToImageGrayScale(imagen_bin9, 'bin_output7.jpg')\n",
        "\n",
        "\n",
        "    # print('Mean result of 10 passes')\n",
        "    # saveTensorToImageGrayScale(forwardToModel(model, image, times_pass_model = 10, type_combination = 'mean'), 'dropout_10mean_result.jpg')\n",
        "    # saveTensorToImageGrayScale(forwardToModel(model, image, times_pass_model = 10, type_combination = 'max'), 'dropout_10max_result.jpg')\n",
        "\n",
        "    bboxes = getConnectedComponents(imagen_bin7)\n",
        "\n",
        "    drawBoxesOnTensor(imagen_bin7, bboxes)\n",
        "    saveBoxesOnTensorToImage(imagen_bin7, bboxes, 'bin_boxes5.jpg')\n",
        "\n",
        "    resizedBboxes = [resize_box(box, BBOX_REDIMENSIONED_RECOVER, BBOX_REDIMENSIONED_RECOVER) for box in bboxes]\n",
        "\n",
        "    saveBoxesPredictedAndGroundTruth(imagen_bin7, resizedBboxes, bboxes, 'resized_model_output.jpg')\n",
        "\n",
        "    saveBoxesPredictedAndGroundTruth(image, resizedBboxes, GT_bbox, 'resized_model_GT.jpg')\n",
        "    # drawTensorImageGrayScale(targetImage)\n",
        "    # print(target['boxes'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQGyGw33Ow9Q"
      },
      "source": [
        "## Testing Resize BB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmcpLhP9OtY-"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    evalData = iter(data_loader_train)\n",
        "    next(evalData)\n",
        "    next(evalData)\n",
        "    batch = next(evalData)\n",
        "    image, target, targetImage = batch\n",
        "\n",
        "    boxes = [box for box in target['boxes'].squeeze().numpy().tolist()]\n",
        "    boxesResized = [resize_box(box, 1/0.8,  1/0.8) for box in boxes]\n",
        "\n",
        "    print(boxes)\n",
        "    print(boxesResized)\n",
        "\n",
        "    imageDraw = draw_back_white_bb_image(height=512, width=512, boxes=boxes)\n",
        "    imageDrawResized = draw_back_white_bb_image(height=512, width=512, boxes=boxesResized)\n",
        "\n",
        "    # saveTensorToImageGrayScale(model(image.to(device)).cpu(), 'dropout3_mean_pass1.jpg')\n",
        "    # saveTensorToImageGrayScale(model(image.to(device)).cpu(), 'dropout3_mean_pass2.jpg')\n",
        "    # saveTensorToImageGrayScale(model(image.to(device)).cpu(), 'dropout3_mean_pass3.jpg')\n",
        "\n",
        "    # drawTensorImageGrayScale(model(image.to(device)).cpu())\n",
        "    # drawTensorImageGrayScale(model(image.to(device)).cpu())\n",
        "    # drawTensorImageGrayScale(model(image.to(device)).cpu())\n",
        "\n",
        "    # imagenRes = model(image.to(device)).cpu()\n",
        "    # imagen_bin5 = binarizarTensor(imagenRes, bin_threshold_percentaje = 0.3)\n",
        "    # imagen_bin7 = binarizarTensor(imagenRes, bin_threshold_percentaje = 0.5)\n",
        "    # imagen_bin9 = binarizarTensor(imagenRes, bin_threshold_percentaje = 0.7)\n",
        "\n",
        "    # saveTensorToImageGrayScale(imagenRes, 'bin_prediction.jpg')\n",
        "    # saveTensorToImageGrayScale(imagen_bin5, 'bin_output3.jpg')\n",
        "    # saveTensorToImageGrayScale(imagen_bin7, 'bin_output5.jpg')\n",
        "    # saveTensorToImageGrayScale(imagen_bin9, 'bin_output7.jpg')\n",
        "\n",
        "\n",
        "    # Convert the PIL image to a NumPy array\n",
        "\n",
        "    img_array = np.array(imageDraw)\n",
        "\n",
        "    # Plot the image using matplotlib\n",
        "    plt.imshow(img_array)\n",
        "    plt.show()\n",
        "\n",
        "    drawTensorImageGrayScale(image[0][0])\n",
        "    drawTensorImageGrayScale(targetImage[0][0])\n",
        "    # saveTensorToImageGrayScale(imagenRes, 'bin_prediction.jpg')\n",
        "    # drawTensorImageGrayScale(imagen_bin7)\n",
        "    # saveTensorToImageGrayScale(imagen_bin9, 'bin_output7.jpg')\n",
        "\n",
        "\n",
        "    # print('Mean result of 10 passes')\n",
        "    # saveTensorToImageGrayScale(forwardToModel(model, image, times_pass_model = 10, type_combination = 'mean'), 'dropout_10mean_result.jpg')\n",
        "    # saveTensorToImageGrayScale(forwardToModel(model, image, times_pass_model = 10, type_combination = 'max'), 'dropout_10max_result.jpg')\n",
        "\n",
        "    # bboxes = getConnectedComponents(imagen_bin7)\n",
        "\n",
        "    # drawBoxesOnTensor(imagen_bin7, bboxes)\n",
        "    # saveBoxesOnTensorToImage(imagen_bin7, bboxes, 'bin_boxes5.jpg')\n",
        "    # drawTensorImageGrayScale(targetImage)\n",
        "    # print(target['boxes'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aCSkyjLiSND"
      },
      "source": [
        "# Experimento 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vibwHQvJira3"
      },
      "source": [
        "## Iniciar variables y datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNvW6-2xivoe"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'Capitan' # 'Capitan'\n",
        "batch_size = 1\n",
        "modelName = 'SAE'\n",
        "resized_shape = SAE_IMAGE_SIZE\n",
        "path_checkpoint = modelName.lower() + \"_save\"\n",
        "\n",
        "resize_boxes = True\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbbtGCpw20tm"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok9dxwnh20tm"
      },
      "outputs": [],
      "source": [
        "modelNames = ['Model1']\n",
        "\n",
        "modelSavePath  = {\n",
        "    modelNames[0]: 'SAE_d2_r8',\n",
        "}\n",
        "\n",
        "modelResizedBB  = {\n",
        "    modelNames[0]: True,\n",
        "}\n",
        "\n",
        "\n",
        "modelDropoutProb = {\n",
        "    modelNames[0]: 0.2,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZUitVb320tm"
      },
      "source": [
        "## Train one dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4Yjtw_z20tm"
      },
      "source": [
        "### Train SEILS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCfBck1d20tn"
      },
      "outputs": [],
      "source": [
        "seilsDatasetLoader = DatasetLoader('SEILS')\n",
        "train_json, train_img = seilsDatasetLoader.loadTrainPaths()\n",
        "val_json, val_img = seilsDatasetLoader.loadValPaths()\n",
        "\n",
        "\n",
        "for modelName in modelNames:\n",
        "    model = SAE()\n",
        "    model.set_dropout_probability(modelDropoutProb[modelName])\n",
        "\n",
        "    dataset_box_resize = BBOX_REDIMENSION if modelResizedBB[modelName] else None\n",
        "\n",
        "    # Load dataset\n",
        "    dataset_train = TrainMusicDataset(name='SEILS', jsonPaths=train_json, imagesPaths=train_img, resize_shape=resized_shape, transforms=get_transform(), box_resize=dataset_box_resize)\n",
        "    dataset_eval = TrainMusicDataset(name='SEILS', jsonPaths=val_json, imagesPaths=val_img, resize_shape=resized_shape, transforms=get_transform(), box_resize=BBOX_REDIMENSION)\n",
        "\n",
        "    # Dataset loaders\n",
        "    data_loader_train = DataLoader( dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    data_loader_eval = DataLoader( dataset_eval, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    model = trainModel(data_loader_train, data_loader_eval, model, 100, 50, path_checkpoint =  f'/content/drive/MyDrive/TFG/scripts/models/uniq/{modelSavePath[modelName]}_SEILS.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSwTwq5dU8Zq"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/models/SAE_dropout05_redim8.pt'\n",
        "model_dropout = 0.5\n",
        "model = torch.load(model_path, map_location=torch.device(device))\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "bin_umbrals_to_try = np.arange(0, 1, 0.01)\n",
        "best_bin_thresholdMean = []\n",
        "best_bin_thresholdMax = []\n",
        "\n",
        "\n",
        "best_bin_thresholdMax.append(getBestIoUThreshold(model, type_combination = 'max', model_resize_boxes = True, bin_umbrals_to_try = bin_umbrals_to_try, times_pass_model = times_pass_model, dropout_value=model_dropout))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn1VWpU2pQmo"
      },
      "outputs": [],
      "source": [
        "matrix_of_iou = np.asarray([[calculate_iou(box_pred, box_true) for box_true in []] for box_pred in []])\n",
        "matrix_of_iou.size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRWAbQfI20tn"
      },
      "source": [
        "### Train FMT_M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrrjLpfO20tn"
      },
      "outputs": [],
      "source": [
        "fmtmDatasetLoader = DatasetLoader('FMT_M')\n",
        "train_json, train_img = fmtmDatasetLoader.loadTrainPaths()\n",
        "val_json, val_img = fmtmDatasetLoader.loadValPaths()\n",
        "\n",
        "\n",
        "for modelName in modelNames:\n",
        "    model = SAE()\n",
        "    model.set_dropout_probability(modelDropoutProb[modelName])\n",
        "\n",
        "    dataset_box_resize = BBOX_REDIMENSION if modelResizedBB[modelName] else None\n",
        "\n",
        "    # Load dataset\n",
        "    dataset_train = TrainMusicDataset(name='FMT_M', jsonPaths=train_json, imagesPaths=train_img, resize_shape=resized_shape, transforms=get_transform(), box_resize=dataset_box_resize)\n",
        "    dataset_eval = TrainMusicDataset(name='FMT_M', jsonPaths=val_json, imagesPaths=val_img, resize_shape=resized_shape, transforms=get_transform(), box_resize=BBOX_REDIMENSION)\n",
        "\n",
        "    # Dataset loaders\n",
        "    data_loader_train = DataLoader( dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    data_loader_eval = DataLoader( dataset_eval, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    model = trainModel(data_loader_train, data_loader_eval, model, 100, 50, path_checkpoint =  f'/content/drive/MyDrive/TFG/scripts/models/uniq/{modelSavePath[modelName]}_FMT_M.pt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhMXn7mN20tn"
      },
      "source": [
        "### Train FMT_C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEEVS_wv20tn"
      },
      "outputs": [],
      "source": [
        "\n",
        "fmtcDatasetLoader = DatasetLoader('FMT_C')\n",
        "train_json, train_img = fmtcDatasetLoader.loadTrainPaths()\n",
        "val_json, val_img = fmtcDatasetLoader.loadValPaths()\n",
        "\n",
        "\n",
        "for modelName in modelNames:\n",
        "    model = SAE()\n",
        "    model.set_dropout_probability(modelDropoutProb[modelName])\n",
        "\n",
        "    dataset_box_resize = BBOX_REDIMENSION if modelResizedBB[modelName] else None\n",
        "\n",
        "    # Load dataset\n",
        "    dataset_train = TrainMusicDataset(name='FMT_C', jsonPaths=train_json, imagesPaths=train_img, resize_shape=resized_shape, transforms=get_transform(), box_resize=dataset_box_resize)\n",
        "    dataset_eval = TrainMusicDataset(name='FMT_C', jsonPaths=val_json, imagesPaths=val_img, resize_shape=resized_shape, transforms=get_transform(), box_resize=BBOX_REDIMENSION)\n",
        "\n",
        "    # Dataset loaders\n",
        "    data_loader_train = DataLoader( dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    data_loader_eval = DataLoader( dataset_eval, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    model = trainModel(data_loader_train, data_loader_eval, model, 100, 50, path_checkpoint =  f'/content/drive/MyDrive/TFG/scripts/models/uniq/{modelSavePath[modelName]}_FMT_C.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YajPUT0uUwVz"
      },
      "source": [
        "## Eval best umbral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dun4QwcqVJ6V"
      },
      "outputs": [],
      "source": [
        "modelNames = ['bestModel']\n",
        "\n",
        "modelLoad  = {\n",
        "    modelNames[0]: 'SAE_d2_r8',\n",
        "}\n",
        "\n",
        "modelResizedBB  = {\n",
        "    modelNames[0]: True,\n",
        "}\n",
        "\n",
        "modelPredictDropout  = {\n",
        "    modelNames[0]: True,\n",
        "}\n",
        "\n",
        "modelDropoutProb = {\n",
        "    modelNames[0]: 0.2,\n",
        "}\n",
        "\n",
        "modelCombinationOp  = {\n",
        "    modelNames[0]: 'mean',\n",
        "}\n",
        "\n",
        "modelCombinationNum  = {\n",
        "    modelNames[0]: 100,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr2siGfU6Psg"
      },
      "source": [
        "### Eval with Capitan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZG4UkRN6Rig"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'Capitan'\n",
        "datasetLoader = DatasetLoader(dataset_name)\n",
        "val_json, val_img = datasetLoader.loadValPaths()\n",
        "\n",
        "dirToSave = f'/content/drive/MyDrive/TFG/valResults{dataset_name}'\n",
        "\n",
        "for modelName in modelNames:\n",
        "    dataset_box_resize = BBOX_REDIMENSION if modelResizedBB[modelName] else None\n",
        "\n",
        "    # Load dataset\n",
        "    dataset_eval = TrainMusicDataset(name=dataset_name, jsonPaths=val_json, imagesPaths=val_img, resize_shape=resized_shape, transforms=get_transform(), box_resize=BBOX_REDIMENSION)\n",
        "\n",
        "    # Dataset loaders\n",
        "    data_loader_eval = DataLoader( dataset_eval, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    model_path = f'/content/models/uniq/{modelLoad[modelName]}_{dataset_name}.pt'\n",
        "    model = torch.load(model_path, map_location=torch.device(device))\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    if modelPredictDropout[modelName]:\n",
        "        model.enable_eval_dropout()\n",
        "        model.set_dropout_probability(modelDropoutProb[modelName])\n",
        "\n",
        "    bin_umbrals_to_try = np.arange(0, 1, 0.01)\n",
        "    best_bin_thresholdMean = []\n",
        "    best_bin_thresholdMax = []\n",
        "\n",
        "\n",
        "    getBestIoUThreshold(model, model_name=f'{dirToSave}/{dataset_name}-{modelName}-', data_loader_eval=data_loader_eval,\n",
        "                        type_combination=modelCombinationOp[modelName],\n",
        "                        model_resize_boxes=modelResizedBB[modelName], bin_umbrals_to_try=bin_umbrals_to_try,\n",
        "                        times_pass_model=modelCombinationNum[modelName], dropout_value=modelDropoutProb[modelName]\n",
        "                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLJOfPhRVVx8"
      },
      "source": [
        "### Eval with SEILS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTq9Mak5UzZf"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'SEILS'\n",
        "datasetLoader = DatasetLoader(dataset_name)\n",
        "val_json, val_img = datasetLoader.loadValPaths()\n",
        "\n",
        "dirToSave = f'/content/drive/MyDrive/TFG/valResults{dataset_name}'\n",
        "\n",
        "for modelName in modelNames:\n",
        "    dataset_box_resize = BBOX_REDIMENSION if modelResizedBB[modelName] else None\n",
        "\n",
        "    # Load dataset\n",
        "    dataset_eval = TrainMusicDataset(name=dataset_name, jsonPaths=val_json, imagesPaths=val_img, resize_shape=resized_shape, transforms=get_transform(), box_resize=BBOX_REDIMENSION)\n",
        "\n",
        "    # Dataset loaders\n",
        "    data_loader_eval = DataLoader( dataset_eval, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    model_path = f'/content/models/uniq/{modelLoad[modelName]}_{dataset_name}.pt'\n",
        "    model = torch.load(model_path, map_location=torch.device(device))\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    if modelPredictDropout[modelName]:\n",
        "        model.enable_eval_dropout()\n",
        "        model.set_dropout_probability(modelDropoutProb[modelName])\n",
        "\n",
        "    bin_umbrals_to_try = np.arange(0, 1, 0.01)\n",
        "    best_bin_thresholdMean = []\n",
        "    best_bin_thresholdMax = []\n",
        "\n",
        "    getBestIoUThreshold(model, model_name=f'{dirToSave}/{dataset_name}-{modelName}-', data_loader_eval=data_loader_eval,\n",
        "                        type_combination=modelCombinationOp[modelName],\n",
        "                        model_resize_boxes=modelResizedBB[modelName], bin_umbrals_to_try=bin_umbrals_to_try,\n",
        "                        times_pass_model=modelCombinationNum[modelName], dropout_value=modelDropoutProb[modelName]\n",
        "                        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWx8yKQTW6o7"
      },
      "source": [
        "### Eval with FMT_C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WKMOHI9W9GK"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'FMT_C'\n",
        "datasetLoader = DatasetLoader(dataset_name)\n",
        "val_json, val_img = datasetLoader.loadValPaths()\n",
        "\n",
        "dirToSave = f'/content/drive/MyDrive/TFG/valResults{dataset_name}'\n",
        "\n",
        "for modelName in modelNames:\n",
        "    dataset_box_resize = BBOX_REDIMENSION if modelResizedBB[modelName] else None\n",
        "\n",
        "    # Load dataset\n",
        "    dataset_eval = TrainMusicDataset(name=dataset_name, jsonPaths=val_json, imagesPaths=val_img, resize_shape=resized_shape, transforms=get_transform(), box_resize=BBOX_REDIMENSION)\n",
        "\n",
        "    # Dataset loaders\n",
        "    data_loader_eval = DataLoader( dataset_eval, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    model_path = f'/content/models/uniq/{modelLoad[modelName]}_{dataset_name}.pt'\n",
        "    model = torch.load(model_path, map_location=torch.device(device))\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    if modelPredictDropout[modelName]:\n",
        "        model.enable_eval_dropout()\n",
        "        model.set_dropout_probability(modelDropoutProb[modelName])\n",
        "\n",
        "    bin_umbrals_to_try = np.arange(0, 1, 0.01)\n",
        "    best_bin_thresholdMean = []\n",
        "    best_bin_thresholdMax = []\n",
        "\n",
        "# getBestIoUThresholdMultipleDatasets\n",
        "    getBestIoUThreshold(model, model_name=f'{dirToSave}/{dataset_name}-{modelName}-', data_loader_eval=data_loader_eval,\n",
        "                        type_combination=modelCombinationOp[modelName],\n",
        "                        model_resize_boxes=modelResizedBB[modelName], bin_umbrals_to_try=bin_umbrals_to_try,\n",
        "                        times_pass_model=modelCombinationNum[modelName], dropout_value=modelDropoutProb[modelName]\n",
        "                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqXmTAY97xp5"
      },
      "source": [
        "### Eval with Capitan + SEILS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQta6SMd8A5d"
      },
      "outputs": [],
      "source": [
        "datasetName1 = 'Capitan'\n",
        "datasetLoader1 = DatasetLoader(datasetName1)\n",
        "val_json1, val_img1 = datasetLoader1.loadValPaths()\n",
        "\n",
        "datasetName2 = 'SEILS'\n",
        "datasetLoader2 = DatasetLoader(datasetName2)\n",
        "val_json2, val_img2 = datasetLoader2.loadValPaths()\n",
        "\n",
        "dirToSave = f'/content/drive/MyDrive/TFG/valResults{datasetName1}+{datasetName2}'\n",
        "\n",
        "for modelName in modelNames:\n",
        "    dataset_box_resize = BBOX_REDIMENSION if modelResizedBB[modelName] else None\n",
        "\n",
        "    # Load dataset\n",
        "    dataset_eval1 = TrainMusicDataset(name=dataset_name, jsonPaths=val_json1, imagesPaths=val_img1, resize_shape=resized_shape, transforms=get_transform(), box_resize=BBOX_REDIMENSION)\n",
        "\n",
        "    # Dataset loaders\n",
        "    data_loader_eval1 = DataLoader( dataset_eval1, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "     # Load dataset\n",
        "    dataset_eval2 = TrainMusicDataset(name=dataset_name, jsonPaths=val_json2, imagesPaths=val_img2, resize_shape=resized_shape, transforms=get_transform(), box_resize=BBOX_REDIMENSION)\n",
        "\n",
        "    # Dataset loaders\n",
        "    data_loader_eval2 = DataLoader( dataset_eval2, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    model_path = f'/content/models/comb/{modelLoad[modelName]}_{datasetName1}+{datasetName2}.pt'\n",
        "    model = torch.load(model_path, map_location=torch.device(device))\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    if modelPredictDropout[modelName]:\n",
        "        model.enable_eval_dropout()\n",
        "        model.set_dropout_probability(modelDropoutProb[modelName])\n",
        "\n",
        "    bin_umbrals_to_try = np.arange(0, 1, 0.01)\n",
        "    best_bin_thresholdMean = []\n",
        "    best_bin_thresholdMax = []\n",
        "\n",
        "    getBestIoUThresholdMultipleDatasets(model, model_name=f'{dirToSave}/{dataset_name}-{modelName}-', data_loader_eval=[data_loader_eval1, data_loader_eval2],\n",
        "                        type_combination=modelCombinationOp[modelName],\n",
        "                        model_resize_boxes=modelResizedBB[modelName], bin_umbrals_to_try=bin_umbrals_to_try,\n",
        "                        times_pass_model=modelCombinationNum[modelName], dropout_value=modelDropoutProb[modelName]\n",
        "                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcVTwndh9P9R"
      },
      "source": [
        "### Eval with Capitan+FMT_C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4W3-FrX9UPL"
      },
      "outputs": [],
      "source": [
        "datasetName1 = 'Capitan'\n",
        "datasetLoader1 = DatasetLoader(datasetName1)\n",
        "val_json1, val_img1 = datasetLoader1.loadValPaths()\n",
        "\n",
        "datasetName2 = 'FMT_C'\n",
        "datasetLoader2 = DatasetLoader(datasetName2)\n",
        "val_json2, val_img2 = datasetLoader2.loadValPaths()\n",
        "\n",
        "dirToSave = f'/content/drive/MyDrive/TFG/valResults{datasetName1}+{datasetName2}'\n",
        "\n",
        "for modelName in modelNames:\n",
        "    dataset_box_resize = BBOX_REDIMENSION if modelResizedBB[modelName] else None\n",
        "\n",
        "    # Load dataset\n",
        "    dataset_eval1 = TrainMusicDataset(name=dataset_name, jsonPaths=val_json1, imagesPaths=val_img1, resize_shape=resized_shape, transforms=get_transform(), box_resize=BBOX_REDIMENSION)\n",
        "\n",
        "    # Dataset loaders\n",
        "    data_loader_eval1 = DataLoader( dataset_eval1, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "     # Load dataset\n",
        "    dataset_eval2 = TrainMusicDataset(name=dataset_name, jsonPaths=val_json2, imagesPaths=val_img2, resize_shape=resized_shape, transforms=get_transform(), box_resize=BBOX_REDIMENSION)\n",
        "\n",
        "    # Dataset loaders\n",
        "    data_loader_eval2 = DataLoader( dataset_eval2, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    model_path = f'/content/models/comb/{modelLoad[modelName]}_{datasetName1}+{datasetName2}.pt'\n",
        "    model = torch.load(model_path, map_location=torch.device(device))\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    if modelPredictDropout[modelName]:\n",
        "        model.enable_eval_dropout()\n",
        "        model.set_dropout_probability(modelDropoutProb[modelName])\n",
        "\n",
        "    bin_umbrals_to_try = np.arange(0, 1, 0.01)\n",
        "    best_bin_thresholdMean = []\n",
        "    best_bin_thresholdMax = []\n",
        "\n",
        "    getBestIoUThresholdMultipleDatasets(model, model_name=f'{dirToSave}/{datasetName1}-{datasetName2}-', data_loader_eval=[data_loader_eval1, data_loader_eval2],\n",
        "                        type_combination=modelCombinationOp[modelName],\n",
        "                        model_resize_boxes=modelResizedBB[modelName], bin_umbrals_to_try=bin_umbrals_to_try,\n",
        "                        times_pass_model=modelCombinationNum[modelName], dropout_value=modelDropoutProb[modelName]\n",
        "                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naUSApFN9WDs"
      },
      "source": [
        "### Eval with SEILS+FMT_C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuTjKM_i9YDZ"
      },
      "outputs": [],
      "source": [
        "datasetName1 = 'SEILS'\n",
        "datasetLoader1 = DatasetLoader(datasetName1)\n",
        "val_json1, val_img1 = datasetLoader1.loadValPaths()\n",
        "\n",
        "datasetName2 = 'FMT_C'\n",
        "datasetLoader2 = DatasetLoader(datasetName2)\n",
        "val_json2, val_img2 = datasetLoader2.loadValPaths()\n",
        "\n",
        "dirToSave = f'/content/drive/MyDrive/TFG/valResults{datasetName1}+{datasetName2}'\n",
        "\n",
        "for modelName in modelNames:\n",
        "    dataset_box_resize = BBOX_REDIMENSION if modelResizedBB[modelName] else None\n",
        "\n",
        "    # Load dataset\n",
        "    dataset_eval1 = TrainMusicDataset(name=dataset_name, jsonPaths=val_json1, imagesPaths=val_img1, resize_shape=resized_shape, transforms=get_transform(), box_resize=BBOX_REDIMENSION)\n",
        "\n",
        "    # Dataset loaders\n",
        "    data_loader_eval1 = DataLoader( dataset_eval1, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "     # Load dataset\n",
        "    dataset_eval2 = TrainMusicDataset(name=dataset_name, jsonPaths=val_json2, imagesPaths=val_img2, resize_shape=resized_shape, transforms=get_transform(), box_resize=BBOX_REDIMENSION)\n",
        "\n",
        "    # Dataset loaders\n",
        "    data_loader_eval2 = DataLoader( dataset_eval2, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    model_path = f'/content/models/comb/{modelLoad[modelName]}_{datasetName1}+{datasetName2}.pt'\n",
        "    model = torch.load(model_path, map_location=torch.device(device))\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    if modelPredictDropout[modelName]:\n",
        "        model.enable_eval_dropout()\n",
        "        model.set_dropout_probability(modelDropoutProb[modelName])\n",
        "\n",
        "    bin_umbrals_to_try = np.arange(0, 1, 0.01)\n",
        "    best_bin_thresholdMean = []\n",
        "    best_bin_thresholdMax = []\n",
        "\n",
        "    getBestIoUThresholdMultipleDatasets(model, model_name=f'{dirToSave}/{modelName}-', data_loader_eval=[data_loader_eval1, data_loader_eval2],\n",
        "                        type_combination=modelCombinationOp[modelName],\n",
        "                        model_resize_boxes=modelResizedBB[modelName], bin_umbrals_to_try=bin_umbrals_to_try,\n",
        "                        times_pass_model=modelCombinationNum[modelName], dropout_value=modelDropoutProb[modelName]\n",
        "                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXT4_2qc20to"
      },
      "source": [
        "## Test models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBG87eg-20tp"
      },
      "outputs": [],
      "source": [
        "modelNames = ['Capitan', 'SEILS', 'FMT_C', 'Capitan+SEILS', 'Capitan+FMT_C', 'SEILS+FMT_C']\n",
        "\n",
        "\n",
        "\n",
        "modelLoad  = {\n",
        "    modelNames[0] : f'uniq/SAE_d2_r8_{modelNames[0]}.pt',\n",
        "    modelNames[1] : f'uniq/SAE_d2_r8_{modelNames[1]}.pt',\n",
        "    modelNames[2] : f'uniq/SAE_d2_r8_{modelNames[2]}.pt',\n",
        "    modelNames[3] : f'comb/SAE_d2_r8_{modelNames[3]}.pt',\n",
        "    modelNames[4] : f'comb/SAE_d2_r8_{modelNames[4]}.pt',\n",
        "    modelNames[5] : f'comb/SAE_d2_r8_{modelNames[5]}.pt',\n",
        "    }\n",
        "\n",
        "\n",
        "modelResizedBB  = {\n",
        "    modelName:True for modelName in modelNames\n",
        "    }\n",
        "\n",
        "modelPredictDropout  = {\n",
        "    modelName:True for modelName in modelNames\n",
        "    }\n",
        "\n",
        "modelDropoutProb = {\n",
        "    modelName:0.2 for modelName in modelNames\n",
        "    }\n",
        "\n",
        "modelCombinationOp  = {\n",
        "    modelName:'mean' for modelName in modelNames\n",
        "    }\n",
        "\n",
        "modelCombinationNum  = {\n",
        "    modelName:20 for modelName in modelNames\n",
        "    }\n",
        "\n",
        "modelBestUmbral  = {\n",
        "    modelNames[0]: 0.57,\n",
        "    modelNames[1]: 0.45,\n",
        "    modelNames[2]: 0.37,\n",
        "    modelNames[3]: 0.43,\n",
        "    modelNames[4]: 0.45,\n",
        "    modelNames[5]: 0.41,\n",
        "}\n",
        "\n",
        "singularModelNames = modelNames[ :3]  # Capitan, SEILS, FMT_C\n",
        "compostModelNames  = modelNames[3: ]  # All +'s\n",
        "\n",
        "modelsToTestWithCapitan = ['Capitan']#['SEILS+FMT_C']#['SEILS', 'FMT_C', 'Capitan+SEILS', 'Capitan+FMT_C']\n",
        "modelsToTestWithSEILS   = ['Capitan+FMT_C']#['Capitan', 'SEILS', 'FMT_C', 'Capitan+SEILS', 'SEILS+FMT_C']\n",
        "modelsToTestWithFMTC    = ['Capitan+SEILS']#['Capitan', 'SEILS', 'FMT_C', 'Capitan+FMT_C', 'SEILS+FMT_C']\n",
        "\n",
        "modelsToTestWithCapitan = modelNames\n",
        "modelsToTestWithSEILS   = modelNames\n",
        "modelsToTestWithFMTC    = modelNames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38VdDfcb1bQd"
      },
      "outputs": [],
      "source": [
        "\n",
        "for dataset_name in ['Capitan', 'SEILS', 'FMT_C']:\n",
        "  datasetLoader = DatasetLoader(dataset_name)\n",
        "\n",
        "  # datasetLoader.drawBoxesInDataset()\n",
        "\n",
        "  # Load the datasets\n",
        "  test_json, test_img = datasetLoader.loadTestPaths()\n",
        "\n",
        "  # Resized for coco (to check if metrics change)\n",
        "  dataset_test = TestMusicDataset(name=dataset_name, jsonPaths=test_json, imagesPaths=test_img, resize_shape=resized_shape, transforms=get_transform())\n",
        "  data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "\n",
        "  for iteration, batch in enumerate(data_loader_test):\n",
        "      print()\n",
        "      print(iteration)\n",
        "      print()\n",
        "\n",
        "      # Get the inputs and labels from the batch\n",
        "      image, target = batch\n",
        "      # image, targetImage = image.to(device), targetImage.to(device)\n",
        "      targetBoxes = target.squeeze().numpy().tolist()\n",
        "      # drawBoxesOnTensor(image, targetBoxes)\n",
        "\n",
        "      saveBoxesOnTensorToImage(tensor_image=image, bboxes=targetBoxes, image_name=f'/content/drive/MyDrive/TFG/images/{dataset_name}/{iteration}_GT.png')\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "      gc.collect()\n",
        "      print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCi-G1UD20tp"
      },
      "source": [
        "### Test with Capitan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3aciZgR20tp"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'Capitan'\n",
        "datasetLoader = DatasetLoader(dataset_name)\n",
        "\n",
        "# datasetLoader.drawBoxesInDataset()\n",
        "\n",
        "# Load the datasets\n",
        "test_json, test_img = datasetLoader.loadTestPaths()\n",
        "\n",
        "# Resized for coco (to check if metrics change)\n",
        "dataset_test = TestMusicDataset(name=dataset_name, jsonPaths=test_json, imagesPaths=test_img, resize_shape=resized_shape, transforms=get_transform())\n",
        "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "dirToSave = f'/content/drive/MyDrive/TFG/exp2/testResults{dataset_name}'\n",
        "\n",
        "for modelName in modelsToTestWithCapitan:\n",
        "    model_path = f'/content/models/{modelLoad[modelName]}'\n",
        "    model = torch.load(model_path, map_location=torch.device(device))\n",
        "    print(f'Loading model {modelLoad[modelName]}')\n",
        "    model.eval()\n",
        "    if modelPredictDropout[modelName]:\n",
        "\n",
        "        model.enable_eval_dropout()\n",
        "        model.set_dropout_probability(modelDropoutProb[modelName])\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Se guarda el fichero en el directorio indicado con el nombre del modelName\n",
        "    testModel(model,\n",
        "              model_name = modelName,\n",
        "              dir_to_save = dirToSave,\n",
        "              data_loader_test = data_loader_test,\n",
        "              bin_umbrals_to_try = [0.5, 0.7, modelBestUmbral[modelName]],\n",
        "              model_resize_boxes = modelResizedBB[modelName],\n",
        "              times_pass_model  =  modelCombinationNum[modelName],\n",
        "              type_combination  =  modelCombinationOp[modelName],\n",
        "              saveImage = True,\n",
        "              dataset_name = dataset_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEMbU6CI20tq"
      },
      "source": [
        "### Test with SEILS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p2SNB7c20tq"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'SEILS'\n",
        "datasetLoader = DatasetLoader(dataset_name)\n",
        "\n",
        "# datasetLoader.drawBoxesInDataset()\n",
        "\n",
        "# Load the datasets\n",
        "test_json, test_img = datasetLoader.loadTestPaths()\n",
        "\n",
        "# Resized for coco (to check if metrics change)\n",
        "dataset_test = TestMusicDataset(name=dataset_name, jsonPaths=test_json, imagesPaths=test_img, resize_shape=resized_shape, transforms=get_transform())\n",
        "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "dirToSave = f'/content/drive/MyDrive/TFG/exp2/testResults{dataset_name}'\n",
        "\n",
        "for modelName in modelsToTestWithSEILS:\n",
        "    model_path = f'/content/models/{modelLoad[modelName]}'\n",
        "    model = torch.load(model_path, map_location=torch.device(device))\n",
        "    print(f'Loading model {modelLoad[modelName]}')\n",
        "    model.eval()\n",
        "    if modelPredictDropout[modelName]:\n",
        "\n",
        "        model.enable_eval_dropout()\n",
        "        model.set_dropout_probability(modelDropoutProb[modelName])\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Se guarda el fichero en el directorio indicado con el nombre del modelName\n",
        "    testModel(model,\n",
        "              model_name = modelName,\n",
        "              dir_to_save = dirToSave,\n",
        "              data_loader_test = data_loader_test,\n",
        "              bin_umbrals_to_try = [0.5, 0.7, modelBestUmbral[modelName]],\n",
        "              model_resize_boxes = modelResizedBB[modelName],\n",
        "              times_pass_model  =  modelCombinationNum[modelName],\n",
        "              type_combination  =  modelCombinationOp[modelName],\n",
        "              saveImage = True,\n",
        "              dataset_name = dataset_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LafS_EkL20tq"
      },
      "source": [
        "### Test with FMT_C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b2JGb8D20tq"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'FMT_C'\n",
        "datasetLoader = DatasetLoader(dataset_name)\n",
        "\n",
        "# datasetLoader.drawBoxesInDataset()\n",
        "\n",
        "# Load the datasets\n",
        "test_json, test_img = datasetLoader.loadTestPaths()\n",
        "\n",
        "# Resized for coco (to check if metrics change)\n",
        "dataset_test = TestMusicDataset(name=dataset_name, jsonPaths=test_json, imagesPaths=test_img, resize_shape=resized_shape, transforms=get_transform())\n",
        "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "dirToSave = f'/content/drive/MyDrive/TFG/exp2/testResults{dataset_name}'\n",
        "\n",
        "for modelName in modelsToTestWithFMTC:\n",
        "    model_path = f'/content/models/{modelLoad[modelName]}'\n",
        "    model = torch.load(model_path, map_location=torch.device(device))\n",
        "    print(f'Loading model {modelLoad[modelName]}')\n",
        "    model.eval()\n",
        "    if modelPredictDropout[modelName]:\n",
        "\n",
        "        model.enable_eval_dropout()\n",
        "        model.set_dropout_probability(modelDropoutProb[modelName])\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Se guarda el fichero en el directorio indicado con el nombre del modelName\n",
        "    testModel(model,\n",
        "              model_name = modelName,\n",
        "              dir_to_save = dirToSave,\n",
        "              data_loader_test = data_loader_test,\n",
        "              bin_umbrals_to_try = [0.5, 0.7, modelBestUmbral[modelName]],\n",
        "              model_resize_boxes = modelResizedBB[modelName],\n",
        "              times_pass_model  =  modelCombinationNum[modelName],\n",
        "              type_combination  =  modelCombinationOp[modelName],\n",
        "              saveImage = True,\n",
        "              dataset_name = dataset_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIyG8slq20tq"
      },
      "source": [
        "### Test with FMT_M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UILWrPo20tq"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'FMT_M'\n",
        "datasetLoader = DatasetLoader(dataset_name)\n",
        "\n",
        "# datasetLoader.drawBoxesInDataset()\n",
        "\n",
        "# Load the datasets\n",
        "test_json, test_img = datasetLoader.loadTestPaths()\n",
        "\n",
        "# Resized for coco (to check if metrics change)\n",
        "dataset_test = TestMusicDataset(name=dataset_name, jsonPaths=test_json, imagesPaths=test_img, resize_shape=resized_shape, transforms=get_transform())\n",
        "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "dirToSave = f'/content/drive/MyDrive/TFG/exp2/testResults{dataset_name}'\n",
        "\n",
        "for modelName in modelNames:\n",
        "    model_path = f'/content/models/{modelLoad[modelName]}'\n",
        "    model = torch.load(model_path, map_location=torch.device(device))\n",
        "    print(f'Loading model {modelLoad[modelName]}')\n",
        "    model.eval()\n",
        "    if modelPredictDropout[modelName]:\n",
        "\n",
        "        model.enable_eval_dropout()\n",
        "        model.set_dropout_probability(modelDropoutProb[modelName])\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Se guarda el fichero en el directorio indicado con el nombre del modelName\n",
        "    testModel(model,\n",
        "              model_name = modelName,\n",
        "              dir_to_save = dirToSave,\n",
        "              data_loader_test = data_loader_test,\n",
        "              bin_umbrals_to_try = [0.5, 0.7, modelBestUmbral[modelName]],\n",
        "              model_resize_boxes = modelResizedBB[modelName],\n",
        "              times_pass_model  =  modelCombinationNum[modelName],\n",
        "              type_combination  =  modelCombinationOp[modelName])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BM86_V44CZSG"
      },
      "source": [
        "# Predict and show eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pzyWMCaCbqk"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    for image, target, targetImage in iter(data_loader_train):\n",
        "    # saveTensorToImageGrayScale(model(image.to(device)).cpu(), 'dropout3_mean_pass1.jpg')\n",
        "\n",
        "    # drawTensorImageGrayScale(model(image.to(device)).cpu())\n",
        "\n",
        "      imagenRes = model(image.to(device)).cpu()\n",
        "      drawTensorImageGrayScale(imagenRes)\n",
        "      drawTensorImageGrayScale(binarizarTensor(imagenRes, bin_threshold_percentaje = 0.5))\n",
        "      # imagen_bin5 = binarizarTensor(imagenRes, bin_threshold_percentaje = 0.3)\n",
        "      # imagen_bin7 = binarizarTensor(imagenRes, bin_threshold_percentaje = 0.5)\n",
        "      # imagen_bin9 = binarizarTensor(imagenRes, bin_threshold_percentaje = 0.7)\n",
        "\n",
        "      # saveTensorToImageGrayScale(imagenRes, 'bin_prediction.jpg')\n",
        "      # saveTensorToImageGrayScale(imagen_bin5, 'bin_output3.jpg')\n",
        "      # saveTensorToImageGrayScale(imagen_bin7, 'bin_output5.jpg')\n",
        "      # saveTensorToImageGrayScale(imagen_bin9, 'bin_output7.jpg')\n",
        "\n",
        "    # print('Mean result of 10 passes')\n",
        "    # saveTensorToImageGrayScale(forwardToModel(model, image, times_pass_model = 10, type_combination = 'mean'), 'dropout_10mean_result.jpg')\n",
        "    # saveTensorToImageGrayScale(forwardToModel(model, image, times_pass_model = 10, type_combination = 'max'), 'dropout_10max_result.jpg')\n",
        "\n",
        "    # bboxes = getConnectedComponents(model(image.to(device)).cpu())\n",
        "\n",
        "    # drawBoxesOnTensor(model(image.to(device)).cpu(), bboxes)\n",
        "    # drawTensorImageGrayScale(targetImage)\n",
        "    # print(target['boxes'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhXRDVS95lHB"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "evalData = iter(data_loader_eval)\n",
        "next(evalData)\n",
        "next(evalData)\n",
        "batch = next(evalData)\n",
        "image, target, targetImage = batch\n",
        "imagePass = image.to(device)\n",
        "\n",
        "model.train()\n",
        "reconstructed1 = model(imagePass).cpu().detach()\n",
        "reconstructed2 = model(imagePass).cpu().detach()\n",
        "print(torch.equal(reconstructed1, reconstructed2))\n",
        "\n",
        "model.eval()\n",
        "reconstructed1 = model(imagePass).cpu().detach()\n",
        "reconstructed2 = model(imagePass).cpu().detach()\n",
        "print(torch.equal(reconstructed1, reconstructed2))\n",
        "\n",
        "model.enable_eval_dropout()\n",
        "reconstructed1 = model(imagePass).cpu().detach()\n",
        "reconstructed2 = model(imagePass).cpu().detach()\n",
        "print(torch.equal(reconstructed1, reconstructed2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adP68sq49brZ"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "evalData = iter(data_loader_eval)\n",
        "next(evalData)\n",
        "next(evalData)\n",
        "# next(evalData)\n",
        "# next(evalData)\n",
        "batch = next(evalData)\n",
        "image, target, targetImage = batch\n",
        "\n",
        "drawTensorImageGrayScale(image)\n",
        "drawTensorImageGrayScale(targetImage)\n",
        "\n",
        "imagePass = image.to(device)\n",
        "model.to(device)\n",
        "reconstructed = model(imagePass)\n",
        "\n",
        "detached_reconstructed = reconstructed.cpu().detach()\n",
        "\n",
        "drawTensorImageGrayScale(detached_reconstructed)\n",
        "\n",
        "targetBoxes = target['boxes'].squeeze().numpy().tolist()\n",
        "\n",
        "\"\"\"No Area check\"\"\"\n",
        "boxes = getConnectedComponents(detached_reconstructed, min_area=0, targetBoxes=targetBoxes)\n",
        "\n",
        "# drawBoxesPredictedAndGroundTruth(detached_reconstructed, boxes, targetBoxes)\n",
        "\n",
        "\"\"\"Area checking\"\"\"\n",
        "boxes = getConnectedComponents(detached_reconstructed, min_area=100, targetBoxes=targetBoxes)\n",
        "\n",
        "# drawBoxesPredictedAndGroundTruth(detached_reconstructed, boxes, targetBoxes)\n",
        "drawBoxesPredictedAndGroundTruth(image, boxes, targetBoxes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_Ne2YbFcJAx"
      },
      "outputs": [],
      "source": [
        "# Defining the Plot Style\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "# Plotting the last 100 values\n",
        "losses_plot = [loss_plot.cpu().detach().numpy() for loss_plot in losses[-100:]]\n",
        "plt.plot(losses_plot)\n",
        "\n",
        "print(f'ENDED TRAINING')\n",
        "print()\n",
        "\n",
        "\n",
        "transformToPil = T.ToPILImage()\n",
        "\n",
        "print(image.size())\n",
        "for i, item in enumerate(targetImage):\n",
        "    item = transformToPil(item.cpu())#.detach().numpy()\n",
        "    display(item)\n",
        "    # Reshape the array for plotting\n",
        "    #item = item.reshape(-1, 512, 512)\n",
        "    #plt.imshow(item[0])\n",
        "\n",
        "for i, item in enumerate(reconstructed):\n",
        "    item = transformToPil(item.cpu())#.detach().numpy()\n",
        "    display(item)\n",
        "    #item = item.cpu().detach().numpy()\n",
        "    #item = item.reshape(-1, 512, 512)\n",
        "    #plt.imshow(item[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KArIk0v9bra"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# read the image\n",
        "#img = cv2.imread('C:\\\\Users\\\\samuo\\\\OneDrive\\\\Escritorio\\\\TFG\\\\scripts\\\\Capitan\\\\images\\\\12608.JPG')\n",
        "img = cv2.imread('C:\\\\Users\\\\samuo\\\\Downloads\\\\resultadoAE.jpg')\n",
        "\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "_, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "# find the connected components\n",
        "output = cv2.connectedComponentsWithStats(binary, connectivity=8, ltype=cv2.CV_32S)\n",
        "\n",
        "# get the labels, stats, and centroids of each connected component\n",
        "num_labels = output[0]\n",
        "labels = output[1]\n",
        "stats = output[2]\n",
        "centroids = output[3]\n",
        "\n",
        "for i in range(1, num_labels):\n",
        "    # draw a rectangle around each connected component\n",
        "    cv2.rectangle(img, (stats[i, cv2.CC_STAT_LEFT], stats[i, cv2.CC_STAT_TOP]),\n",
        "                  (stats[i, cv2.CC_STAT_LEFT] + stats[i, cv2.CC_STAT_WIDTH],\n",
        "                   stats[i, cv2.CC_STAT_TOP] + stats[i, cv2.CC_STAT_HEIGHT]),\n",
        "                  (0, 0, 255), 2)\n",
        "\n",
        "print(num_labels)\n",
        "cv2.imwrite('C:\\\\Users\\\\samuo\\\\Downloads\\\\processed_resultadoAE.jpg', img)\n",
        "\n",
        "# convert BGR to RGB\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# display the image\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPMa7SrF9bra"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the grayscale image\n",
        "image = cv2.imread('C:\\\\Users\\\\samuo\\\\Downloads\\\\resultadoAE.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "tensor_image = T.ToTensor()(image)\n",
        "bboxes = getConnectedComponents(tensor_image, bin_threshold_percentaje = 0.5)\n",
        "drawBoxesOnTensor(tensor_image, bboxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mzgxfru29brb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Get the current date\n",
        "date_string = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
        "\n",
        "# Format the date as a number string\n",
        "\n",
        "# Print the date string\n",
        "print(date_string)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "c616h0in9brR",
        "bGtj3nxc9brS",
        "TLLMUoTI9brT",
        "6-phXazY9brU",
        "V3m9XlAk_EB6",
        "3Mt9DBEHjAAL",
        "0djNH8k0YKQx",
        "lEr6hgTS20tf",
        "GaTUUfisIu_R",
        "s8-aAbSp-gpi",
        "doJvYfEzJNae",
        "CZeHZQco9brY",
        "fwaHk_ov9brY",
        "Y_phovXS5lG_",
        "kky0VwlY5lG_",
        "BuwkW_u59brZ",
        "8GDTJZGu9brZ",
        "PQGyGw33Ow9Q",
        "7aCSkyjLiSND",
        "DbbtGCpw20tm",
        "3ZUitVb320tm",
        "gRWAbQfI20tn",
        "ZhMXn7mN20tn",
        "YajPUT0uUwVz",
        "Fr2siGfU6Psg",
        "MLJOfPhRVVx8",
        "jWx8yKQTW6o7",
        "zqXmTAY97xp5",
        "BcVTwndh9P9R",
        "naUSApFN9WDs",
        "hXT4_2qc20to",
        "BM86_V44CZSG"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
